{
  "16917": {
    "Automation13": {
      "hand-ins": [
      ]
    },
    "Automation24": {
      "hand-ins": [
      ]
    },
    "CompetenceDocument": {
      "hand-ins": [
        {
          "text": "\nVersion control\n\n\n\n\nPersona\nMy name is Elmira Drost, I am 20 years old and live in Eindhoven. I'm following the course ICT & Infrastructure and I am in my third semester.\u00a0\nBesides that I'm busy with school, I am running a plant business where I sell all kinds of plants. I started my business company back in May and it is a small company. Another thing I work on besides school is that I joined the IT committee for the student dance association Footloose. For the association, I maintain the servers that are running in the background.\nFor Delta I am working on the following projects:\nDex\nOpenRemote\nWifi Attendance\n\nProjects/ Challenges\nChallenges Infrastructure  - (Project 1)\nAt the beginning of the semester, I made some challenges and worked on the Infrastructure proftaak. I did 4 challenges for Automation and presented 2 in front of an online class. On the other side I worked on the proftaak, I was helping with the setup, brainstorming plan of approach and I\u2019ve been working on the research questions.\nDigital Excellence/ DeX - (Project 2)\nThe goal of the DeX-platform (short: DeX) is to make all work more findable and thus improving collaboration. For the DeX project, I'm working on the backend. All the infrastructure-related issues come to my side. This means that I'm currently working on preparing for the migration of Docker Compose to Docker Swarm. After that, I\u2019ll be migrating the environment to a cloud service where I\u2019m doing my research for.\nOpenRemote - (Project 3)\nThe OpenRemote project started off as a Smart Lightning project for Strijp-S. After a meeting with OpenRemote, it was clear that we couldn\u2019t take the project because it was already taken off by another group. Just by discussing with OpenRemote the project for orchestration and managing edge gateways came up. \nThis will make the process for orchestrating, managing and deploying a lot easier. It will be done by an opensource service called OpenBalena. I\u2019ve done some research, tried BalenaCloud and installing the balena OS on a Raspberry pi. In the end, the goal is to get all the process automated through OpenBalena.\nPaaspop 2021 - (Project 4)\nFor this project, I\u2019ve been brainstorming with the group for ideas for on Paaspop. After that, we got the feedback to mail Lorenzo and wait for what Lorenzo wants. Until now we only got a reply back that he is coming back to us, since he also doesn\u2019t know what is needed and what is going to happen with COVID-19.\nUpdate: After talking with the group we decided to put this project on hold since not everyone had the time to work on this project.\nWifi Attendance - (Project 5)\nThe project Wifi attendance is about tracking students and connect it with Canvas to see where students are with particular knowledge.\nFor this project, Daan and I have been working on getting the previous application working, the law of privacy and getting access for the location services.\nTEC for Society Digital connections - (Project 6)\nOutside of Delta, I\u2019m doing a project for TEC for Society Digital connections. Which means I\u2019m working on a project for WijEindhoven (department Buurt in bloei), Vitalis Woongroep and Fontys Research chair (Lectoraat). This is a project to get the elderly going in Vitalis, when they are new to a living group they don\u2019t know the way. I\u2019m solving this problem by making sure that the tablets that they have can scan custom QR-codes to get info from all places inside the building.\n\nKPI-table\nInside the KPI table I use some shortcuts to explain what I\u2019ve done for each KPI. I do this by calling the project in shorter names which is explained down here:\nCI: Challenges Infrastructure\nDex: Digital Excellence\nOR: OpenRemote\nWA: Wifi attendance\nTEC: TEC for Society Digital Connections\n\n\n\nKPI-Matrix\nThis table gives a quick overview of my current progression. What\u2019s already there and what not.\n\n\n\nEvaluation and Reflection\nEvaluation\nAt the beginning of the semester, it was difficult because there was no structure to the projects, I didn\u2019t know what to do and it was unclear which parts of Infrastructure I got an exemption for. For the structure, of the projects, I tried to communicate with the people who were also part of the projects that I\u2019m in. To get to know them and to see how we could get started. In the beginning, it was hard, because I didn\u2019t know them but after some time I started to know them and it got easier. We finally got the structure in the projects that I was doing. I also found it hard not knowing which parts for Infrastructure I got exemptions for, I tried to ask at the beginning of the semester to Peter who didn\u2019t know, after that I asked Eric about it and he said that I got an exemption for all, but coming back to Peter who said that I needed to do all and other teachers were really frustrating. After asking Martin about it he came to Eric for more information which leads to a meeting with Peter, Frank, Eric, Daan and me which finally gave the correct info which was really nice.\nWorking together with Daan didn\u2019t go that well in the beginning but after talking with Daan about it and both realizing that we needed to get going it became better. We planned meetings just to sit in call and work together.\nBesides all the things that I\u2019ve struggled with and that I found hard it was nice that I did learn a lot. I learned a lot on the professional side, on the communicating side and also on the Infrastructure side where I am proud of.\nEvaluation at the end of the semester:\nNow, looking back at the whole semester some points went good and bad. As mentioned above, working with Daan didn\u2019t go well in the beginning, after having some talks it went better but it had some up and downs here and there. I spoke multiple times about it with Daan and when we talked it did go better, but after a week or 2 it went down (sometimes). Talking to him again about it, and how I felt about it did help. \nThe bi-weekly sprint deliveries with OpenRemote did help us in the process. It gave good feedback for the next sprints, as well as some tips. We didn\u2019t finish the last product for the Balena project, but after communicating it with OpenRemote it was okay.\nI saw at the beginning of the OpenRemote project that Niek did struggle with the teamwork on his side with Software. I had the idea that only Niek was doing the hard work and sometimes had some help from Daniel and Martin. When I saw that, I decided to help him with the wind turbine. After helping Niek with the wind turbine I really was hyped about it and I\u2019m happy that I\u2019m working on it with him right now.\nOverall it was a good semester, however, sometimes I had the idea that I had too many things to do. I did prioritize the to do\u2019s, but even though it didn\u2019t work out for some cases that well. I choose to drop the project smart band and I got a chance to drop Doekoe (Proftaak Infrastructure) so I did that when I could.\nI did want to do more things for DeX, but I lost a lot of time on the wind turbine and prioritized the wind turbine over a part of the DeX project. Looking back at it, I should have worked more on DeX. \nSeeing all the projects and things I\u2019ve done, in the next semester I really want to work on projects I like a lot, so I end up with 2 projects or 3. This way I can deliver good work instead of half of the work.\nReflection\nI\u2019ve grown a lot in the last couple of weeks. Planning with a whole group when there was no particular \u2018end\u2019 goal. I\u2019ve got to work with people from different courses which was really nice to see their perspective in a project. \nI\u2019ve learned some \u2018other\u2019 sides of infrastructure, what I mean by that is how other people see it. For instance how software people see infrastructure, how stakeholders see infrastructure. I can now look into their way of thinking. Which is really an eye-opener how to approach projects, meetings and documents.\nReflection at the end of the semester:\nLooking back at all the things I\u2019ve done in the semester I must say that I\u2019m proud of myself. I\u2019ve grown a lot on the aspects of responsibilities (buying a wind turbine..), taking responsibilities for wrapping up a project and for assignments for mostly companies.\nI\u2019m proud of the fact that Niek and I convinced OpenRemote to buy a wind turbine for the project. l am glad that we could convince Boudie (building management) as well to put the wind turbine on Strijp-TQ. To see that the different stakeholders have so much trust in this project is very nice to see. I\u2019ll take their reactions into the future with me, as well as that stakeholders can really trust me.\n\nIn my future career, I want to go further with good communication with the stakeholder. I did this in the semester and I would love to take that with me into the next semester. Bi-weekly or maybe every 3 weeks, sprint delivery or a quick chat would do the trick.\nI must admit, that I\u2019m glad that I had the opportunity for doing Delta. Now, I have a lot more opportunities than I can take in my normal course. In the future, I would easily take more opportunities which I think would be cool.\n\nAttachments\nAttachment 1 \u2013 PL2.4\n\n\nAttachment 2 - TI-2.2\n"
        }
      ]
    },
    "DeXDockerBackupfile": {
      "hand-ins": [
        {
          "text": "Version control\n\n\n\nGoal\nThe goal of this document is to describe the assignment that I got from the project leader of DeX. I got the assignment to convert a back-up file for the database. The file for the back-up is currently being run by a cronjob, I need to convert it to a Dockerfile.\nThe reason why I got this assignment is because DeX want to migrate between cloud services. When the file is added to Docker it might be easier to migrate to the cloud service. \nAt first I needed to learn how to use Docker and how I could make any file, since I didn\u2019t had much knowledge about it. \nLooking at the infrastructure I found out that DeX uses the function docker-compose, which means you can multiple containers with 1 command. \nBackup file itself:\nAfter doing some research I got to make a Docker file and it looked as follows:\n\nBack-up piece in the docker compose file:\nLooking at the setup this was the conversion to docker-compose:\n\nSince we needed to use a specific file to send the back-up to openstack, it needed to have a lot of bind-mounts. It needed to have a bind-mount to the script where you could go to the back-up location, as well as a bind-mount to the script to push the back-ups to openstack. When putting all these bind-mounts into the compose file it wasn\u2019t clear at all. It gave a lot of misunderstanding on how the file worked.\n\nAdvise\nBack-up file database\nWhy we should use the dockerized-file?\nWe could use the dockerized file because it would be very fast and easy to migrate to another cloud service.\nWhy we should not use docker for this?\nThe compose files become very unclear, you can\u2019t read the file that easy anymore. Since DeX do want to migrate to docker-swarm it isn\u2019t helpful to use bind-mounts. Since the folder/ file should be present on every swarm node, when this is not the case the whole service could stop working.\nEnd advise \nIn the end I would advise just to keep the cronjob file. Since we are also going to use docker-swarm, this will prevent that we might deal with services who stopped working.\nFor the migration of hosting services we need to keep the cronjob in mind, that\u2019s why I made the following page on the DeX wiki:\n \nOverall back-up\nLooking at the current infrastructure, the only part where back-ups are being made is the database. I would suggest to back-up the volumes of the servers as well. So when for instance the production server crash, you can easily redeploy it with the back-upped volumes."
        }
      ]
    },
    "DeXHostingDocument": {
      "hand-ins": [
        {
          "text": "DeX\nHosting Proposal\n\n\n02.01.2021\n\n\n\nVersion History\n\n\n\nIntroduction\n\nThis document is created to research and conclude what hosting should be used for DeX and how much it will cost.  \nWe will evaluate various hosting options based on a set of requirements, we will conclude with a recommendation on what host to use.\n\n\nRequirements\nWe currently host on servers from OVH with the following specifications:\n1 vCore CPU\n4 GB RAM \n\nIn the past seven months we have pushed over 2000 commits across two different repositories together with over 25 students, teachers and outside collaborators. Seeing as we\u2019re rapidly developing new functionality which requires additional resources (like ElasticSearch), we will be changing these specifications to:\n2 vCore CPU\n8 GB RAM\nThis should provide plenty of room for us to grow, while offering good performance without breaking the bank.\n\nHardware specifications are not everything, when reviewing which host is best for Digital Excellence, we should consider other requirements:\nReliability\nUptime of the servers/network is taken into account.\nPerformance\nDeX\u2019s platform should not be negatively impacted by the hardware that is being used in the stack.\nServer Location\nThe location where the physical server is located. Should be in the EU due to GDPR, The Netherlands is preferred but not required.\nScalability\nIn the case of high traffic, we should be able to scale our services easily.\nCosts\nLower costs are preferred as long as the other requirements are not impacted too much.\nEase of use\nDirect Availability\nHow quickly can we access the system after ordering it?\nDependency\nWith some hosts, we will be required to communicate with system administrators/security officers from organizations; we would prefer if this was not necessary and that we have full control over the system.\n\nAs with all requirements, there are some that are more important than others. Based on my personal experience as a project leader of Digital Excellence, I can safely say that Reliability, Costs, Performance and Dependency are the most important requirements. Scalability is not extremely important at this point as we see a slow increase in traffic rather than spikes in traffic.\nDigital Excellence hosts two environments, one for production and one for staging. Staging is where production is replicated and where testing is done to make sure production is always in a working state. This means that the specifications above are for a single machine/instance and that we should get 2 machines/instances to cover both environments.\n\nMoSCoW\nThe requirements prioritized based on the MoSCoW- method. \nMust have:\n\nReliability\nThe uptime of the servers needs to be at least 95% \nCost \nPreferred lowered cost\nPerformance \nDeX\u2019s platform should not be negatively impacted by the hardware that is being used in the stack.\nDependency\nWith some hosts, we will be required to communicate with system administrators/security officers from organizations; we would prefer if this was not necessary and that we have full control over the system.\n\nShould have:\nServer location\nPreferred in the Netherlands, but if that is not possible in the EU due to the GDPR.\n\nCould have:\nScalability\nNot the highest priority since it is not using that big in production.\nIn the case of high traffic, we should be able to scale our services easily.\n\nWon\u2019t have:\n-\n\n\n\n\nIn the above matrix, there are self-hosted, cloud-hosted and school-provided contestants. \nCosts are costs in euros per month and have been updated in December 2020. The contestants have been chosen based on popularity\n\n* = OVH has medium reliability because it\u2019s known that in the past years, it has had some outages on the network level. In the past year, it seems to have significantly improved though.\n\n** =The ease of use for Fontys Rancher and Natlab is scoring \u2018Low\u2019 due to the restrictions that are put in place by system administrators. The restrictions were observed when we first tried our Rancher at the start of 2020. These restrictions can make an otherwise simple implementation often way more complicated.\n\n***= FHICT has proven in the past that it\u2019s hosting often has downtime and is inaccessible without proper communication to users. With a different Delta project (Quantified Student), they experienced similar reliability and performance issues. DeX has also tried out Rancher at the start and experienced several moments of downtime without being able to get status updates.\n\n\n\nConclusion\nAssuming there is a budget available for DeX and based on the comparison matrix, there are two obvious knockouts; Fontys Rancher and Fontys Natlab. Leaving some external options in the Cloud and some self-hosted ones. If price is not an issue, we highly recommend going with Azure Cloud Hosting. They have high reliability and performance, and are in negotiations with Fontys to set up discounted hosting(source: discussions with Eric Slaats and Martijn Ruissen regarding hosting at the start of 2020). There are also already some contracts signed regarding data storage (GDPR).\nIn case pricing is a problem, we recommend going with OVH if the location does not matter much or with TransIP if the location is a problem. However, the price difference between OVH and TransIP is quite significant and should be considered. \n\n\n\nIn the table above you can see what we recommend for each price range. The prices are per month. \n\n"
        }
      ]
    },
    "DeXInfrastructureDocumentation": {
      "hand-ins": [
        {
          "text": "DeX\nInfrastructure - Github\n\nElmira Drost\n2-12-2020\n\n\n\n\n\n\n\n\n\nIntroduction\nThis document describes and explains briefly how the infrastructure in DeX looks and works.\nThis document is set up in the following sections:\nSummary\nNetwork\nServers\nIn the summary of this document, the infrastructure is briefly explained. In the network section of this document, there is a network design and a part of where the DNS is being hosted.\nAt the end of the document, there is a section of the servers where the settings are being listed, and where for instance the certificates are from.\n\n\nSummary\nThe DeX infrastructure is based on 2 servers, a staging server and a production server. The servers are both configured and based on containerization. For the containerization, Docker is being used.\nOn both of the servers, there is a docker network setup where all is running. The network consists of 3 different networks called the following:\nmssql-network\nproxy\nrabbitmq-network\nOn the network there are different docker containers running for the environment, the following containers are running inside both of the servers:\ndb\n\napi\nidentity\nnotificationservice\nfrontend\n\nIn the database, the information is being stored like usernames which users are using to login into DeX. The rabbitmq is a message broker which is making sure that when you update a project a message is being sent out to the notificationservice where everyone who follows the project gets an email.\nThe API is the link between the frontend and the database, so when you fill in a project or whether you login to the website a call is being made from the frontend to the database.\nThe identity server is used to identify people when logging into DeX, they will perform a check with the database or in other cases with another authorization method in our case the school API.\nThe frontend is hosted on an Nginx server. On this Nginx server, Angular is being hosted. \n\nNetwork\nNetwork design\n\nBoth the Staging server and the production server are connected to the internet. Inside the servers there is a Docker network, the docker network consist of the following VLANS:\nmssql-network\nproxy\nrabbitmq-network\nThe reasons for each network:\n mssql-network\nThe mssql-network is an internal network of Docker, since it isn\u2019t necessary that these services are available from the outside.\nproxy\nThe Proxy is an external network which can be accessible from the outside, through the proxy you can access the other services.\nrabbitmq-network\nThe rabbitmq-network operates on a bridged network, which means it acts on the same ip-address as the host\n\n\nDNS\nPreviously DeX used name.com, since they offered a Github Education package where you can have a domain name free for 1 year. This package expired, since then DeX is using Cloudflare. Cloudflare is being used mainly for their security, it protects the network from critical vulnerabilities and threats, such as DDoS attacks on layer 3 & layer 4. \nCloudflare has built in proxy, which is being used for the production environment as an extra point of security. However, the Staging environment has no proxy because the SSL certificates will get into an infinite loop, since the certificates are also being used on the production environment.\n\n\nServers\nThe DeX infrastructure is based on 2 servers, a staging server and a production server. The staging server stages the production server. Which means it is a kind of test server where we can test the application and other services.\nThe servers are both configured and based on containerization. For the containerization, Docker is being used. The reason why Docker is being used is that Docker is a containerization platform, since we only want to containerize the application Docker will do the trick.  Another reason why we chose Docker instead of Kubernetes is, Docker is meant to be a fast way to deploy a cluster without a lot of complexity, while Kubernetes does the same thing. Only does Kubernetes have more complexity, which is why we chose Docker so everyone can understand it more easily.\n\n\n\n\n\n\nDigital Excellence \nResearch: Database \ntechnologies \n \nStijn Groenen \n7 April 2020 \n \n \t \n \n \n \t \nPreface \nThe goal of this research is to find out which database software is the most suitable for the Digital Excellence platform. \nThe target audience for this research is software engineers and database and software architects. \nThe main question of this research is: \nWhat database software is the most suitable for the Digital Excellence platform? Method: Literature study (Library) \nThe sub-question is: \n\u25cf What type of database is the most suitable for the Digital Excellence platform? \nMethod: Literature study (Library) \n \nWhat type of database is the most suitable for the Digital \nExcellence platform? \nFor this subquestion, multiple database types and some example use cases are described and the most suitable database type for the Digital Excellence platform will be recommended. The following database types will be described: \nDocument \nGraph \nKey-value \u25cf Relational \nSearch engine \nWide column \n \t \n\nDocument \n(\u200bSources: \u200bAmazon Web Services, Inc., n.d.-b; MongoDB, Inc., n.d.-f) \nA document database stores data in documents. These documents are structured similar to JSON objects. \nThe structure of the documents is flexible. They don't have to be defined beforehand. Each document can have a different structure if needed. This makes it easy for a document to evolve over time. \nDocument databases are easier to use for developers because the data can be stored in the same model format they use in their application (For example JSON). \n \nExample use cases \nContent management \nProduct catalogue \n \t \nGraph \n(\u200bSources: \u200bAmazon Web Services, Inc., n.d.-c; MongoDB, Inc., n.d.-f; Neo4j, Inc., n.d.) \nA graph database stores data in nodes and edges. Nodes store information about things, while edges store information about relationships between different nodes. \nGraph databases are very fast in traversing relationships. This is because relationships between nodes are stored in the database instead of being calculated at query time. This makes graph databases excel at use cases where relationships are important. \n \nExample use cases \nRecommendation engine \nSocial networking \nFraud detection \n \t \n\nKey-value \n(\u200bSources: \u200bAmazon Web Services, Inc., n.d.-d; MongoDB, Inc., n.d.-f) \nA key-value database stores data in keys and values. This makes it a simpler type of database. \nThe values in a key-value database can be retrieved by using the key. The key serves as a unique identifier. Both keys and values can be pretty much anything, ranging from simple strings to complex objects. \nKey-value databases are great for storing large amounts of data where you don't need to use complex queries to retrieve it. \nBecause of the simplicity of the data, the database is very scalable. \n \nExample use cases \nSession storage \nShopping cart \nUser preferences \nCaching \n \t \nRelational \n(\u200bSources: \u200bCodecademy, n.d.; Oracle, n.d.) \nA relational database is a type of database that uses structured tables for storing data. \nTables contain rows and columns in which the data is stored. The columns have predefined names and data types which define the structure of a certain table. \nBecause the structure of the data is predefined it is also consistent. Every row in the table has the same properties as the others. \nRelationships can be used to combine and relate data between different tables or rows. \nMost relational databases use SQL (Structured Query Language) to communicate between the database and the back end application. \n \t \n\nSearch engine \n(\u200bSource: \u200bAmazon Web Services, Inc., n.d.-e) \nA search engine database is a type of database that is dedicated to searching the stored data. In most search engine databases, the data is stored as a document (like the document database), but in addition to that, it also stores indexes. These indexes are used to categorize similar characteristics of the data and to provide searching functionality. \nSearch engine databases are optimized for storing and searching a lot of data which may be unstructured. \nSearch engine databases also typically offer features like full-text search, ranked results (based in relevance or other factors) and advanced search queries. \n \nExample use cases \nText search \nLogging and analysis \n \t \nWide column \n(\u200bSources: \u200bAmazon Web Services, Inc., n.d.-a; MongoDB, Inc., n.d.-f) \nA wide column database stores data in tables, rows and dynamic columns. The database is more flexible than relational databases since it is not required to have the same columns for each row in the table. \nWide column databases can be used to store large amounts of data. \nWide column databases are often seen as two-dimensional key-value databases. \n \nExample use cases \nBig Data / Data warehousing \nAnalytics \nInternet-of-things data \nUser profile \n. \t \nConclusion \nSince the primary feature of the Digital Excellence platform is searching for projects, I recommend using a search engine type database. \nSearch engine type databases store data as a document which is a great structure for the projects on the Digital Excellence platform. Search engine type databases also provide a lot of search features out of the box which makes it easier to implement these features into the Digital Excellence platform. \nIf for some reason (For example cost), a search engine type database wouldn't be an option, a document database with good search features could also be used. \n \n \t \n\nWhat database software is the most suitable for the Digital Excellence platform? \nFor the previous subquestion \"What type of database is the most suitable for the Digital Excellence platform?\", two types of databases were recommended. These types are the search engine type and the document type. \nFor this subquestion, some of the most popular search engine databases and document databases are compared based on the following categories: \nSuitability \nHow suitable is the database software for the use case of the Digital Excellence platform? Does the database software provide features to simplify the development of the Digital Excellence platform? \nFlexibility \nHow difficult is it to adapt the database to new features added to the Digital Excellence platform in the future? \nScalability \nHow scalable is the database software? \nEase of use with ASP.NET Core \nThe back end of the Digital Excellence platform is developed in ASP.NET Core. How difficult is it to use the database software in combination with ASP.NET Core? \nCosts \nHow much does the database software cost? \n \n \nNote: \u200bBecause the hosting provider of the Digital Excellence platform has not been selected yet and to keep options open, database technologies that can only be hosted by a specific hosting provider or cloud provider have been left out. \nDocument \nApache CouchDB \nApache CouchDB is a document type database software maintained by the Apache Software Foundation. \u200b(The Apache Software Foundation, n.d.-a) \n \nSuitability \nThe projects in the Digital Excellence platform can be stored as documents. This makes it easy and fast to retrieve the projects without having to do complex queries or join operations. \nCouchDB supports full-text search, but this requires a lot of additional configuration since it is not included out of the box. The full-text search of CouchDB is done using Apache Lucene. \u200b(The Apache Software Foundation, n.d.-c) \n \nFlexibility \nApache CouchDB allows for unstructured or semi-structured data. This makes it easy to change the format of the data in the future without having to change the already existing data. \u200b(The Apache Software Foundation, n.d.-d) \n \nScalability \nApache CouchDB offers both vertical and horizontal scaling. \u200b(The Apache Software Foundation, n.d.-b) \nVertical scaling means that the server on which the database runs is upgraded to allow for more performance or capacity. \nHorizontal scaling means that the database is replicated into multiple servers. This will split the workload over multiple servers which allows for more performance and capacity at a lower cost than vertical scaling. \nApache CouchDB uses an eventual consistency model. This means that the data that is stored on all the different servers, but the updates are not consistent, they are synced. This means that the user can sometimes see the \"old\" data in case it has not yet synced to the other servers. For the \nDigital Excellence platform, this is not an issue. \u200b(The Apache Software Foundation, n.d.-b) \n \nEase of use with ASP.NET Core \nApache CouchDB can be used using a RESTful API. \u200b(The Apache Software Foundation, n.d.-a) \nThere is also an unofficial library available for ASP.NET Core which has support for creating requests. This library is unfortunately very minimal. \u200b(Wertheim, n.d.) \n \nCosts \nApache CouchDB is open-source and free to use. \u200b(The Apache Software Foundation, n.d.-a) \n \nWhen enabling the full-text search feature, Apache Lucene needs to be installed. This will increase system resource usage. \u200b(The Apache Software Foundation, n.d.-c) \n \t \nMongoDB \nMongoDB is document type database software by MongoDB, Inc. Data is stored as documents which are similar to JSON objects. \u200b(MongoDB, Inc., n.d.-e) \n \nSuitability \nThe projects in the Digital Excellence platform can be stored as documents. This makes it easy and fast to retrieve the projects without having to do complex queries or join operations. \nMongoDB also offers built-in text search. This makes it easy to develop a search feature for the projects. \u200b(MongoDB, Inc., n.d.-d) \n \nFlexibility \nMongoDB allows for unstructured or semi-structured data. This makes it easy to change the format of the data in the future without having to change the already existing data. \u200b(MongoDB, Inc., n.d.-e) \n \nScalability \nMongoDB offers both vertical and horizontal scaling. (MongoDB, Inc., n.d.-c) \nVertical scaling means that the server on which the database runs is upgraded to allow for more performance or capacity. \nHorizontal scaling means that the database is replicated into multiple servers. This will split the workload over multiple servers which allows for more performance and capacity at a lower cost than vertical scaling. \n \nEase of use with ASP.NET Core \nThere is an official driver available for ASP.NET Core which makes it easy to use MongoDB in an ASP.NET Core application. The driver has support for models and can automatically convert models into database objects and back using annotations and repository-like pattern. \n(Khandelwal, 2019) \nCosts \nThere are two versions of the MongoDB software. There is a community version which is free to use and there is a paid enterprise version which comes with some additional features and support. \u200b(MongoDB, inc., n.d.) \nThere is also a cloud-hosted option for MongoDB called MongoDB Atlas. The cheapest plan for \nAtlas is free, but this option is limited on performance and expendability. The paid option (including a dedicated cluster instead of a shared one) starts at $57 per month. Atlas also comes with professional support. \u200b(MongoDB, Inc., n.d.-a, n.d.-b) \n \t \n\nCouchbase \nCouchbase is a document type database which was originally created as a merge between CouchDB and Membase. \u200b(Couchbase, n.d.-h) \n \nSuitability \nThe projects in the Digital Excellence platform can be stored as documents. This makes it easy and fast to retrieve the projects without having to do complex queries or join operations. \nCouchbase supports full-text search out of the box. The out of the box full-text search of Couchbase uses Bleve. Bleve is a powerful indexing and full-text search library. \u200b(Couchbase, n.d.-a, n.d.-c, n.d.-h) \n \nFlexibility \nCouchbase allows for unstructured or semi-structured data. This makes it easy to change the format of the data in the future without having to change the already existing data. \u200b(Couchbase, n.d.-h) \n \nScalability \nCouchbase offers both vertical and horizontal scaling. \u200b(Couchbase, n.d.-h) \nVertical scaling means that the server on which the database runs is upgraded to allow for more performance or capacity. \nHorizontal scaling means that the database is replicated into multiple servers. This will split the workload over multiple servers which allows for more performance and capacity at a lower cost than vertical scaling. \n\n\nCouchbase can be used using a RESTful API. \u200b(Couchbase, n.d.-b) \nThere is also an official library available for ASP.NET Core which makes it easy to use Couchbase in an ASP.NET Core application. The library has support for models and can automatically convert models into database objects and back using annotations and repository-like pattern. The library also supports queries using Couchbase's N1QL query language or using .NET's LINQ. (Couchbase, n.d.-d, n.d.-e) \n \nCosts \nCouchbase is open-source and free to use. \u200b(Couchbase, n.d.-h) \nThere is also a cloud-hosted option for Couchbase called Couchbase Cloud. This option is currently in beta. \u200b(Couchbase, n.d.-g) \nThere is also paid professional support available. \u200b(Couchbase, n.d.-f) \n \n \t \nSearch engine \nApache Solr \nApache Solr is a search engine type database based on Apache Lucene. \u200b(The Apache Software Foundation, n.d.-e) \n \nSuitability \nThe main feature of Apache Solr is really advanced full-text searching. Searching is also the most important feature of the Digital Excellence platform. With Apache Solr, it will be much easier to implement a search system with advanced features like relevance, filtering and more. \u200b(The Apache Software Foundation, n.d.-e) \n \nFlexibility \nThese data is an Apache Solr database is schemaless. This makes it easy to change the format of the data in the future without having to change the already existing data. \u200b(The Apache Software Foundation, n.d.-e) \nApache Solr is also extendable using various plugins (For example search suggestions). \u200b(The Apache Software Foundation, n.d.-e) \n \nScalability \nApache Solr supports both vertical and horizontal scaling. \u200b(The Apache Software Foundation, n.d.-e) \nVertical scaling means that the server on which the database runs is upgraded to allow for more performance or capacity. \nHorizontal scaling means that the database is replicated into multiple servers. This will split the workload over multiple servers which allows for more performance and capacity at a lower cost than vertical scaling. \n \n \nApache Solr can be used using a REST-like API. \u200b(The Apache Software Foundation, n.d.-e) \nThere is also an unofficial library available for ASP.NET Core which has support for models and can automatically convert models into database objects and back using annotations and repository-like pattern. \u200b(SolrNet, n.d.) \n \nCosts \nApache Solr is open-source and free to use. \u200b(The Apache Software Foundation, n.d.-e) \nCompared to other databases Apache Solr uses more system resources. This can increase the cost. \u200b(Educba, n.d.) \nSince Apache Solr is primarily a search engine, it is commonly used in addition to another database to combine the search features of Apache Solr with the features that are the primary focus of other databases (like constraints, correctness and robustness). \u200b(Educba, n.d.) \n \nElasticsearch \nElasticsearch is search engine type database software by Elasticsearch B.V.. Elasticsearch is based on Apache Lucene and was first released in 2010. \u200b(Elasticsearch B.V., n.d.-f) \n \nSuitability \nThe main feature of Elasticsearch is really advanced and fast searching. Searching is also the most important feature of the Digital Excellence platform. With Elasticsearch it will be much easier to implement a search system with advanced features like relevance, filtering and more. (Elasticsearch B.V., n.d.-f) \n \nFlexibility \nJust like document type databases, Elasticsearch stores data in documents. These documents can be semi-structured or unstructured. This makes it easy to change the format of the data in the future without having to change the already existing data. \u200b(Elasticsearch B.V., n.d.-f) \nElasticsearch also supports highly optimized analytical queries. This can be used by the Digital Excellence platform to gather analytics and statistics in the future in order to optimize the platform even further. \u200b(Elasticsearch B.V., n.d.-f) \n \nScalability \nElasticsearch supports both vertical and horizontal scaling. \u200b(Elasticsearch B.V., n.d.-b) \nVertical scaling means that the server on which the database runs is upgraded to allow for more performance or capacity. \nHorizontal scaling means that the database is replicated into multiple servers. This will split the workload over multiple servers which allows for more performance and capacity at a lower cost than vertical scaling. \n \n \n \nElasticsearch can be used using a RESTful API. \u200b(Elasticsearch B.V., n.d.-f) \nThere are also 2 official libraries available for ASP.NET Core which makes it easy to use \nElasticsearch in an ASP.NET Core application. \u200b(Elasticsearch B.V., n.d.-d, n.d.-e) \nThere is a low-level driver which can be used to create Elasticsearch requests and responses using C# methods. \u200b(Elasticsearch B.V., n.d.-d) \nThere is also a high-level driver which has support for models and can automatically convert models into database objects and back using annotations and repository-like pattern. (Elasticsearch B.V., n.d.-d, n.d.-e) \n \nCosts \nElasticsearch is free to use. There is an option for paid professional support. \nThere is also a cloud-hosted option that starts at $16 per month. \u200b(Elasticsearch B.V., n.d.-a) \nCompared to other databases Elasticsearch uses more system resources. This can increase the cost. \u200b(Elasticsearch B.V., n.d.-c) \nSince Elasticsearch is primarily a search engine, it is commonly used in addition to another database to combine the search features of Elasticsearch with the features that are the primary focus of other databases (like constraints, correctness and robustness). \u200b(Brasetvik, 2013) \n \t \nComparison matrix \nThe matrix below compares the different database software based on the 5 predefined categories. \nThe database software is ranked using the following options: \nExcellent \nGood \nMedium \nBad \n \n \n \t \n\n\nConclusion \nSince the search engine type databases are commonly used in combination with other databases, my advise is to use a document type database. While the document type databases are very similar I would recommend Couchbase for its good support for ASP.NET Core and for its built-in advanced full-text search features. \nIn the future, it will still be possible to add a search engine database in addition to the existing document type database. This will provide additional more advanced searching capabilities and performance. It will also allow for additional analysis and statistics. \n \t \n\nReferences \nAmazon Web Services, Inc. (n.d.-a). What is a Columnar Database? Retrieved from https://aws.amazon.com/nosql/columnar/ \n \nAmazon Web Services, Inc. (n.d.-b). What Is a Document Database? Retrieved from https://aws.amazon.com/nosql/document/ \n \nAmazon Web Services, Inc. (n.d.-c). What Is a Graph Database? Retrieved from https://aws.amazon.com/nosql/graph/ \n \nAmazon Web Services, Inc. (n.d.-d). What Is a Key-Value Database? Retrieved from https://aws.amazon.com/nosql/key-value/ \n \nAmazon Web Services, Inc. (n.d.-e). What Is a Search-Engine Database? Retrieved from https://aws.amazon.com/nosql/search/ \n \nBrasetvik, A. (2013, September 15). Elasticsearch as a NoSQL Database. Retrieved from https://www.elastic.co/blog/found-elasticsearch-as-nosql \n \nCodecademy. (n.d.). What is a Relational Database Management System? Retrieved from https://www.codecademy.com/articles/what-is-rdbms-sql \n \nCouchbase. (n.d.-a). Bleve. Retrieved from https://blevesearch.com/ \n \nCouchbase. (n.d.-b). Compare Couchbase vs CouchDB NoSQL Systems. Retrieved from https://www.couchbase.com/comparing-couchbase-vs-couchdb \n \nCouchbase. (n.d.-c). Full Text Search: Fundamentals. Retrieved from https://docs.couchbase.com/server/current/fts/full-text-intro.html \n \nCouchbase. (n.d.-d). Install and Start Using the .NET SDK with Couchbase Server. Retrieved from https://docs.couchbase.com/dotnet-sdk/2.7/start-using-sdk.html \n \nCouchbase. (n.d.-e). .NET Sample App Backend Tutorial. Retrieved from https://docs.couchbase.com/dotnet-sdk/2.7/sample-app-backend.html \n \nCouchbase. (n.d.-f). Professional Services. Retrieved from https://www.couchbase.com/professional-services \n \nCouchbase. (n.d.-g). What Is DBaaS? \u2013 Database-as-a-Service Provider. Retrieved from https://www.couchbase.com/products/cloud \n \nCouchbase. (n.d.-h). Why Couchbase? Retrieved from https://docs.couchbase.com/server/current/introduction/intro.html \n \nEducba. (n.d.). What is Apache Solr? Retrieved from https://www.educba.com/apache-solr/ \n \nElasticsearch B.V. (n.d.-a). Elastic Cloud: Hosted Elasticsearch, Hosted Search. Retrieved from https://www.elastic.co/cloud/ \n \nElasticsearch B.V. (n.d.-b). Elasticsearch: The Official Distributed Search & Analytics Engine. Retrieved from https://www.elastic.co/elasticsearch/ \n \nElasticsearch B.V. (n.d.-c). Hardware | Elasticsearch: The Definitive Guide [2.x]. Retrieved from https://www.elastic.co/guide/en/elasticsearch/guide/current/hardware.html \n \nElasticsearch B.V. (n.d.-d). Introduction | Elasticsearch.Net and NEST: the .NET clients [7.x]. Retrieved from https://www.elastic.co/guide/en/elasticsearch/client/net-api/current/introduction.html \n \nElasticsearch B.V. (n.d.-e). NEST - High level client. Retrieved from https://www.elastic.co/guide/en/elasticsearch/client/net-api/current/nest.html \n \nElasticsearch B.V. (n.d.-f). What is Elasticsearch? Retrieved from https://www.elastic.co/what-is/elasticsearch \n \nKhandelwal, P. (2019, August 17). Create a web API with ASP.NET Core and MongoDB. Retrieved from https://docs.microsoft.com/en-us/aspnet/core/tutorials/first-mongo-app?view=aspnetcore-3.1 \n \nMongoDB, Inc. (n.d.-a). Managed MongoDB Hosting | Database-as-a-Service. Retrieved from https://www.mongodb.com/cloud/atlas \n \nMongoDB, inc. (n.d.). MongoDB Enterprise Advanced. Retrieved from https://www.mongodb.com/products/mongodb-enterprise-advanced \n \nMongoDB, Inc. (n.d.-b). Pricing. Retrieved from https://www.mongodb.com/pricing \n \nMongoDB, Inc. (n.d.-c). Sharding \u2014 MongoDB Manual. Retrieved from https://docs.mongodb.com/manual/sharding/ \n \nMongoDB, Inc. (n.d.-d). Text Search \u2014 MongoDB Manual. Retrieved from https://docs.mongodb.com/manual/text-search/ \n \nMongoDB, Inc. (n.d.-e). What Is MongoDB? Retrieved from https://www.mongodb.com/what-is-mongodb \n \nMongoDB, Inc. (n.d.-f). What is NoSQL? NoSQL Databases Explained. Retrieved from https://www.mongodb.com/nosql-explained \n \nNeo4j, Inc. (n.d.). What is a Graph Database? Retrieved from https://neo4j.com/developer/graph-database/ \n \nOracle. (n.d.). What is a relational database? Retrieved from https://www.oracle.com/database/what-is-a-relational-database/ \n \nSolrNet. (n.d.). SolrNet/SolrNet. Retrieved from https://github.com/SolrNet/SolrNet \n \nThe Apache Software Foundation. (n.d.-a). Apache CouchDB. Retrieved from https://couchdb.apache.org/ \n \nThe Apache Software Foundation. (n.d.-b). Apache CouchDB Documentation - Eventual \nConsistency. Retrieved from https://docs.couchdb.org/en/master/intro/consistency.html \n \nThe Apache Software Foundation. (n.d.-c). Apache CouchDB Documentation - Search. Retrieved from https://docs.couchdb.org/en/master/ddocs/search.html \n \nThe Apache Software Foundation. (n.d.-d). Apache CouchDB Documentation - Technical \nOverview. Retrieved from https://docs.couchdb.org/en/stable/intro/overview.html \n \nThe Apache Software Foundation. (n.d.-e). Apache Solr. Retrieved from https://lucene.apache.org/solr/features.html \n \nWertheim, D. (n.d.). danielwertheim/mycouch. Retrieved from https://github.com/danielwertheim/MyCouch \n \n\n\n\nAdvise Message Brokers\nFor a notification system implementation for the DeX platform\n\n\n\n\n\n\n\n\n\n\n\n\n\nMax van Hattum, Martin Markov\n5-10-2020\n\n\nTable of Contents\n\nVersion History\n\n\n\nIntroduction\nFor the DeX platform it will be necessary to send users notifications by means of different message services. For this to be released one component is needed to manage notifications intended for different subscribers. This component should not only store the data to be sent but should also be responsible for load balancing and making sure messages are delivered. \nMany existing services implement a so-called message broker service to handle this. (Google, z.d.)\nResearch questions\nMain question\nTo give direction to this research a main question to be answered will be formulated. There are several aspects that need to be kept in mind when researching what the preferred solution for the DeX platform will be. The solution should be secure, scalable, functional, and compatible with the current system. Keeping these criteria in mind results in the following research question: \u201cWhat is a secure, scalable and compatible message broker service best suited for the DeX platform?\u201d\nSub questions\nTo get to the answer to this question, the subject will be divided in multiple smaller subjects. The following questions will be answered:\nWhat does the DeX platform want to achieve?\nWhat is a message broker?\nWhy use a message broker service?\nWhat are important aspects of a message broker service?\nWhat are currently available message broker services?\nHow do currently available message broker services compare?\n\n\nDOT Framework\nWhat\nThis research is going to take place in both the application domain and the available work domain. The current software system of the DeX Platform needs to be analysed to get the best service fit for the context. However, before this can be done research needs to be done about message broker (services) in general so that a better understanding can be achieved, allowing us to find the best fit for the current context. \n\nHow\nTo get an overview of what exactly we are trying to achieve we first applied the Field method in a talk with one of the stakeholders and project leader Brend Smids and another team member Niray Mak. Then we focused heavily on the Library method to gather existing information and expertise regarding message brokers. Finally, we compared the gathered information, and setup prototypes for the most promising options which belongs to the Showroom research.\n\nWhy\nThese methods were chosen because we want to attain a good overview of what is needed for the DeX Platform and how existing work might already solve the issue. When this is clear we can focus on achieving the best fit for the context, leaving room for potential improvements.\n\n\nWhat does the DeX platform want to achieve?\nTo get a grip on what the DeX Platform wants, a meeting was scheduled with Brend Smits, Niray Mak, Martin Markov and Max van Hattum. There Brend explained that currently there is no way to send notifications to users. He wanted a scalable solution for this keeping in mind that maybe in the future multiple ways of sending notifications are going to be used.\nHe went on to describe how currently the architecture of the REST API is a monolithic structure, but how they might want to gradually change to a more microservice oriented architecture. He wants a solution where a service in the API can register notifications to a message broker, and then let the message broker handle things from that point, distributing it to a specified notification service. \nMoreover, he stated that he would prefer it if this was all locally hosted, using Docker and that the solution should be simple to use. \n\nWhat is a message broker?\nA message broker is sometimes also called Integration Broker or interface engine. It is a service that minimally message transformation and routing services, communication program to program. (Gartner, z.d.)\nMost often it is able to store messages, keep track of which messages need to be delivered and balance the load of delivering messages.\n \nWhy use a message broker?\nMessage brokers make the process of data exchange simple and reliable. They use different protocols that show how the message should be transmitted, processed and consumed. They allow asynchronous communication which allows both the producer and the consumer to interact directly with the message broker and not between each other. While a producer can enqueue new messages a consumer may read from it simultaneously without blocking it. Message brokers also allow us to better scale the communication between different services on demand.\n\nIn the context of DeX, we are planning to use a message broker so we can process notifications to our users asynchronously. The producer of the eventual message broker will be our API which will enqueue the notification message, how it should be sent (ex. via email), to which user and when should it be sent. The consumer will be the service responsible for sending the message.\n\nWhat are important aspects of a message broker service?\nForemost the message broker service should be able to validate, transform and route messages. The main goal of the message broker architecture is decoupling programs while facilitating communication between these programs, while keeping them unaware of each other (Ejsmont, 2015, pp. 275\u2013276). \nMoreover cost, scalability, compatibility, hosting, ease of use and speed are important subjects to research.\nWhat are currently available message broker services?\nAWS SQS\n1 million requests free\nHosted by Amazon\nCompatible with .NET\nGood documentation \nOfficial Library available \nGoogle Cloud Pub/Sub\n10GB gratis, 40$ per 1TiB after\nHosted by google\nCompatible with .NET\nGreat documentation: \nOfficial Library available\nRabbitMQ\nOwn hosting, hosting specific pricing\nGood documentation: \nOfficial Library available for .NET\nOpen source\nBig community\nPulsar 2.0\nOwn hosting, hosting specific pricing\nGreat documentation: \nPorted library available for ASP.NET\nOpen source\n\nApache Kafka\nOwn hosting, hosting specific pricing\nGood documentation: \nCommunity library available for .NET\nOpen source\n\n\n\nComparing the important aspects of these services\nTo compare the services, we take into account several aspects; ease of use, scalability, hosting, cost, support and if the service is open source. The project leader communicated that own hosting and ease of use are the most important aspects. Cost certainly plays an important role too. \n\n\n\n\nVerifying requirements with a prototype\nTo verify that the requirements are being met and to analyse the resource usage by the service, we set up a local demo. \nRabbitMQ\nThe system requirements are not high, a minimum of 256mb of RAM always needs to be free and at least 50MB of disk space must always be available to prevent failures. (RabbitMQ, z.d.)\nInstallation for a demo is simple when using docker:\ndocker run -it --rm --name rabbitmq -p 5672:5672 -p 15672:15672 rabbitmq:3-management\nThis includes a monitoring tool (not fit for production, but other tools are available for this). Running this exposes the rabbitmq service on the default port and gives access to the monitor tool with credentials: guest - guest.\nTo validate that the service fulfils the needs of the system and to monitor system usage it is necessary to set up a publisher and one or more clients. The publisher will register multiple messages, the clients will consume them.\nSince the clients will be doing a task based on a message, we have incorporated the competing consumer pattern. A consumer will only get a new task when it is finished with another.\nTwo windows console apps are created, one for publishing one thousand messages and one for receiving. \nConsumer\n\n\n\n\nProducer\n\n\n\n\nResult\nStart up two workers by entering dotnet run  into a console opened in the folder where the worker app resides. Then start up one publisher with a message with dotnet run Message:.. \nThe number of dots represent the amount of seconds this task takes to run.\nThe broker now adds one thousand messages to the queue and sends them one by one to connected consumers. It only sends a new message after it has received acknowledgment from the worker that the task is completed.  \n\nAs you can see the messages are distributed between the workers. When checking out the monitoring tool for ram and disk space usages, the values stay low. \n\nBefore registering messages\n\n\nAfter registering messages with 2 seconds delay per task\n\n\nAfter subscribing six more workers\n\n\nApache Kafka\nWhile there are not official system requirements on the official documentation, Confluent, the most famous platform to use Kafka with, is recommending a minimum of 6GB RAM.  \nTo run Kafka locally we have to run Apache ZooKeeper alongside with it for maintaining the configuration information. To ease the process of local configuration and ensure that the correct services are used we are going to use Docker and docker-compose. We have set-up the following \ndocker-compose.yml\n\n\n\nThen to run it we are going to execute docker-compose up .\n\nFrom this image it can be seen that the configuration of the Kafka is not as straight forward as the one for running RabbitMQ. Also, there is not a monitoring tool coming out-of-the-box as the one that RabbitMQ provides, so the performance is not easily measurable.\n\nTo produce the same example as with RabbitMQ we are going to implement a pub-sub communication.\nProducer\n\n\nConsumer\n\n\nResult\n\n\nIn the result we can see that we launched one producer with two consumer instances which read from the same Kafka topic.\nFinal comparison\nWhile both tools could be used as a message broker, it appears that RabbitMQ is the better solution for DeX as it is only a message broker and it is doing its job really efficient. Kafka is a great platform, but it adds plenty of overhead when used only as a message broker which is the use case of DeX. Plus, the system requirements are much higher compared to RabbitMQ.\n\nConclusion\nWe conclude that based on the comparison of the different technologies and the research, the selected technology to use for the message broker of the DeX notification system is RabbitMQ. We advise this service because it supports its own hosting, has good integration for .NET Core and has an active community surrounding the service.\n\nReferences\nEjsmont, A. (2015). Web Scalability for Startup Engineers. McGraw-Hill Education.\nGartner. (z.d.). Definition of IB (Integration Broker) - Gartner Information Technology Glossary. Geraadpleegd 5 oktober 2020, van https://www.gartner.com/en/information-technology/glossary/ib-integration-broker\nGoogle. (z.d.). Cloud Pub/Sub |. Google Cloud. Geraadpleegd 5 oktober 2020, van https://cloud.google.com/pubsub#customers"
        }
      ]
    },
    "OpenRemoteBalenaAPIcalls": {
      "hand-ins": [
        {
          "text": "\nVersion control\n\n\nResearch\nWhen using OpenBalena it is hard to get information on the devices. Which release they run, the CPU usage, if they are online or offline etc. With this document we want to simplify that process.\nGoal\nThe goal of the research and document is to get releases of software that is running on IoT devices. OpenRemote wants to use OpenBalena in order to update and orchestrate IoT devices, however if they don\u2019t know which version of their software is running on their IoT devices OpenBalena does not make sense. This we want to tackle with the Balena API, with the needed research we want to see if  we can get the information which release the device is running through the API.\nSetup of the calls\nAPI calls are the way to get information about the OpenBalena server. The balena API uses the Open Data Protocol ()\nThese parts support the Balena API:\nGET: view information about a resource\nPOST: create a new resource\nPATCH: modify an existing resource\nDELETE: remove a resource\nIn our case we want to view information from the API so we should use the GET part of the API call.\nLooking at the API documentation from Balena, we can see how APIs are being built up:\nThis is the original API call for getting the application name and device type for each application, this is also seen as the \u201ctest\u201d API call.\ncurl -X GET \"https://api.balena-cloud.com/v5/application?\\$select=app_name,device_type\" \\ \n-H \"Content-Type: application/json\" \\ \n-H \"Authorization: Bearer <AUTH_TOKEN>\"\n---------\nWe can see that the calls consists of 3 parts:\nCurl \u2013 get\t https://link.com/...\nIn this field you can fill in the needed information that you want to request.\n-H content type\nIn which language and type the API call will reply\nAuthorization \nHere you need to fill in the token from OpenBalena, without the token you can\u2019t get any data out of the API\nMaking of an API key\nTo make an API-key you need to fill in the following command:\nbalena api-key generate Example\nRegistered api key 'Example':\nAPI-KeyIsListedHere\n\nPotential API calls\nIn this chapter I will list all the potential API calls that we can use in our project.\nGet the release device is pinned to\ncurl -X GET \\\n\"https://api.balena-cloud.com/v5/device(<ID>)?\\$select=should_be_running__release\" \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer <AUTH_TOKEN>\"\n\nThis API call can be useful to see which release the specific device should run, for instance the newest version of the application is being pushed to the devices. You can check if the device received the new version with the should be running release\n\n\nGet current release running on device\nThe is_on__commit field contains the hash of the release currently deployed to the device.\ncurl -X GET \\\n\"https://api.balena-cloud.com/v5/device(<ID>)?\\$select=is_on__commit\" \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer <AUTH_TOKEN>\"\n\nThis API call can be useful to see which release is currently running on 1 specific device. With this API call we can achieve the goal of the research. Because this API call gives the current release of a device.\n\n\n\nGet application by id along with its devices\ncurl -X GET \\\n\"https://api.balena-cloud.com/v5/application(<ID>)?\\$expand=owns__device\" \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer <AUTH_TOKEN>\"\n\nThis API call gives the information of an application. For instance, the application is running the OpenRemote software, with this command we can see which devices are connected to the OpenRemote application.\n\n\nAPI Calls\nIn this chapter I will describe which API call we need to get the right information. After checking and doing some research the following API calls will return which release of the application is in use:\nThe best and easiest way to get the releases of all devices will be the following command:\ncurl -X GET  \"https://api.balena.openremote.io/v6/device\" -H \"Content-Type: application/json\"  -H \"Authorization: Bearer e0fTlXMl9SvleddMuxRkCR9596hJEyW3\"\n\nFor getting just a release from 1 device you could use the following command:\ncurl -X GET \"https://api.balena.openremote.io/v6/device(2)?\\$select=is_running__release\"  -H \"Content-Type: application/json\"   -H \"Authorization: Bearer e0fTlXMl9SvleddMuxRkCR9596hJEyW3\"\nWith these API calls we can get the needed results for OpenRemote and tackle the problem for the usage of OpenBalena. Because this API call can give the release version of devices, without the calls it wouldn\u2019t make sense to use OpenBalena for OpenRemote.\n\nResult\nThe API call of getting the devices and their info will output the following per device:\n\"d\":[\n{\n\"created_at\":\"2020-12-10T14:31:10.677Z\",\n\"modified_at\":\"2020-12-15T10:19:16.778Z\",\n\"id\":2,\n\"actor\":6,\n\"api_heartbeat_state\":\"online\",\n\"uuid\":\"e568bc8782ff91eaf60fcb8e14b7abe4\",\n\"local_id\":null,\n\"device_name\":\"quiet-sun\",\n\"note\":null,\n\"is_of__device_type\":{\n\"__id\":29\n},\n\"belongs_to__application\":{\n\"__id\":2\n},\n\"is_online\":true,\n\"last_connectivity_event\":\"2020-12-10T14:32:52.588Z\",\n\"is_connected_to_vpn\":true,\n\"last_vpn_event\":\"2020-12-10T14:32:52.588Z\",\n\"is_locked_until__date\":null,\n\"logs_channel\":null,\n\"public_address\":null,\n\"vpn_address\":\"100.64.0.4\",\n\"ip_address\":\"192.168.178.150\",\n\"mac_address\":\"DC:A6:32:D1:A3:B9 66:C5:88:90:CC:6F\",\n\"memory_usage\":2469,\n\"memory_total\":7864,\n\"storage_block_device\":\"/dev/mmcblk0\",\n\"storage_usage\":1375,\n\"storage_total\":6742,\n\"cpu_usage\":25,\n\"cpu_temp\":55,\n\"is_undervolted\":false,\n\"cpu_id\":\"100000005f78925\",\n\"is_running__release\":{\n\"__id\":2\n},\n\"download_progress\":null,\n\"status\":\"Idle\",\n\"os_version\":\"balenaOS 2.58.6+rev1\",\n\"os_variant\":\"dev\",\n\"supervisor_version\":\"11.14.0\",\n\"provisioning_progress\":null,\n\"provisioning_state\":\"\",\n\"api_port\":48484,\n\"api_secret\":\"ed9386a609269dc4eed9269fe7955a52\",\n\"is_managed_by__service_instance\":{\n\"__id\":1\n},\n\"should_be_running__release\":null,\n\"is_managed_by__device\":null,\n\"is_web_accessible\":false,\n\"overall_status\":\"idle\",\n\"overall_progress\":null\n}\nWhile looking at the output of the API call we could use this the best, it will give the information from all the devices. \nIn our case we are interested in the \u201cis_running__release\u201d  this is showing us which release the device is currently working on. By release we mean the release of the application that the device is currently running. For instance the first push of the OpenRemote software would running release 1 and the update for the software would be running release 2.\n\n\n\n\n\n\n\n\n\n\n\n"
        }
      ]
    },
    "OpenRemoteBalenaCloudformationOpenBalena": {
      "hand-ins": [
        {
          "text": "\n\nVersion control\n\n\n\nIntroduction\nThis document is set up for the description of the Cloudformation script, this document can make sure that the purpose of the script is clear, as well as it functionality. The script can be used by OpenRemote to set up the OpenBalena server. \nThis document will contain the following items:\nWhy is it needed?\nWhat is the functionality of the script?\nThe code of the script\nThe result when running the script\n\nWhy is the script needed?\nThis script is needed so OpenRemote can easily deploy OpenBalena on Amazon Web Services. With this script they can easily deploy, automate the roll-out and after that manage IoT devices. OpenRemote can also use it for their company to help others deploy the OpenBalena server. \nThis script can realize the requirements for the OpenBalena server, based on the needs of OpenRemote. The needs of OpenRemote are that they can orchestrate the IoT devices, but also that it is done in a secure and automatic way.\nWhat is the functionality?\nThis script can be used to automate the deployment of OpenBalena. This script will make a server where OpenBalena is running. \nThe script can be adjusted so it will fit within the environment of everyone. If someone else instead of OpenRemote want to deploy OpenBalena they will need to change the following variables:\nDNS domain\nHostname\nKeypairs\nDNS records\nThey will need to adjust the UserData line where OpenBalena is configured based on mail address, password and domain name. Which is the following line:\nsudo\u00a0bash\u00a0/home/ubuntu/open-balena/scripts/quickstart\u00a0-U\u00a0balena@example.com\u00a0-P\u00a0PasswordHere\u00a0-d\u00a0balena.openremote.io\n\n\n\nCode\n---\nAWSTemplateFormatVersion:\u00a0'2010-09-09'\nDescription:\u00a0'Template\u00a0to\u00a0install\u00a0OpenBalena\u00a0on\u00a0AWS\u00a0environment\u00a0on\u00a0ubuntu'\nParameters:\n\u00a0\u00a0DomainName:\n\u00a0\u00a0\u00a0\u00a0Description:\u00a0DNS\u00a0domain\u00a0for\u00a0created\u00a0stack\u00a0(clear\u00a0it\u00a0is\u00a0you\u00a0want\u00a0the\u00a0deployment\u00a0with\u00a0an\u00a0insecure\u00a0SSL\u00a0certificate)\n\u00a0\u00a0\u00a0\u00a0Type:\u00a0String\n\u00a0\u00a0\u00a0\u00a0Default:\u00a0balena.openremote.io\n\u00a0\u00a0HostName:\n\u00a0\u00a0\u00a0\u00a0Description:\u00a0Hostname\u00a0of\u00a0the\u00a0balenaserver\n\u00a0\u00a0\u00a0\u00a0Type:\u00a0String\n\u00a0\u00a0\u00a0\u00a0Default:\u00a0openbalena\n\u00a0\u00a0HostedZone:\n\u00a0\u00a0\u00a0\u00a0Description:\u00a0Use\u00a0AWS\u00a0hosted\u00a0zone\n\u00a0\u00a0\u00a0\u00a0Type:\u00a0String\n\u00a0\u00a0\u00a0\u00a0Default:\u00a0true\n\u00a0\u00a0\u00a0\u00a0AllowedValues:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0true\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0false\n\u00a0\u00a0KeyName:\n\u00a0\u00a0\u00a0\u00a0Description:\u00a0Name\u00a0of\u00a0an\u00a0existing\u00a0EC2\u00a0KeyPair\u00a0to\u00a0enable\u00a0SSH\u00a0access\u00a0to\u00a0the\u00a0instance\n\u00a0\u00a0\u00a0\u00a0Type:\u00a0AWS::EC2::KeyPair::KeyName\n\u00a0\u00a0\u00a0\u00a0Default:\u00a0openbalena\n\u00a0\u00a0\u00a0\u00a0ConstraintDescription:\u00a0must\u00a0be\u00a0the\u00a0name\u00a0of\u00a0an\u00a0existing\u00a0EC2\u00a0KeyPair.\n\u00a0\u00a0InstanceType:\n\u00a0\u00a0\u00a0\u00a0Description:\u00a0EC2\u00a0instance\u00a0type\u00a0(don't\u00a0change\u00a0it\u00a0unless\u00a0you\u00a0know\u00a0what\u00a0you\u00a0are\u00a0doing)\n\u00a0\u00a0\u00a0\u00a0Type:\u00a0String\n\u00a0\u00a0\u00a0\u00a0#\u00a0AWS\u00a0Compute\u00a0Optimizer\u00a0reports\u00a0that\u00a0this\u00a0instance\u00a0type\u00a0is\u00a0under-provisioned\u00a0and\u00a0advise\u00a0to\u00a0use\n\u00a0\u00a0\u00a0\u00a0#\u00a0more\u00a0than\u00a0double\u00a0the\u00a0price\u00a0instance.\u00a0However,\u00a0it\u00a0works\u00a0in\u00a0our\u00a0test\u00a0setup.\n\u00a0\u00a0\u00a0\u00a0Default:\u00a0t3a.small\n\u00a0\u00a0\u00a0\u00a0ConstraintDescription:\u00a0must\u00a0be\u00a0a\u00a0valid\u00a0EC2\u00a0instance\u00a0type.\n\u00a0\u00a0InstanceAMI:\n\u00a0\u00a0\u00a0\u00a0Description:\u00a0Managed\u00a0AMI\u00a0ID\u00a0for\u00a0EC2\u00a0Instance\u00a0(don't\u00a0change\u00a0it\u00a0unless\u00a0you\u00a0know\u00a0what\u00a0you\u00a0are\u00a0doing)\n\u00a0\u00a0\u00a0\u00a0Type\u00a0:\u00a0String\n\u00a0\u00a0\u00a0\u00a0Default:\u00a0ami-0b5247d4d01653d09\nConditions:\n\u00a0\u00a0DnsRecordCreate:\u00a0!Equals\u00a0[!Ref\u00a0HostedZone,\u00a0true]\n\u00a0\u00a0DomainName:\u00a0!Not\u00a0[\u00a0!Equals\u00a0[!Ref\u00a0DomainName,\u00a0\"\"]\u00a0]\nOutputs:\n\u00a0\u00a0\u00a0\u00a0InstanceIP:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Description:\u00a0The\u00a0Instance\u00a0public\u00a0IP\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Value:\u00a0!GetAtt\u00a0EC2Instance.PublicIp\n\u00a0\u00a0\u00a0\u00a0PublicUrl:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Condition:\u00a0DomainName\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Description:\u00a0openbalena\u00a0Instance\u00a0URL\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Value:\u00a0!Join\u00a0[\u00a0\".\",\u00a0[\u00a0!Ref\u00a0HostName,\u00a0!Ref\u00a0DomainName\u00a0]\u00a0]\nResources:\n\u00a0\u00a0DNSRecord:\n\u00a0\u00a0\u00a0\u00a0Condition:\u00a0DnsRecordCreate\n\u00a0\u00a0\u00a0\u00a0Type:\u00a0AWS::Route53::RecordSet\n\u00a0\u00a0\u00a0\u00a0Properties:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0HostedZoneName:\u00a0!Join\u00a0[\u00a0\"\",\u00a0[\u00a0!Ref\u00a0DomainName,\u00a0\".\"\u00a0]\u00a0]\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Name:\u00a0!Ref\u00a0DomainName\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0TTL:\u00a0'60'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Type:\u00a0A\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0ResourceRecords:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0!GetAtt\u00a0EC2Instance.PublicIp\n\u00a0\u00a0DNSRecord2:\n\u00a0\u00a0\u00a0Condition:\u00a0DnsRecordCreate\n\u00a0\u00a0\u00a0Type:\u00a0AWS::Route53::RecordSet\n\u00a0\u00a0\u00a0Properties:\n\u00a0\u00a0\u00a0\u00a0\u00a0HostedZoneName:\u00a0!Join\u00a0[\u00a0\"\",\u00a0[\u00a0!Ref\u00a0DomainName,\u00a0\".\"]\u00a0]\n\u00a0\u00a0\u00a0\u00a0\u00a0Name:\u00a0api.balena.openremote.io.\n\u00a0\u00a0\u00a0\u00a0\u00a0TTL:\u00a0'60'\n\u00a0\u00a0\u00a0\u00a0\u00a0Type:\u00a0CNAME\n\u00a0\u00a0\u00a0\u00a0\u00a0ResourceRecords:\n\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0!GetAtt\u00a0EC2Instance.PublicDnsName\n\u00a0\u00a0DNSRecord3:\n\u00a0\u00a0\u00a0Condition:\u00a0DnsRecordCreate\n\u00a0\u00a0\u00a0Type:\u00a0AWS::Route53::RecordSet\n\u00a0\u00a0\u00a0Properties:\n\u00a0\u00a0\u00a0\u00a0\u00a0HostedZoneName:\u00a0!Join\u00a0[\u00a0\"\",\u00a0[\u00a0!Ref\u00a0DomainName,\u00a0\".\"]\u00a0]\n\u00a0\u00a0\u00a0\u00a0\u00a0Name:\u00a0vpn.balena.openremote.io.\n\u00a0\u00a0\u00a0\u00a0\u00a0TTL:\u00a0'60'\n\u00a0\u00a0\u00a0\u00a0\u00a0Type:\u00a0CNAME\n\u00a0\u00a0\u00a0\u00a0\u00a0ResourceRecords:\n\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0!GetAtt\u00a0EC2Instance.PublicDnsName\n\u00a0\u00a0DNSRecord4:\n\u00a0\u00a0\u00a0Condition:\u00a0DnsRecordCreate\n\u00a0\u00a0\u00a0Type:\u00a0AWS::Route53::RecordSet\n\u00a0\u00a0\u00a0Properties:\n\u00a0\u00a0\u00a0\u00a0\u00a0HostedZoneName:\u00a0!Join\u00a0[\u00a0\"\",\u00a0[\u00a0!Ref\u00a0DomainName,\u00a0\".\"]\u00a0]\n\u00a0\u00a0\u00a0\u00a0\u00a0Name:\u00a0registry.balena.openremote.io.\n\u00a0\u00a0\u00a0\u00a0\u00a0TTL:\u00a0'60'\n\u00a0\u00a0\u00a0\u00a0\u00a0Type:\u00a0CNAME\n\u00a0\u00a0\u00a0\u00a0\u00a0ResourceRecords:\n\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0!GetAtt\u00a0EC2Instance.PublicDnsName\n\u00a0\u00a0DNSRecord5:\n\u00a0\u00a0\u00a0Condition:\u00a0DnsRecordCreate\n\u00a0\u00a0\u00a0Type:\u00a0AWS::Route53::RecordSet\n\u00a0\u00a0\u00a0Properties:\n\u00a0\u00a0\u00a0\u00a0\u00a0HostedZoneName:\u00a0!Join\u00a0[\u00a0\"\",\u00a0[\u00a0!Ref\u00a0DomainName,\u00a0\".\"]\u00a0]\n\u00a0\u00a0\u00a0\u00a0\u00a0Name:\u00a0s3.balena.openremote.io.\n\u00a0\u00a0\u00a0\u00a0\u00a0TTL:\u00a0'60'\n\u00a0\u00a0\u00a0\u00a0\u00a0Type:\u00a0CNAME\n\u00a0\u00a0\u00a0\u00a0\u00a0ResourceRecords:\n\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0!GetAtt\u00a0EC2Instance.PublicDnsName\n\u00a0\u00a0EC2Instance:\n\u00a0\u00a0\u00a0\u00a0Type:\u00a0AWS::EC2::Instance\n\u00a0\u00a0\u00a0\u00a0Properties:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0InstanceType:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Ref:\u00a0InstanceType\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0KeyName:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Ref:\u00a0KeyName\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0ImageId:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Ref:\u00a0InstanceAMI\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0SecurityGroups:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0Ref:\u00a0InstanceSecurityGroup\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Tags:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0Key:\u00a0\"Name\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Value:\u00a0\"openbalena\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0UserData:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Fn::Base64:\u00a0!Sub\u00a0|\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0#!/bin/bash\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0apt-get\u00a0update\u00a0&&\u00a0apt-get\u00a0install\u00a0-y\u00a0build-essential\u00a0git\u00a0docker.io\u00a0libssl-dev\u00a0nodejs\u00a0npm\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0curl\u00a0-L\u00a0https://github.com/docker/compose/releases/download/1.27.4/docker-compose-Linux-x86_64\u00a0-o\u00a0/usr/local/bin/docker-compose\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0chmod\u00a00755\u00a0/usr/local/bin/docker-compose\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0usermod\u00a0-a\u00a0-G\u00a0sudo\u00a0ubuntu\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0usermod\u00a0-a\u00a0-G\u00a0docker\u00a0ubuntu\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0su\u00a0ubuntu\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0cd\u00a0/home/ubuntu\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0git\u00a0clone\u00a0https://github.com/balena-io/open-balena.git\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0cd\u00a0/home/ubuntu/open-balena\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0docker-compose\u00a0--version\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0sleep\u00a05\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0sudo\u00a0bash\u00a0/home/ubuntu/open-balena/scripts/quickstart\u00a0-U\u00a0balena@example.com\u00a0-P\u00a0PasswordHere\u00a0-d\u00a0balena.openremote.io\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0sleep\u00a05\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0sudo\u00a0systemctl\u00a0start\u00a0docker\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0sleep\u00a05\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0sudo\u00a0bash\u00a0/home/ubuntu/open-balena/scripts/compose\u00a0up\u00a0-d\n\u00a0\u00a0InstanceSecurityGroup:\n\u00a0\u00a0\u00a0\u00a0Type:\u00a0AWS::EC2::SecurityGroup\n\u00a0\u00a0\u00a0\u00a0Properties:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0GroupDescription:\u00a0SSH\u00a0+\u00a0HTTP\u00a0+\u00a0HTTPS\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0SecurityGroupIngress:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0IpProtocol:\u00a0tcp\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0FromPort:\u00a0'22'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0ToPort:\u00a0'22'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0CidrIp:\u00a00.0.0.0/0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0IpProtocol:\u00a0tcp\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0FromPort:\u00a0'80'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0ToPort:\u00a0'80'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0CidrIp:\u00a00.0.0.0/0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0IpProtocol:\u00a0tcp\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0FromPort:\u00a0'443'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0ToPort:\u00a0'443'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0CidrIp:\u00a00.0.0.0/0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0IpProtocol:\u00a0icmp\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0FromPort:\u00a0'8'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0ToPort:\u00a0'-1'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0CidrIp:\u00a00.0.0.0/0\n\nNote:\nThe script will only run on the Ireland server of Amazon, due the instance type and the instance AM\u00cf which is only supported by the Ireland server.\n\nThe link to the Github repository:\n \n\n\nResult\nWith the script you can create a stack with new resources. You can do this inside Amazon CloudFormation services.  Here you can upload the script by using the create stack with new resources :\n\nThe script makes sure that the stack details get filled in automatically (except the stack name):\n\nAfter this the stack will be created.\nThe stack is created which you can see down below:\n\n\nWhen it is fully created you can see all the parameters configured:\nThe DNS records are being add automatically:\n\n\nAn instance (Virtual machine) is created:\n\n\nWe can check if the OpenBalena running on the server, by pinging the api:\n"
        }
      ]
    },
    "OpenRemoteBalenaDocumentation": {
      "hand-ins": [
        {
          "text": "\nVersion control\n\n\nIntroduction\nThis document is set up for OpenRemote for the needed information about OpenBalena. It can also give the needed insight for (future) students.\nThis document can be seen as a kind of user manual to see how you need to use OpenBalena, as well as a reference document when you are stuck while working with OpenBalena.\nWhat is OpenBalena?\nOpenBalena is an orchestration tool for IoT devices. OpenBalena is built on Docker. When orchestrating an app, you basically orchestrate the docker file.\nSo for instance, you can add a whole repository with a docker file to an IoT device. \nYou can use OpenBalena for automatic deployment, however you do need to flash the BalenaOS with the application on a SD card. When putting in the SD card into the Raspberry Pi it will automatically deploy the software on the Raspberry Pi. \n\n\nInstallation\n\nThe installation of OpenBalena is based on a CloudFormation script which can be run with AWS. The script will install OpenBalena and its requirements. It will configure the network with the CNAMES and the firewall settings.\nThe script, the explanation and the outcome can be found at \n\nCertificate\nThe OpenBalena server will create a certificate while installing, this certificate needs to be added to the local machines to use Balena.\nFor the extraction of the certificates the file permissions need to change. Currently when extracting the file without changing the permissions it will give an error.\nThe following files need to have different permissions:\nhome/ubuntu/open-balena/config/certs/root/  this map\nhome/ubuntu/open-balena/config/certs/root/ca.crt  this file\nThe files need to have the following permissions 0755. \nWhen that is done you can extract the file using scp. On Windows the following was done:\nC:\\Users\\Elmira>pscp -i \"C:\\Users\\Elmira\\Desktop\\balena private.ppk\" ubuntu@34.243.4.24:/home/ubuntu/open-balena/config/certs/root/ca.crt C:\\Users\\Elmira\\Desktop\\Compose\n\nSetup local machine\nThe local machine in this setup is using Linux Ubuntu.\nTo set the certificate in the local machine the following needs to be done:\n$ sudo cp ca.crt /usr/local/share/ca-certificates/ca.crt\n$ sudo update-ca-certificates\n$ sudo systemctl restart docker\n\nBalena CLI\nWhen working with balena you will need to install Balena CLI on your local machine;. With the CLI you can configure balena and use its functionalities.\nDown below the steps for installing Balena CLI:\nDownload the latest zip file from the\u00a0. Look for a file name that ends with \"-standalone.zip\", for example:\nbalena-cli-vX.Y.Z-linux-x64-standalone.zip\nExtract the zip file contents to any folder you choose. The extracted contents will include a\u00a0balena-cli\u00a0folder.\nAdd the\u00a0balena-cli\u00a0folder to the system's\u00a0PATH\u00a0environment variable. There are several ways of achieving this on Linux: See this\u00a0StackOverflow post. Close and reopen the terminal window so that the changes to PATH can take effect.\nIn our case what we most of the time were using was the following:\nexport PATH=$PATH:~/balena-cli\nHowever you need to run this command every time you are using a new terminal session.\nCheck that the installation was successful by running the following commands on a command terminal:\nbalena version\u00a0- should print the CLI's version\nbalena help\u00a0- should print a list of available commands\n\n\n\nBy default Balena CLI is using the BalenaCloud configuration instead of the OpenBalena configuration. Add the following line to the CLI's configuration file, replacing \"mydomain.com\" with the domain name of the OpenBalena server:\nbalenaUrl: \"balena.openremote.io\"\nThe CLI configuration file can be found at:\nOn Linux or macOS: ~/.balenarc.yml\nOn Windows: %UserProfile%\\_balenarc.yml\nIn most of the cases the file doesn\u2019t exist, in that case you need to create the file.\nThe last step in the CLI installation is to set an environment variable to the root certificate. This way the CLI can connect securely to the OpenBalena server.\n \n\nBalena login\nAfter the last steps try logging into Balena, using the following commands:\nBalena login\nSelect credentials\nFill in the email address\nFill in the password\nYou are successfully logged in now!\nIf you are not sure that you are logged in, check it with the following:\n$balena whoami\n\n\n\nApplication\nBefore you can push a application to a Raspberry Pi or an Orange Pi, you need to create an app. An app is an application that you create and after that you can add the needed code.\nNew application\nTo create a new application run the following command:\n$ balena app create demo\nChoose the device type that you want to use with the application and it is finished.\n\nWhen needing a list of all the applications made you can run the following:\n$ balena apps\n\n\n\nDevices\nOnce we have an application, it\u2019s time to start provisioning devices. To do this, first download a balenaOS image from balena.io. Pick the development image that is appropriate for your device.\nUnzip the downloaded image and use the balena CLI to configure it:\n$ balena os configure ~/Downloads/balena-cloud-raspberrypi3-2.58.3+rev1-dev-v11.14.0.img --app demo\n\nWhen configuring you have 2 options:\nWi-Fi\nEthernet\nAccording which you want to use with your RPI you can choose whatever you like.\nBalenaOS \nTo configure the RPIs with the right image, we need to use BalenaEtcher. For this we need the image and a location to flash the image to.\n\nAfter that and plugging it into a RPI we need to wait for a few minutes and the devices will show in our BalenaCLI\n\n\nTo check if all the devices are connected, you can run the following command:\n$ balena devices\n\nFor more specific information on the devices you can run the following command:\n\n\nNew release\nTo push a new release to your RPIs you need to be in a working directory with the application you want to push. For instance your application is running in :~/test/test2, you need to be in that directory. When you are in that directory you can push the application with the following command:\n$ balena deploy demo \u2013logs\n\n\nWhen pushing a release/app you need to make sure that it is Balena compatible. For instance, Balena doesn\u2019t support higher versions of docker-compose (it only supports up until 2.1).\n\n\n\nAPI calls\nInside the BalenaCLI you can work with API calls. These API calls can be used to get the needed information from the server.\nMaking of an API key\nTo make an API-key you need to fill in the following command:\n$ balena api-key generate Example\nRegistered api key 'Example':\nAPI-KeyIsListedHere\n\nFor more info on how to use the API calls, see Appendix B.\n\nAppendix\nAppendix A \u2013 Cloudformation script OpenBalena install\n\n\n\nVersion control\n\n\n\nIntroduction\nThis document is set up for the description of the Cloudformation script, this document can make sure that the purpose of the script is clear, as well as it functionality. The script can be used by OpenRemote to set up the OpenBalena server. \nThis document will contain the following items:\nWhy is it needed?\nWhat is the functionality of the script?\nThe code of the script\nThe result when running the script\n\nWhy is the script needed?\nThis script is needed so OpenRemote can easily deploy OpenBalena on Amazon Web Services. With this script they can easily deploy, automate the roll-out and after that manage IoT devices. OpenRemote can also use it for their company to help others deploy the OpenBalena server. \nThis script can realize the requirements for the OpenBalena server, based on the needs of OpenRemote. The needs of OpenRemote are that they can orchestrate the IoT devices, but also that it is done in a secure and automatic way.\nWhat is the functionality?\nThis script can be used to automate the deployment of OpenBalena. This script will make a server where OpenBalena is running. \nThe script can be adjusted so it will fit within the environment of everyone. If someone else instead of OpenRemote want to deploy OpenBalena they will need to change the following variables:\nDNS domain\nHostname\nKeypairs\nDNS records\nThey will need to adjust the UserData line where OpenBalena is configured based on mail address, password and domain name. Which is the following line:\nsudo\u00a0bash\u00a0/home/ubuntu/open-balena/scripts/quickstart\u00a0-U\u00a0balena@example.com\u00a0-P\u00a0PasswordHere\u00a0-d\u00a0balena.openremote.io\n\n\n\nCode\n---\nAWSTemplateFormatVersion:\u00a0'2010-09-09'\nDescription:\u00a0'Template\u00a0to\u00a0install\u00a0OpenBalena\u00a0on\u00a0AWS\u00a0environment\u00a0on\u00a0ubuntu'\nParameters:\n\u00a0\u00a0DomainName:\n\u00a0\u00a0\u00a0\u00a0Description:\u00a0DNS\u00a0domain\u00a0for\u00a0created\u00a0stack\u00a0(clear\u00a0it\u00a0is\u00a0you\u00a0want\u00a0the\u00a0deployment\u00a0with\u00a0an\u00a0insecure\u00a0SSL\u00a0certificate)\n\u00a0\u00a0\u00a0\u00a0Type:\u00a0String\n\u00a0\u00a0\u00a0\u00a0Default:\u00a0balena.openremote.io\n\u00a0\u00a0HostName:\n\u00a0\u00a0\u00a0\u00a0Description:\u00a0Hostname\u00a0of\u00a0the\u00a0balenaserver\n\u00a0\u00a0\u00a0\u00a0Type:\u00a0String\n\u00a0\u00a0\u00a0\u00a0Default:\u00a0openbalena\n\u00a0\u00a0HostedZone:\n\u00a0\u00a0\u00a0\u00a0Description:\u00a0Use\u00a0AWS\u00a0hosted\u00a0zone\n\u00a0\u00a0\u00a0\u00a0Type:\u00a0String\n\u00a0\u00a0\u00a0\u00a0Default:\u00a0true\n\u00a0\u00a0\u00a0\u00a0AllowedValues:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0true\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0false\n\u00a0\u00a0KeyName:\n\u00a0\u00a0\u00a0\u00a0Description:\u00a0Name\u00a0of\u00a0an\u00a0existing\u00a0EC2\u00a0KeyPair\u00a0to\u00a0enable\u00a0SSH\u00a0access\u00a0to\u00a0the\u00a0instance\n\u00a0\u00a0\u00a0\u00a0Type:\u00a0AWS::EC2::KeyPair::KeyName\n\u00a0\u00a0\u00a0\u00a0Default:\u00a0openbalena\n\u00a0\u00a0\u00a0\u00a0ConstraintDescription:\u00a0must\u00a0be\u00a0the\u00a0name\u00a0of\u00a0an\u00a0existing\u00a0EC2\u00a0KeyPair.\n\u00a0\u00a0InstanceType:\n\u00a0\u00a0\u00a0\u00a0Description:\u00a0EC2\u00a0instance\u00a0type\u00a0(don't\u00a0change\u00a0it\u00a0unless\u00a0you\u00a0know\u00a0what\u00a0you\u00a0are\u00a0doing)\n\u00a0\u00a0\u00a0\u00a0Type:\u00a0String\n\u00a0\u00a0\u00a0\u00a0#\u00a0AWS\u00a0Compute\u00a0Optimizer\u00a0reports\u00a0that\u00a0this\u00a0instance\u00a0type\u00a0is\u00a0under-provisioned\u00a0and\u00a0advise\u00a0to\u00a0use\n\u00a0\u00a0\u00a0\u00a0#\u00a0more\u00a0than\u00a0double\u00a0the\u00a0price\u00a0instance.\u00a0However,\u00a0it\u00a0works\u00a0in\u00a0our\u00a0test\u00a0setup.\n\u00a0\u00a0\u00a0\u00a0Default:\u00a0t3a.small\n\u00a0\u00a0\u00a0\u00a0ConstraintDescription:\u00a0must\u00a0be\u00a0a\u00a0valid\u00a0EC2\u00a0instance\u00a0type.\n\u00a0\u00a0InstanceAMI:\n\u00a0\u00a0\u00a0\u00a0Description:\u00a0Managed\u00a0AMI\u00a0ID\u00a0for\u00a0EC2\u00a0Instance\u00a0(don't\u00a0change\u00a0it\u00a0unless\u00a0you\u00a0know\u00a0what\u00a0you\u00a0are\u00a0doing)\n\u00a0\u00a0\u00a0\u00a0Type\u00a0:\u00a0String\n\u00a0\u00a0\u00a0\u00a0Default:\u00a0ami-0b5247d4d01653d09\nConditions:\n\u00a0\u00a0DnsRecordCreate:\u00a0!Equals\u00a0[!Ref\u00a0HostedZone,\u00a0true]\n\u00a0\u00a0DomainName:\u00a0!Not\u00a0[\u00a0!Equals\u00a0[!Ref\u00a0DomainName,\u00a0\"\"]\u00a0]\nOutputs:\n\u00a0\u00a0\u00a0\u00a0InstanceIP:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Description:\u00a0The\u00a0Instance\u00a0public\u00a0IP\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Value:\u00a0!GetAtt\u00a0EC2Instance.PublicIp\n\u00a0\u00a0\u00a0\u00a0PublicUrl:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Condition:\u00a0DomainName\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Description:\u00a0openbalena\u00a0Instance\u00a0URL\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Value:\u00a0!Join\u00a0[\u00a0\".\",\u00a0[\u00a0!Ref\u00a0HostName,\u00a0!Ref\u00a0DomainName\u00a0]\u00a0]\nResources:\n\u00a0\u00a0DNSRecord:\n\u00a0\u00a0\u00a0\u00a0Condition:\u00a0DnsRecordCreate\n\u00a0\u00a0\u00a0\u00a0Type:\u00a0AWS::Route53::RecordSet\n\u00a0\u00a0\u00a0\u00a0Properties:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0HostedZoneName:\u00a0!Join\u00a0[\u00a0\"\",\u00a0[\u00a0!Ref\u00a0DomainName,\u00a0\".\"\u00a0]\u00a0]\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Name:\u00a0!Ref\u00a0DomainName\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0TTL:\u00a0'60'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Type:\u00a0A\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0ResourceRecords:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0!GetAtt\u00a0EC2Instance.PublicIp\n\u00a0\u00a0DNSRecord2:\n\u00a0\u00a0\u00a0Condition:\u00a0DnsRecordCreate\n\u00a0\u00a0\u00a0Type:\u00a0AWS::Route53::RecordSet\n\u00a0\u00a0\u00a0Properties:\n\u00a0\u00a0\u00a0\u00a0\u00a0HostedZoneName:\u00a0!Join\u00a0[\u00a0\"\",\u00a0[\u00a0!Ref\u00a0DomainName,\u00a0\".\"]\u00a0]\n\u00a0\u00a0\u00a0\u00a0\u00a0Name:\u00a0api.balena.openremote.io.\n\u00a0\u00a0\u00a0\u00a0\u00a0TTL:\u00a0'60'\n\u00a0\u00a0\u00a0\u00a0\u00a0Type:\u00a0CNAME\n\u00a0\u00a0\u00a0\u00a0\u00a0ResourceRecords:\n\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0!GetAtt\u00a0EC2Instance.PublicDnsName\n\u00a0\u00a0DNSRecord3:\n\u00a0\u00a0\u00a0Condition:\u00a0DnsRecordCreate\n\u00a0\u00a0\u00a0Type:\u00a0AWS::Route53::RecordSet\n\u00a0\u00a0\u00a0Properties:\n\u00a0\u00a0\u00a0\u00a0\u00a0HostedZoneName:\u00a0!Join\u00a0[\u00a0\"\",\u00a0[\u00a0!Ref\u00a0DomainName,\u00a0\".\"]\u00a0]\n\u00a0\u00a0\u00a0\u00a0\u00a0Name:\u00a0vpn.balena.openremote.io.\n\u00a0\u00a0\u00a0\u00a0\u00a0TTL:\u00a0'60'\n\u00a0\u00a0\u00a0\u00a0\u00a0Type:\u00a0CNAME\n\u00a0\u00a0\u00a0\u00a0\u00a0ResourceRecords:\n\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0!GetAtt\u00a0EC2Instance.PublicDnsName\n\u00a0\u00a0DNSRecord4:\n\u00a0\u00a0\u00a0Condition:\u00a0DnsRecordCreate\n\u00a0\u00a0\u00a0Type:\u00a0AWS::Route53::RecordSet\n\u00a0\u00a0\u00a0Properties:\n\u00a0\u00a0\u00a0\u00a0\u00a0HostedZoneName:\u00a0!Join\u00a0[\u00a0\"\",\u00a0[\u00a0!Ref\u00a0DomainName,\u00a0\".\"]\u00a0]\n\u00a0\u00a0\u00a0\u00a0\u00a0Name:\u00a0registry.balena.openremote.io.\n\u00a0\u00a0\u00a0\u00a0\u00a0TTL:\u00a0'60'\n\u00a0\u00a0\u00a0\u00a0\u00a0Type:\u00a0CNAME\n\u00a0\u00a0\u00a0\u00a0\u00a0ResourceRecords:\n\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0!GetAtt\u00a0EC2Instance.PublicDnsName\n\u00a0\u00a0DNSRecord5:\n\u00a0\u00a0\u00a0Condition:\u00a0DnsRecordCreate\n\u00a0\u00a0\u00a0Type:\u00a0AWS::Route53::RecordSet\n\u00a0\u00a0\u00a0Properties:\n\u00a0\u00a0\u00a0\u00a0\u00a0HostedZoneName:\u00a0!Join\u00a0[\u00a0\"\",\u00a0[\u00a0!Ref\u00a0DomainName,\u00a0\".\"]\u00a0]\n\u00a0\u00a0\u00a0\u00a0\u00a0Name:\u00a0s3.balena.openremote.io.\n\u00a0\u00a0\u00a0\u00a0\u00a0TTL:\u00a0'60'\n\u00a0\u00a0\u00a0\u00a0\u00a0Type:\u00a0CNAME\n\u00a0\u00a0\u00a0\u00a0\u00a0ResourceRecords:\n\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0!GetAtt\u00a0EC2Instance.PublicDnsName\n\u00a0\u00a0EC2Instance:\n\u00a0\u00a0\u00a0\u00a0Type:\u00a0AWS::EC2::Instance\n\u00a0\u00a0\u00a0\u00a0Properties:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0InstanceType:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Ref:\u00a0InstanceType\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0KeyName:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Ref:\u00a0KeyName\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0ImageId:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Ref:\u00a0InstanceAMI\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0SecurityGroups:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0Ref:\u00a0InstanceSecurityGroup\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Tags:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0Key:\u00a0\"Name\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Value:\u00a0\"openbalena\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0UserData:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Fn::Base64:\u00a0!Sub\u00a0|\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0#!/bin/bash\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0apt-get\u00a0update\u00a0&&\u00a0apt-get\u00a0install\u00a0-y\u00a0build-essential\u00a0git\u00a0docker.io\u00a0libssl-dev\u00a0nodejs\u00a0npm\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0curl\u00a0-L\u00a0https://github.com/docker/compose/releases/download/1.27.4/docker-compose-Linux-x86_64\u00a0-o\u00a0/usr/local/bin/docker-compose\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0chmod\u00a00755\u00a0/usr/local/bin/docker-compose\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0usermod\u00a0-a\u00a0-G\u00a0sudo\u00a0ubuntu\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0usermod\u00a0-a\u00a0-G\u00a0docker\u00a0ubuntu\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0su\u00a0ubuntu\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0cd\u00a0/home/ubuntu\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0git\u00a0clone\u00a0https://github.com/balena-io/open-balena.git\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0cd\u00a0/home/ubuntu/open-balena\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0docker-compose\u00a0--version\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0sleep\u00a05\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0sudo\u00a0bash\u00a0/home/ubuntu/open-balena/scripts/quickstart\u00a0-U\u00a0balena@example.com\u00a0-P\u00a0PasswordHere\u00a0-d\u00a0balena.openremote.io\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0sleep\u00a05\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0sudo\u00a0systemctl\u00a0start\u00a0docker\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0sleep\u00a05\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0sudo\u00a0bash\u00a0/home/ubuntu/open-balena/scripts/compose\u00a0up\u00a0-d\n\u00a0\u00a0InstanceSecurityGroup:\n\u00a0\u00a0\u00a0\u00a0Type:\u00a0AWS::EC2::SecurityGroup\n\u00a0\u00a0\u00a0\u00a0Properties:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0GroupDescription:\u00a0SSH\u00a0+\u00a0HTTP\u00a0+\u00a0HTTPS\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0SecurityGroupIngress:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0IpProtocol:\u00a0tcp\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0FromPort:\u00a0'22'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0ToPort:\u00a0'22'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0CidrIp:\u00a00.0.0.0/0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0IpProtocol:\u00a0tcp\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0FromPort:\u00a0'80'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0ToPort:\u00a0'80'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0CidrIp:\u00a00.0.0.0/0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0IpProtocol:\u00a0tcp\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0FromPort:\u00a0'443'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0ToPort:\u00a0'443'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0CidrIp:\u00a00.0.0.0/0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0IpProtocol:\u00a0icmp\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0FromPort:\u00a0'8'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0ToPort:\u00a0'-1'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0CidrIp:\u00a00.0.0.0/0\n\nNote:\nThe script will only run on the Ireland server of Amazon, due the instance type and the instance AM\u00cf which is only supported by the Ireland server.\n\nThe link to the Github repository:\n \n\n\nResult\nWith the script you can create a stack with new resources. You can do this inside Amazon CloudFormation services.  Here you can upload the script by using the create stack with new resources :\n\nThe script makes sure that the stack details get filled in automatically (except the stack name):\n\nAfter this the stack will be created.\nThe stack is created which you can see down below:\n\n\nWhen it is fully created you can see all the parameters configured:\nThe DNS records are being add automatically:\n\n\nAn instance (Virtual machine) is created:\n\n\nWe can check if the OpenBalena running on the server, by pinging the api:\n\n\n\n\nAppendix B \u2013 API Research\n\nVersion control\n\n\nResearch\nWhen using OpenBalena it is hard to get information on the devices. Which release they run, the CPU usage, if they are online or offline etc. With this document we want to simplify that process.\nGoal\nThe goal of the research and document is to get releases of software that is running on IoT devices. OpenRemote wants to use OpenBalena in order to update and orchestrate IoT devices, however if they don\u2019t know which version of their software is running on their IoT devices OpenBalena does not make sense. This we want to tackle with the Balena API, with the needed research we want to see if  we can get the information which release the device is running through the API.\nSetup of the calls\nAPI calls are the way to get information about the OpenBalena server. The balena API uses the Open Data Protocol ()\nThese parts support the Balena API:\nGET: view information about a resource\nPOST: create a new resource\nPATCH: modify an existing resource\nDELETE: remove a resource\nIn our case we want to view information from the API so we should use the GET part of the API call.\nLooking at the API documentation from Balena, we can see how APIs are being built up:\nThis is the original API call for getting the application name and device type for each application, this is also seen as the \u201ctest\u201d API call.\ncurl -X GET \"https://api.balena-cloud.com/v5/application?\\$select=app_name,device_type\" \\ \n-H \"Content-Type: application/json\" \\ \n-H \"Authorization: Bearer <AUTH_TOKEN>\"\n---------\nWe can see that the calls consists of 3 parts:\nCurl \u2013 get\t https://link.com/...\nIn this field you can fill in the needed information that you want to request.\n-H content type\nIn which language and type the API call will reply\nAuthorization \nHere you need to fill in the token from OpenBalena, without the token you can\u2019t get any data out of the API\nMaking of an API key\nTo make an API-key you need to fill in the following command:\nbalena api-key generate Example\nRegistered api key 'Example':\nAPI-KeyIsListedHere\n\nPotential API calls\nIn this chapter I will list all the potential API calls that we can use in our project.\nGet the release device is pinned to\ncurl -X GET \\\n\"https://api.balena-cloud.com/v5/device(<ID>)?\\$select=should_be_running__release\" \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer <AUTH_TOKEN>\"\n\nThis API call can be useful to see which release the specific device should run, for instance the newest version of the application is being pushed to the devices. You can check if the device received the new version with the should be running release\n\n\nGet current release running on device\nThe is_on__commit field contains the hash of the release currently deployed to the device.\ncurl -X GET \\\n\"https://api.balena-cloud.com/v5/device(<ID>)?\\$select=is_on__commit\" \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer <AUTH_TOKEN>\"\n\nThis API call can be useful to see which release is currently running on 1 specific device. With this API call we can achieve the goal of the research. Because this API call gives the current release of a device.\n\n\n\nGet application by id along with its devices\ncurl -X GET \\\n\"https://api.balena-cloud.com/v5/application(<ID>)?\\$expand=owns__device\" \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer <AUTH_TOKEN>\"\n\nThis API call gives the information of an application. For instance, the application is running the OpenRemote software, with this command we can see which devices are connected to the OpenRemote application.\n\n\nAPI Calls\nIn this chapter I will describe which API call we need to get the right information. After checking and doing some research the following API calls will return which release of the application is in use:\nThe best and easiest way to get the releases of all devices will be the following command:\ncurl -X GET  \"https://api.balena.openremote.io/v6/device\" -H \"Content-Type: application/json\"  -H \"Authorization: Bearer e0fTlXMl9SvleddMuxRkCR9596hJEyW3\"\n\nFor getting just a release from 1 device you could use the following command:\ncurl -X GET \"https://api.balena.openremote.io/v6/device(2)?\\$select=is_running__release\"  -H \"Content-Type: application/json\"   -H \"Authorization: Bearer e0fTlXMl9SvleddMuxRkCR9596hJEyW3\"\nWith these API calls we can get the needed results for OpenRemote and tackle the problem for the usage of OpenBalena. Because this API call can give the release version of devices, without the calls it wouldn\u2019t make sense to use OpenBalena for OpenRemote.\n\nResult\nThe API call of getting the devices and their info will output the following per device:\n\"d\":[\n{\n\"created_at\":\"2020-12-10T14:31:10.677Z\",\n\"modified_at\":\"2020-12-15T10:19:16.778Z\",\n\"id\":2,\n\"actor\":6,\n\"api_heartbeat_state\":\"online\",\n\"uuid\":\"e568bc8782ff91eaf60fcb8e14b7abe4\",\n\"local_id\":null,\n\"device_name\":\"quiet-sun\",\n\"note\":null,\n\"is_of__device_type\":{\n\"__id\":29\n},\n\"belongs_to__application\":{\n\"__id\":2\n},\n\"is_online\":true,\n\"last_connectivity_event\":\"2020-12-10T14:32:52.588Z\",\n\"is_connected_to_vpn\":true,\n\"last_vpn_event\":\"2020-12-10T14:32:52.588Z\",\n\"is_locked_until__date\":null,\n\"logs_channel\":null,\n\"public_address\":null,\n\"vpn_address\":\"100.64.0.4\",\n\"ip_address\":\"192.168.178.150\",\n\"mac_address\":\"DC:A6:32:D1:A3:B9 66:C5:88:90:CC:6F\",\n\"memory_usage\":2469,\n\"memory_total\":7864,\n\"storage_block_device\":\"/dev/mmcblk0\",\n\"storage_usage\":1375,\n\"storage_total\":6742,\n\"cpu_usage\":25,\n\"cpu_temp\":55,\n\"is_undervolted\":false,\n\"cpu_id\":\"100000005f78925\",\n\"is_running__release\":{\n\"__id\":2\n},\n\"download_progress\":null,\n\"status\":\"Idle\",\n\"os_version\":\"balenaOS 2.58.6+rev1\",\n\"os_variant\":\"dev\",\n\"supervisor_version\":\"11.14.0\",\n\"provisioning_progress\":null,\n\"provisioning_state\":\"\",\n\"api_port\":48484,\n\"api_secret\":\"ed9386a609269dc4eed9269fe7955a52\",\n\"is_managed_by__service_instance\":{\n\"__id\":1\n},\n\"should_be_running__release\":null,\n\"is_managed_by__device\":null,\n\"is_web_accessible\":false,\n\"overall_status\":\"idle\",\n\"overall_progress\":null\n}\nWhile looking at the output of the API call we could use this the best, it will give the information from all the devices. \nIn our case we are interested in the \u201cis_running__release\u201d  this is showing us which release the device is currently working on. By release we mean the release of the application that the device is currently running. For instance the first push of the OpenRemote software would running release 1 and the update for the software would be running release 2.\n"
        }
      ]
    },
    "OpenRemoteBalenaResearchBalena": {
      "hand-ins": [
        {
          "text": "\n\nVersion control\n\n\n\nPreface\nThis document serves as a research document for the comparison between OpenBalena and BalenaCloud. With this document we can advise OpenRemote which Balena version they should use. For this, we will be looking at the requirements for OpenRemote, which is the following:\nIt needs to Open-Source\nOpenRemote is an Open-source based company, they have a vision that everything they do needs to be Open-source so everyone could use it. To fit their vision the requirement of Open-source has been set up.\nIt can be used to automate the orchestration of the OpenRemote software on Edge Gateways\nIn our case it will need to work on Raspberry Pis, since OpenRemote has Raspberry Pis ready for this project.\nIt would be nice if the software of Balena could integrate with the OpenRemote dashboard\nCurrently we are comparing only OpenBalena and BalenaCloud, since these services can easily orchestrate IoT devices and OpenRemote has requested to use Balena. Balena came up by a previous research of OpenRemote and they asked us to research Balena further.\nAt the end of the document there will be an advise for OpenRemote for which service they could best use. This will be based on the requirements of OpenRemote.\nWhat is OpenBalena & BalenaCloud?\nOpenBalena and BalenaCloud are both orchestration tools for IoT devices. You can add an \u2018app\u2019 to the software, like an application. Once it is added to Balena you can use BalenaCloud and OpenBalena to push this application to all kinds of devices in 1 time.\nThis will orchestrate all the software to the devices that are connected, it can be Raspberry Pis but it can also be Orange Pis.\n\n\n\nBalenaCloud\nThe key functions of BalenaCloud:\nHosted by Balena\nNo worries about the following:\nSecurity\nMaintenances\nScaling\nReliability\nMultiple users & organizations\nWeb-based dashboard\nUpdates with binary container delta\nOpenBalena\nThe key functions of OpenBalena:\nSelf-hosted\nSingle user\nNo web-based dashboard\nNo updates with binary container deltas\n\n\nCost\nBoth OpenBalena and BalenaCloud include costs. For OpenBalena it is for the hosting and BalenaCloud it is for the application. It is possible to \n\nBalenaCloud\n\n\nDevices\nThere are 2 types for adding more devices without changing the kind of subscription:\nEssentials: Run a single container. No access to device URLs.\n$ 1/ device / month\nMicroservices: Run multiple containers and expose ports via Device URL.\n$ 1.50/ device / month\n\nOpenBalena\nRequirements for system:\nDocker >= 18.05.0\nDocker Compose >= 1.11\nOpenSSL >= 1.0.0\npython >= 2.7 or >=3.4\n\n\n\n\n\n\nMatrix\n\nConclusion\nLooking back at the requirements of OpenRemote:\nIt needs to Open-Source\nIt can be used to automate the orchestration of the OpenRemote software on Edge Gateways\nIt would be nice if the software of Balena could integrate with the OpenRemote dashboard\nI would advise to use OpenBalena instead of BalenaCloud. OpenBalena fits all the requirements of OpenRemote and that is why this is the most suitable option.\nThey could also use BalenaCloud, but this would be a lot more expensive looking at the costs of it. Besides the cost, BalenaCloud isn\u2019t an Open-Source, which means it is not in line with their vision. "
        }
      ]
    },
    "OpenRemoteProjectproposal": {
      "hand-ins": [
        {
          "text": "\n\n\nVersion control\n\n\nProject Assignment\nProblem to solve\nNo accurate prediction models for wind power generation without a subscription.\nThere are already several prediction models for this online, but these are hidden behind a subscription. \nThere are no Edge Gateways within OpenRemote at the moment.\nCheck the potential usage of Tribuo .\n\nProject goals\nSoftware\nA prediction model (preferably in Java) for wind power, based on outside factors like wind speed, direction and configuration. This model should be able to run on a RPI. \nAn easy to use dashboard which displays the wind power prediction \nInfrastructure \nThe ability to create and manage Edge Gateways from a central instance.\n\nProject scope\nWe will be researching the Tribuo library and any other potential libraries as a machine-learning candidate.  \nResearch and create a dataset for the wind power prediction model.\nCreating a dashboard on which the power prediction can be viewed.\nDeveloping a prediction model that will be able to run on an RPI3.\nCreating a solution for deploying edge gateways.\n\nResearch Questions & Sub questions\nHow to predict the generation of wind power?\nWhat type of prediction model would be beneficial?\nWhat ML library would be optimal for predicting wind power generation? (QoL, efficiency, community etc.)\nHow to deploy the OpenRemote software on edge gateways in a manageable fashion.\nHow to deploy?\nHow can it be kept manageable?\n\n\n\nProject Deliverables\nInfrastructure\nA solution for orchestrating deployments from OpenRemote software on edge gateways.\nDocumentation\nConfigurations\nUser install guide\nSoftware\nA prediction model for approximating wind power generation.\nA software solution which has the potential to display the predicted power yield on an OpenRemote dashboard. \nDocumentation\n\n\n\nTeam\nThe team consist of the following members:\nNiek van Dam\nDaan de Weirdt\nElmira Drost \nDaniel Vaswani\nMartin Markov\nInside the team there are 2 subgroups, one is based on the Software side and the other is based on the Infrastructure side.\nSoftware\nThe group of software will be focusing on the Software side of the project. They will be focusing on the prediction of the wind power. The group of Software consist of the following members:\nNiek van Dam\nDaniel Vaswani\nMartin Markov\nThere is 1 group leader in the Software group who is also responsible for the communication between the stakeholders. The group leader is: Niek van Dam.\nThey will be solving the following sub questions for the project:\nHow to predict the generation of wind power?\nWhat type of prediction model would be beneficial?\nWhat ML library would be optimal for predicting wind power generation? (efficiency, community etc.)\nInfrastructure\nThe group of Infrastructure will be focusing on the Infrastructure side of the project. They will be focusing on deploying software on edge gateways. The group of Infrastructure consist of the following members:\nDaan de Weirdt\nElmira Drost\nThere is 1 group leader in the Infrastructure group who is also responsible for the communication between the stakeholders. The group leader is: Daan de Weirdt.\nThey will be solving the following sub questions for the project:\nHow to deploy the OpenRemote software on edge gateways in a manageable fashion.\nHow to deploy?\nHow can it be kept manageable?\n\n \n\nActivities and Planning\nDuring this project, several activities will be undertaken to ensure success. While a more detailed planning will be available in this projects Microsoft Teams environment, a rough timeline will be sketched here. \n\n\nRisks\nIn the following section are listed probable underlying problems (and some probable solutions) that may arise throughout the project.\nGeneral\nCovid-19\nDue to these uncertain times, there is a chance that physical meetings and team meetings can be delayed/cancelled. This would delay the progression of the project. \nSoftware\nSelecting suitable technologies\nWhen choosing technologies, the software development team should be aware of the underlying risk of getting stuck with a technical problem that is not listed in the documentation of the used technology, or is not yet discovered as a bug/implemented feature from its maintainers. In case of a similar problem the development team should choose technologies that are widely used and have large enough community which would eventually be able to help\nWhile it could be tempting to use cutting-edge technology, the development team should take into consideration the risk of using It as the main one for the project.\nSelecting source of data\nWhen selecting data source, the development team should take into consideration the probability of getting misleading data from an external source, no matter how well the service is advertised\nEven though it may seem preferable to set up our sensors for gathering wind data from the buildings, there is always a chance of not setting them up correctly or using not sensitive enough devices\nBudgeting\nIn the case of any purchasing needs, we should check the costs and discuss them with OpenRemote\nWork organization\nThroughout the work process, any conflicts, blockers or miscommunication between team members can arise. Problems like this should be resolved in an \u201cagile\u201c manner by discussing them with the team.\n\nInfrastructure\nDelivery time\nWhen purchasing the Raspberry PI\u2019s online, there is a chance that online delivery will get delayed. This would delay the testing and implementation of OpenBalena. \n\n"
        }
      ]
    },
    "OpenRemoteSprintdeliveries": {
      "hand-ins": [
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWind Energy Sprint Delivery \n\n\n\n\nWind Energy\nSprint Delivery \nDaan de Weirdt \nNiek  van Dam\nDaniel Vaswani\nMartin markov\nElmira Drost \n\n\n\nIndex\nGoal of the sprint\nPOC \nNext sprint\n\n\n\nGoal of the sprint\nResearch data sources\nPoC Software\nPoC Infrastructure\n\n\n\nResearch Data Sources\nWindmill Power Generated Data\nWindstats.nl offers historical data for power generated for each windmill in the windfarms in The Netherlands\nWind Historical Weather Data\nOpenWeatherMap API offers bulk historical weather data for any location for up to 40 years back\n\n\n\n\nPoC Software\nLSTM RNN\nDesign \nTurbine dataset\nNaN values\n\n\n\n\n\nPoC Infrastructure\nBalenaCloud\nBalenaOS\nBalenaOS Raspberry Pi\n\n\n\n\nNext Sprint \nSoftware: \u00a0\tRebase/optimize prediction model on the previously created PoC\nInfrastructure :\tTesting on Raspberry PI with OpenRemote Software \nMisc: \nSolving NaN values with specialist from Fontys\nObtain or Convert docker file to a lower version\n\n\n\nQuestions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n.MsftOfcThm_Accent1_Fill {\n fill:#BE977B; \n}\n.MsftOfcThm_Accent1_Stroke {\n stroke:#BE977B; \n}\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/docProps/thumbnail.jpeg\n\n"
        },
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWind Energy Sprint Delivery \n\n\n\n\nWind Energy\nSprint Delivery \nDaan de Weirdt \nNiek  van Dam\nDaniel Vaswani\nMartin markov\nElmira Drost \n\n\n\nIndex\nGoal of the sprint\nResearch data sources\nPOC's\u00a0\nDemo\nNext sprint\n\n\n\nGoal of the sprint\nData sources\nPoC Software\nPoC Infrastructure\n\n\n\nResearch Data Sources\n\nWindstats gives turbine specs\nNo real-time turbine data\u00a0\n\n\n\n\n\n\n\nResearch Data Sources\nOption 1:\nResearch new datasets\n+ More relevant data\u00a0\n+ Larger dataset\n- Potentially\u00a0slow development\n- Possibly costly\nOption 2:\nCreate a wind turbine/weather station\n+ More accurate data\n+ No subscription\n- Retrieving data\n- Time consuming\n\n\n\n\nResearch Data Sources\nOption 3:\nHybrid option (openweather + our own turbine)\n+ Most accurate data\u00a0\n- Time consuming\n\n\n\n\nPoC Software\nLSTM RNN\nTurbine dataset\n\n\n\n\n\n\nWind Turbine for backtesting\nAutoRegressive Integrated Moving Average\nPopular time series forecasting\n\n\n\n\n\nPoC Infrastructure\nOpenBalena vs BalenaCloud\nOpenBalena\nDemo\n\n\n\n\n\nDemo\n\n\n\nSoftware\n\n\nInfrastructure\n\n\n\nNext Sprint \nS: \u00a0Create final prediction model\nI:\u00a0\u00a0 Linking more RPI\u2019s with OpenBalena\nMisc: \n\n\n\nQuestions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n.MsftOfcThm_Accent1_Fill {\n fill:#BE977B; \n}\n.MsftOfcThm_Accent1_Stroke {\n stroke:#BE977B; \n}\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/docProps/thumbnail.jpeg\n\n"
        },
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWind Energy Sprint Delivery \n\n\n\n\nWind Energy\nSprint Delivery \nDaan de Weirdt \nNiek  van Dam\nDaniel Vaswani\nMartin markov\nElmira Drost \n\n\n\n1\n\n\nIndex\nGoal of the sprint\nWind turbine\nCloud formation\nNext sprint\n\n\n\nGoal of the sprint\nWind Turbine setup\nCloudformation script\n\n\n\nWind turbine\nDocument Building Management\nPermit\n\n\n\n\n\nCloud formation\nCloud-formation\u00a0succesfull\nInstall-script for openbalena not fully functional.\n\n\n\n\n\n\nNext Sprint \nS: Backtesting model on the real-time data  & installation of wind turbine\nI: Deployment of OpenBalena using cloudformation and research OpenBalena-api.\nMisc: \n\n\n\nQuestions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n.MsftOfcThm_Accent1_Fill {\n fill:#BE977B; \n}\n.MsftOfcThm_Accent1_Stroke {\n stroke:#BE977B; \n}\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/docProps/thumbnail.jpeg\n\n"
        },
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWind Energy Sprint Delivery \n\n\n\n\nWind Energy\nSprint Delivery \nDaan de Weirdt \nNiek  van Dam\nDaniel Vaswani\nMartin markov\nElmira Drost \n\n\n\n1\n\n\nIndex\n\n\n\nGoal of the sprint\n\n\nWind turbine\n\n\nCloud formation\n\n\nNext sprint\n\n\n\nGoal of the sprint\nInstallation of turbine\nCreation of API\nGet releases from OpenBalena\n\n\n\n\nData gathering for prediction model\nArduino + Anemometer\n2 Locations\nAPI for storing data\nRedesign prediction model\n\n\n\n\nPrediction model\nNew layers/nodes\nNew parameters \nOpenWeather data\nCross-referenced wind output\n\n\n\n\n\nInfrastructure\nAPI Calls releases researched\nDeployed Openremote software on RPI\nFirst stages of redeployscript\nDeployment in production not recommended\n\n\n\n\n\nFinal sprint\nS: \nSetting up API + Documentation\nRedesign of the prediction model\nCreation of weather + power dataset for prediction model\n\nI: \nFinishing up Documentation\nFinish redeployscript\n\n\n\n\nQuestions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n.MsftOfcThm_Accent1_Fill {\n fill:#BE977B; \n}\n.MsftOfcThm_Accent1_Stroke {\n stroke:#BE977B; \n}\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/docProps/thumbnail.jpeg\n\n"
        },
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWind Energy Sprint Delivery \n\n\n\n\nWind Energy\nSprint Delivery \nDaan de Weirdt \nNiek  van Dam\nMartin markov\nElmira Drost \n\n\n\n1\n\n\nIndex\n\n\n\nGoal of the sprint\n\n\nWind turbine\n\n\nInfrastructure\n\n\nNext semester\n\n\nPrediction Model\n\n\n\nGoal of the sprint\nFinishing the Documentation for Infrastructure & Software\nFinish\u00a0redeployscript\n\n\n\n\nData gathering for prediction model\nStill work in progress\nArduino/ESP32 + Anemometer\n2 Locations\nAPI for storing data\n\n\n\n\nWind Turbine installation\nGreen light from building management\nInstallation next semester\nCloud controller installation\n\n\n\nPrediction model\nParameters:\nOpenWeather prediction data\nTemperature\nHumidity\nPressure\nWind speed\nWind degrees\nWeather ID\nFormula estimated wind power\nDocumentation + code delivery\n\n\n\n\n\n\nInfrastructure\nDocumentation\n\n\n\n\n\nNext semester\nSet up the wind turbine on location\nCloud controller TESUP\n\n\n\n\n\nQuestions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n.MsftOfcThm_Accent1_Fill {\n fill:#BE977B; \n}\n.MsftOfcThm_Accent1_Stroke {\n stroke:#BE977B; \n}\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/docProps/thumbnail.jpeg\n\n"
        },
        {
          "text": "Version control\n\n\n\nReflection\nLooking back at the sprint deliveries of the semester I had the idea that sprint deliveries were a good thing. With the sprint deliveries we kept OpenRemote up to date, they had an open mindset which gave good feedback.\nThe feedback was really good, at first we used BalenaCloud instead of OpenBalena. OpenRemote gave the feedback that they wanted to use OpenBalena because it is open-source. Besides that feedback, they also asked us to work inside AWS with CloudFormation. We didn\u2019t know what CloudFormation was at first, but after doing some research it meant a script for automatic deployment of Virtual machines and other parameters (network config).\nInside this project, we split into 2 groups, the Software group and the Infrastructure group. Even though we split up we did had contact with each other and the sprint deliveries with each other. This went good for all the times, most of the times inside the meeting for the deliveries were Niek and I talking. But that went for the same thing as the normal meetings inside the project. \nLooking back at the fact that we didn\u2019t deliver a product at the end of every sprint, was, in my opinion, a little disappointing. You agree with a company and I think that you should stick to that agreement. But at the end for the Infrastructure part, we almost delivered all parts which I am proud of.\nOverall, I did miss the communication sometimes with all the 5 project members. Now, most of the times it was just Niek and me, but we found out that for instance Daniel and Martin didn\u2019t have anything with AI. The communication, in the beginning, was also a little bad with Daan and me but in the end, we fixed that. When we did fix it, we worked well together to finish this project.\nAll in all, I\u2019m proud I did this project and I learned a lot on the field of communication with the members and with stakeholders. I\u2019m looking forward to work on part 2 with the wind turbine  "
        }
      ]
    },
    "OpenRemoteWindturbineEric": {
      "hand-ins": [
        {
          "text": "Version control\n\n\n\nSituation\nSince the last press conference on the 14th of December the Netherlands has declared a country-wide lockdown. This caused us to be unable to actively work on the wind turbine project as planned. Due to this series of unfortunate events, we started looking for alternative wind turbine setups which can be worked at from home. After brainstorming with Eric, we came up with a plan to use an Arduino in Elmira\u2019s and Niek\u2019s backyard to simulate a wind turbine.\u00a0\n\nThe setup\nWe looked at two different approaches for this experiment, using two different tutorials. In both tutorials, wind speed is measured using an anemometer. The main difference in both is that they use a different methodology for measuring the RPM. The RJ-11 meter uses a RJ-11 adapter to export the wind speed data, whereas the inspeed anemometer uses a power cable, where you have to measure the amount of rotations per second manually.  \n\nAfter comparing feasibility and effectivity, we decided to choose the RJ-11 Anemometer, and are going to continue developing the prototype with that.\nComponents list:\nThe setup will be the following:\n\n\nArduino\u00a0Uno\u00a0WiFi \nAnemometer\nRJ-11 Connector for anemometer\n4.7K Resistor\nExternal 5V power source\n\n\nThe data to be gathered\nThe Arduino will be set up to record the current data every x minutes. Once these x minutes have passed, we will start the gathering of data. The anemometer will record the current wind speed in m/s, accompanied by an OpenWeather API call which returns the current weather status.\nThe data from the anemometer will be converted to what the power output should have been, had we used a TESUP turbine.  We can calculate this output by referencing the spec sheet of the turbine, which shows the output in W (watts) at the given wind speed. This will be stored as the \u2018current power output\u2019. Once the data collection is done, we will use this current power output as output variable in the prediction model. This means that this data will be the \u2018expected output\u2019 of the prediction model.  \nThe OpenWeather API will return a collection of weather data statistics, e.g.: Type of weather, temperature, the angle of the wind. This weather data will be used as the prediction parameters, these are parameters that the prediction model will use as input to predict the output variables.  \n\n\n\nPlanning\nPlanning for delivery time of the parts:\n\nPlanning for setup of the windspeeldcalculator2.0:\n"
        },
        {
          "text": "\n\nVersion control\n\n\n\n\nIntroduction\nThis document is meant to give insight into why we want to install a wind turbine on the TQ building. \nWe are a group of Fontys ICT Delta students working on a project for , where we have to predict Wind Turbine power output. For this we would like to install a wind turbine on the roof of the TQ building. \nWhy are we installing a turbine?\nWe are a group of Delta students working on a project for OpenRemote, where we have to predict the wind power which turbines give off. So far we created a proof of Concept with Artificial Intelligence, this is currently working an ideal wind dataset with a predefined wind turbine. This means that our algorithm will be able to predict the wind turbine power output in ideal situations with wind turbine x. \nAfter we have researched and found out that the Proof of Concept works as expected, we enter the second stage of the project. We need to create a \u2018dynamic\u2019 prediction model based on the proof of concept. In this case, dynamic means that the model must be able to predict the wind power output of different types of wind turbines, by just giving in a few specifications. In order to create this dynamic model we need to install our own wind turbine and train it on the real life data as a first step. \nWhere could it optimally be?\nOur intentions are to install the wind turbine on top of the roof of Strijp-TQ. Specifically, on top of the elevated part of the roof , located above the staircases. We chose this location because this is close to our base of operations, combined with the fact that this is high up, and eligible for harvesting a lot of wind data. \n \n\n\nPermit\nA permit for the installation of a wind turbine is not necessary, as the windmill is less than 4 meters. A permit is also not needed in case the turbine isn\u2019t connect to the powergrid.\nWhat is the impact\nIt will have an impact on the weight (circa 30kgs) of the building and it adds a little extra height (3,58 meter) on the building.\nWhat has to be taken into account?\nWe have to take the bearing force of the pole into account when installing this turbine. We calculated the maximum force and torque that a pole should withstand in order to stand on the roof while maintaining a safe environment. The highest force that our pole should be able to withstand is 24kg, which we calculated by taking the highest windspeed which Brabant has seen in the last 15 years (100 km/h).\nCalculations\nThe calculations are as follows:\nAverage air density = 1.225 kg/m3\nMass of the air (m) = air density (kg/m3)  * area (m2)= 1.225 kg/m3   *  0.25m2 (assuming the pole is 250cm and 10cm wide) = 0.306 kg\nAcceleration (a) = max wind speed(m/s)2  = 100 km/h =  27.7778 m/s2 = 661.6\nForce to withstand = m * a = 661.6 * 0.306 = 202.4 N = 20.639 KG\nWhat is the added value?\nThe added value for Strijp-TQ is that it can come in the media as a positive point. Strijp-TQ will have a good imago with green energy and a learning environment. \nSafety\nFor the safety we kept the following things in mind:\nVery high windspeeds till 100km/h\nSteady pole for the turbine\n\nWhat will be installed?\nParts installed\nOn the roof of Strijp-TQ there will be the following items installed:\nA turbine\nCharge controller\n2,5 meter pole of steel\nBattery for the energy\nArduino \nPot with soil\nTurbine\nSpecifications of the turbine\n\n\nConclusion\nA turbine on the roof for 2 months until the 8 of February.\nQuestions? \nIf you have any questions contact Eric Slaats for the needed information."
        },
        {
          "text": " \n\n\nVersion control\n\n \n\nDIY Turbine\nEase of Installation\nThe installation process for a DIY wind turbine may take more time than we have right now. To create an optimized and efficient wind turbine we will need to install a handful of motors including a gearbox. The DIY turbine also needs to be able to withstand the outside weather for a month or more, so it needs to be sturdy and wind-resistant. This will require extra designs and more time.  \nAdditional components\nThe components we would need are as follows:\nPower converter \nDynamo\n3d printed frame for weather-resistance\nVoltmeter (in addition to power converter)\nArduino or RPI\n3D printed casing\t \n\nCost\nThe cheapest option. 3D printing the parts needed will be close to free of charge due to Fontys. Besides 3D printing, we will also need to buy some basic parts to make the turbine work, this would total to around 30-50euros.\nRisks\nSubstantial risk. We are not sure if a direct simple motor connection to blades is sufficient in generating power. Without a gearbox, you run the problem of precision being severely inconsistent thus compromising on the model accuracy. In addition to this, having a gearbox lowers the size and weight of the motor needed to achieve the same amount of energy.\nAdditionally, the turbine will have to be resistant to the elements we expose it to. Waterproofing and stability are both required to be on the highest level we can get them, as these conditions could break the turbine without any effort.  \nOutput\nThe power generated is bottlenecked by the size of the blades for the 3D printer can print. This has a direct correlation. Likely 30cm to 100cm blade diameter.\nPower specifications\nThe efficiency of the system will be determined by us. This can only happen once we have the motor specifications as well as the blade size. This process itself will take some time.\nConclusion \nThis approach is equivalent to starting from scratch. A gearbox is not present, direct drives are more reliable when it comes to maintenance, but a more substantial motor is needed to compete with the energy generated by the smaller motor.\n\nSmall Turbine\nEase of installation\nEasy installation, A RPI and voltmeter are needed to collect data on power generated. \nCost\nInexpensive. The device itself costs \u20ac10, though external hardware could be required.\nRisks\nPower output could be too low to be indicative of the performance of a larger system. It is also possible that due to the minuscule size of the turbine, it will only spin on a day where the wind speed is high. \nOutput\n0.5W turbine rated at 100 ~ 6000RPM. Blades have a 100mm diameter.\nConclusion\nThe blade design is vertical as opposed to the more common fan design. Accuracy is to be determined. \n\nPurchasable commercial turbine\nEase of installation\n\nSlight complexity in the installation of the RPI/Arduino with a power converter/voltmeter. To install this, we will need to rewire some of the components in the turbine, which can cause delays. The additional components we would need are as follows:\nVoltmeter\nPower converter\nRPI/Arduino (for measuring and uploading data)\nCost\nThe most expensive option. These turbines can range anywhere from \u20ac200 to \u20ac1000. More contemplation needs to be done on which specific turbine. \nRisks\nThe high-power output could cause stress and damage to the system built for a significantly smaller turbine. Unpredictable shipping times could cause delayed work, in general, would mean that we collect data thus compromising the model accuracy.\nOutput\nLargest output due to size of turbine, circuitry and materials used in manufacturing. Its rated power goes anywhere from 100W to 500W at this price scale.\nWe have selected a 400W turbine rated at 800rpm. It costs about \u20ac137 and takes 1-2 weeks to ship from Amazon.nl. Blade size is not specified but it is the largest of the three proposed turbines.\nConclusion \nThe most overall complex solution in terms of work, cost, and time margins. \nGetting the power information through the wind turbine controller will need some sort of power inversion from AC to DC and a proper circuit diagram. Battery solution needs to be considered.\n\n\n\nDecision matrix\nTo decide which option will be most viable for us, we have created a decision matrix. This matrix will give points to each solution on a certain \u2018parameter\u2019. These parameters can be seen in column 1 and have gradings per solution in the row. \nThese criteria will be added up, from which the result will be chosen on the solution with the highest point total.\n\n\n\n\nPlans\nSoon we would like to get started with the implementing of a real turbine to get accurate measurements. Before getting started with modifying and purchasing a fully-fledged wind turbine we will buy a small Proof of Concept wind turbine first, to test the setup. This is also to make sure that we do not accidentally destroy a more expensive piece of equipment.  \nSmall turbine\nThe first proof of concept that we will create is a small turbine which can generate small amounts of power. The goal of a small turbine is gaining more experience with applying sensors to a turbine. After setting up this small turbine for testing purposes, we can collect basic data and compare it to the prediction model.\nTo gather the data, we will be using the following equipment:\nA(n) RPi/Arduino\nVoltmeter (measuring output)\nPower converter (probably not necessary for small form-factor turbines but will be used in the bigger turbine)\nWe will connect the output from the turbine to a power converter, which goes into a voltmeter and sends it to the Arduino/RPi. \nTesting\nAfter setting up a small turbine, we want to test if the measurements are relevant. We can do this by downscaling the prediction algorithm to smaller values which are more in-line with the power that the turbine will be generating. We can do this after a few days of data gathering. We cannot expect the prediction algorithm to be top-shelf because we will not be able to have enough data for a solid model.\nCommercial turbine purchase\nAfter testing a small turbine, we will be able to get an idea of whether the small turbine idea will be viable. If this is the case, we can forward the implementation to a bigger wind turbine. Once the turbine is set up, we can once again train a new model on the output. The setup will be identical to the one we have used before, using the same items as listed at the small turbine section.\nCost\n\n\n\n\nEnd product\nIn the end, we decided to order a commercial turbine instead of a small turbine. We discussed this with OpenRemote, who said that it is the most realistic and efficient option. The wind turbine needed to have a specification sheet with the wind speed relative to the power output, so we can properly calculate the output for our model.\nThe end product will be a mobile wind turbine, this so we can change the location for the measurements and it will be easier to install and to remove.\nFor this we chose the TESUP2400 wind turbine which has the following requirements:\n\nVoltmeter\nFor the measuring, we will be looking into the controller that comes with the TESUP. The version that we are going to get is 12 Volt. For the measuring, we want to reverse engineer the controller to get the data and sent it to our server instead of the one of TESUP.\nBatteries\nThe power of the wind turbine will go through the controller to batteries. Those batteries will be charged with the power of the windmill and will drain from using a water cooker.\n\nLinks\nSmall turbine:\n\nCommercial turbine:\n \nVoltage sensor\n \n"
        }
      ]
    },
    "PersonalcontributionDoekoe": {
      "hand-ins": [
        {
          "text": "\n\nHet project\n1.0 Client \nFabian Slabbers van Twelve(Bedrijf) & Peter Hogenberg(Docent)\n1.1 Problem to solve\u00a0 \nHet bedrijf heeft een huidig systeem wat ze graag zouden willen verbeteren, vooral in de snelheid en de gebruikersvriendelijkheid.\n1.2 Project Goals\u00a0 \nEen gemakkelijk systeem waarbij kassa\u2019s snel, veilig en geautomatiseerd uitgerold kunnen worden en deze daarna op locatie kunnen worden beheerd waar nodig.\n1.3 Project Scope\u00a0 \nEr gaat onderzoek gedaan worden naar het automatisch uitrollen van verschillende elementen, deze elementen zullen daarna gemaakt gaan worden zoals de stakeholder deze graag zou zien. Er zal voornamelijk op de infrastructuur-elementen gefocust worden, er zal dus geen complex of uitgebreid kassa programma gemaakt gaan worden maar een eenvoudige placeholder.\n1.4 Research Questions and Sub questions\u00a0 \nVoor dit onderzoek hebben wij de volgende hoofdvraag:\nHoe kan een kassasysteem uitgerold en remote beheerd worden?\nDe volgende deelvragen komen ook aanbod:\nHoe snel kunnen kassa\u2019s uitgerold worden?\nOp welke manieren kunnen kassasystemen worden uitgerold?\nHoe wordt er gezorgd dat de systemen gemonitord kunnen worden?\nWat is het budget voor het project?\nWelke manier heeft de voorkeur om de verschillende systemen aan elkaar te verbinden?\nHoe wordt deze omgeving beveiligd?\nHoe worden technische problemen opgelost?\nHoe kunnen medewerkers toegevoegd en verwijderd worden?\nWat zijn de benodigde systemen?\nOp welke manier is het mogelijk om data van de kassa  te verweken?\n\n\n1.5 Research Strategies to be used\u00a0\nLibrary\nAvailable product analysis\nKijken naar de huidige stand van zakendoor in gesprek te gaan met stakeholder.\nBest good and bad practices\nErvaringen vanuit de challanges meenemen in het project.\nExpert interview\nInterview met de stakeholder die al weet hoe het huidige systeem werkt en veel ervaring heeft. Gesprekken met leraaren die ervaringen hebben in de gebieden waar wij mee bezig zijn.\nLiterature study\nVoornamelijk via google opzoek naar relevante informatie.\nField\nDomain modelling\nEerste netwerktekening maken voor een beter overzicht van wat we moeten gaan doen.\nExplore user requirements\nOpzetten van requierments naar aanleiding van het gesprek met de stakeholder.\nLab\nComponent test\nTesten van individuele machines.\nComputer simulation\nEerste tests uitdraaien op VM\u2019s voordat we echte kassa\u2019s gaan gebruiken.\nNon-functional test\nVoor de verschillende onderdelen tests draaien met gegenereerde data of data die nog niet compleet is.\nSecurity test\nKijken of we in ons eigen systeem de beveiligingen kunnen omzeilen.\nSystem test\nWanneer er een versie is gemaakt die werkend zou moeten zijn, deze ook uittesten.\nShowroom\nProduct review\t\nVerschillende malen feedback vragen aan de stakeholders.\nPeer review\nAan andere leerlingen vragen wat zij van ons product vinden.\nBenchmark test\nHet testen van de snelheid van het systeem en kijken of het na een tijdje fout gaat.\n\n\nWorkshop\nBrainstorm\nHet generen van ideeen die toegepast kunnen worden in het project.\nPrototyping\nHet maken van netwerk tekeningen en deels opzetten van de infrastructuur.\nRequirements prioritization\nHiervoor willen wij MoSCoW toepassen op onze requierments.\n\n1.6 Project Deliverables\u00a0\nEen werkende omgeving die meteen gebruikt kan worden, deze bestaat uit meerdere machines. \n2. Team \nWij hebben een team met de volgende leden:\nDaan Rossy \nDaan de Weirdt \nTjerk van Gerwen\nElmira Drost\nMarc Hoogendoorn\nLisa van Gulick\nMarc Hoogendoorn is de teamleider.\nElmira en Daan de Weirdt zullen een mindere rol uitvoeren binnen het project omdat zij nog activiteiten moeten uitvoeren buiten het project.\n\n\n3. Activities and Planning\u00a0 \nWij gaan aan dit project werken vanaf 1 september tot 23 Januari. Wij gaan in onze definitieve planning werken met scrum.\n\n\n\n4. Test Environment\u00a0\nEen kleine eerste tekening om de schaal van het project te zien. Wij hebben vanuit het netlab een extern IP adres nodig. Voor de exacte middelen per VM die we nodig hebben is nog meer onderzoek nodig. \n\n\n5. Risks\u00a0\nBij het organiseren en het uitvoeren van dit project kunnen er verschillende risico\u2019s optreden, deze risico\u2019s kunnen de haalbaarheid van dit project verhinderen.\nInterne risico\u2019s\n\nDe haalbaarheid van de opdracht:\nDoormiddel van te weinig kennis kan er geen verzekering zijn dat de opdracht 100% haalbaar is.\nHet project heeft te maken met 2 leden die niet voor 100% meetellen omdat zij ook bezig zijn met veel andere projecten, hierdoor moeten we extra letten op de taakverdeling.\nEen definitieve deadline:\nAls producten later worden afgeleverd kan het ervoor zorgen dat het systeem en netwerk niet binnen de tijd valt van het project, dit kan als gevolg hebben dat de deadline niet wordt behaald.\nOnvoldoende inbreng van de stakeholder\nAls de stakeholder niet genoeg informatie levert kan het voorkomen dat het niet helemaal aan de behoefte van de stakeholder voldoet.\nExterne risico\u2019s:\nCorona:\nWanneer er een grote uitbraak komt en wij niet meer naar school mogen zal dit invloed hebben op het project. Hierdoor is er geen verzekering dat de opdracht 100% haalbaar is.\nOnvoldoende tijd voor besluitvorming:\nWanneer er bij alleen maar idee\u00ebn naar voren komen maar er niks definitief gebruikt wordt en er geen besluiten worden genomen loopt het project vast wegens gebrek aan voortgang.\nOnduidelijke projectgrenzen, afbakening:\nEr moeten duidelijke afspraken gemaakt worden over wat nou wel en wat nou niet relevant is voor het project, kortom er moet voorkomen worden dat er onnodige onderdelen in het project verwerkt worden. \n\n\n"
        },
        {
          "text": "\n\nVersiebeheer\n\n\nHoe kunnen medewerkers toegevoegd en verwijderd worden?\nMedewerkers die kunnen worden toegevoegd en verwijderd worden doormiddel van procedures. Afhankelijk van de functie wordt er gekeken welke rechten zij hebben tot onderdelen.\nOnboarding van medewerkers\nAls er een nieuwe medewerker in het bedrijf komt wordt het volgende proces gerealiseerd:\n\n\n\n\n\nOffboarding van medewerkers\nAls er een medewerker weggaat in het bedrijf komt een soort gelijk proces, zoals weergegeven is:\n\n\n\nSelfservice portal\nVoor het selfservice portal kunnen verschillende varianten gebruikt worden. \nIn het onderzoek worden de volgende selfservice portals meegenomen worden:\nPipefy\nPersonio\nMintHCM\n\nScript\nEen andere mogelijkheid dan een selfservice portal is een script te runnen waarbij zelf de gegevens worden ingevuld.\nDit gaat dan doormiddel van een orchestration tool en in ons geval Ansible. \nConclusie\nUiteindelijk is er gekozen voor een optie waarbij gebruik wordt gemaakt van een script. Dit omdat het niet relevant is in de evenementen sector doordat er vaak maar 1 kassa gebruikt wordt.\n\n\nHoe wordt deze omgeving beveiligd?\nDe omgeving die wordt gebruikt om kassasystemen uit te rollen moet beveiligd worden. Voor de beveiliging kan er aan een aantal onderdelen gedacht worden, zoals:\nFirewall\nRegels voor het verkeer\nPoorten open of dicht\nVPN service\nSIEM\nMonitoren van attacks \nMonitoren van het netwerkverkeer\nKassa\u2019s\nFirewall systeem\nAnti virus software\nFysieke toegang belemmeren\nUser restricties\nAlleen kassa software weergeven (meer niet)\n\nFirewall\nVoor de firewall is er onderzoek gedaan naar verschillende soorten firewall software. Er is een overweging gemaakt tussen de volgende firewalls:\nOPNsense\nPFsense\nSophos\nUntagle\nUiteindelijk is gekozen voor de PFsense variant aangezien deze opensource is en alle medewerkers hebben ervaring met pfsense. \nSIEM\nVoor de SIEM is er gekozen voor ELK Stack in combinatie met Kibana. Dit komt doordat de docent dit aanraadde. \nKassa\u2019s\nDe kassa\u2019s die draaien op Linux, dus wordt daarop standaard firewall ufw opgezet. De antivirus software die we op de kassa\u2019s gaan zetten is AVG.\nOm de fysieke toegang te belemmeren zorgen we dat alle usb poorten geblokkeerd zijn.\nGebruikers die de kassa\u2019s gebruiken mogen alleen toegang hebben tot de kassa software. Daarbij krijgen zij geen administrator rechten. De administrators hebben alleen toegang tot de rest en dat is Doekoe.\n\n\nWelke systemen zijn benodigd?\nVoorbeeld netwerk diagram:\n\nFirewall\nEen firewall is benodigd om de beveiliging van het uit te rollen netwerk te verbeteren.\nDatabase\nEr is een database benodigd om onder andere transacties op te slaan\nMonitoringserver\nEen monitoringserver is benodigd om de systemen op een overzichtelijke manier te kunnen monitoren. En in te kunnen grijpen wanneer voordat er iets misgaat.\nOrchestrationserver\nEr is een orchestrationserver benodigd om het uitrollen van de systemen te kunnen organiseren en automatiseren. Daarnaast helpt dit om consistentie tussen verschillende installaties te organiseren.\nKassa\u2019s\nDit zijn de client-machines waarnaar de kassa software uitgerold moet kunnen worden.\n\n\n\n\nOp welke manieren kunnen kassasystemen worden uitgerold?\n\nOnze stakeholder geeft aan dat ze momenteel gebruik maken van PXE servers of van een USB stick om de images mee over te zetten, zij zijn daar zelf zeer tevreden over, toch willen we nog even kijken naar eventuele anders mogelijkheden.\nWat gebruikt de stakeholder nu:\nKaseya*\nPXE server\nUSB\n*Wij kunnen geen gebruik maken van Kaseya aangezien dit betaalde software is. Met deze software kan je zowel RDP als commando\u2019s uitvoeren op de host PC.\n\nPXE (images)\nMinimale benodigdheden PXE:\nDHCP server\nTFTP server\nEen mogelijke all-in one oplossing is Serva voor windows:\n\nDeze zorgt dus ook voor de DHCP en TFTP en uiteraard nog veel meer, dit zou een makkelijke oplossing zijn omdat het alles in een is.\nEen andere optie is TFTPD32:\n \nOok deze applicatie heeft alle benodigdheden in zich om een PXE op te zetten, ook is hij IPV6 ready.\nAls laatste kunnen we Windows ook zelf gaan aanpassen om er een TFTP van te maken met alle benodigdheden, dit heeft niet mijn voorkeur omdat we daar allemaal nog niet veel kennis van hebben.\n\nConfiguratie OS\nMomenteel zijn wij met challanges vooral bezig geweest met Ansible daarom hebben wij daar een grote voorkeur voor ontwikkeld. Eventuele andere manieren zijn:\nPuppet\nEngage\nChef\nKaseya\n\n\nHoe wordt er gezorgd dat de systemen gemonitord kunnen worden?\nOm deze vraag te beantwoorden moeten we eerst kijken naar welke systemen zijn er en wat willen we hier van monitoren. Ik zal dus eerst alle systemen opsommen en wat hiervan gemonitoord kan worden. Hierna zal ik onderzoeken hoe dat we dit gaan monitoren.\nSystemen\nHet project bestaat uit de volgende machines/systemen\nProxy\nSQL database\nFirewall met VPN\nRouter\nKassa systeem\nPXE\nAnsible\nBackup server\nWebserver voor medewerkers\nWat kan er worden gemonitord?\nHieronder een aantal mogelijkheden wat er allemaal kan worden gemonitord\nLogin\u2019s\nAanpassingen\nVerkeer\nPerformance (CPU, storage, memory)\nError\u2019s\nWeb certificaten\nHoe gaan we dit monitoren?\nWe moeten hiervoor 2 soorten monitoring aanhouden. We gaan namelijk werken met logs die worden gegenereerd door meerdere devices en we gaan werken met een monitoring systeem die bijvoorbeeld de performance, errors en webverkeer kan monitoren.\nLogs\nHet mooiste zou zijn als we een centraal loggingstation hebben die alles bijhoud en verzameld. Ik heb hier research naar gedaan hiervoor zijn verschillende mogelijkheden. Deze zijn te vinden via onderstaande link. Later zal nog een keuze worden gemaakt welk logging systeem het beste voor ons werkt.\n\n\nMonitoring\nVoor monitoring zijn er ook veel verschillende opties echter ik heb tijdens een challange monitoring kennis op gedaan met zabbix waar ik nog graag meer over wil leren. Dus ik raad zabbix aan als monitoring systeem hierin zitten voldoende opties om alles te kunnen monitoren.\n\n\nWat is het budget voor het project?\nVia de stakeholder is er geen direct budget opgelegt maar om toch grensen aan te geven gaan we uitzoeken hoeveel de onderhoud van het huidige systeem kost incl het uitdraaien van kassa\u2019s. Al deze gegevens zijn helaas niet accuraat omdat we geen kennis hebben over het huidige systeem. Maar deze gegevens zullen wellicht wel een goed beeld geven over de kosten van het bedrijf. Als we de kosten weten dan kunnen we uitzoeken hoeveel geld we voor ze kunnen verdienen door een betere oplossing. Hiermee kunnen we dan het budget van het project bepalen.\nBudget\nEr is in principe geen budget voor het project. Dit betekent dat alles zo laag mogelijk moet zitten in kosten. Daarom zal er veel opensource zijn. In het positiefste geval kunnen we het pitchen aan de stakeholder hij kan dan kijken of er iets mee gedaan kan worden. De huidige kosten van het systeem zijn erg hoog dus zolang we veel opensource aanhouden is ons systeem al snel voordeliger dan het huidige systeem van de stakeholder.\n\n\n\nHoe snel kunnen kassa\u2019s uitgerold worden?\nToegepaste methodes\nInleiding\nOm te bepalen hoe snel kassa\u2019s ongeveer uitgerold kunnen worden is het nodig om te weten welk besturingssysteem gebruikt gaat worden, hoe de systemen geconfigureerd gaan worden en wat de specificaties van de kassasystemen zijn.\nSituatie\nOmdat op het moment van schrijven de software/scripts om de installatie te regelen nog niet geschreven zijn beperken de resultaten van dit onderzoek tot de installatie van het besturingssysteem.\nIn het onderzoek wordt uitgegaan van een installatie van CentOS inclusief GUI. De hardware specificaties zijn op dit moment ook nog niet bekend. Daarom wordt voor de computer simulatie (labonderzoek) gebruik gemaakt van een virtuele machine op het netwerklab met de standaard configuratie (2x CPU (1 socket, 2 cores), 4GB RAM, 80GB Disk).\nIn deze vraag wordt alleen beantwoord hoe snel een losse kassa ge\u00efnstalleerd kan worden, niet hoe snel een heel netwerk inclusief databaseserver uitgerold kan worden!\nResultaten\nCommunity research\nUit onderzoek op internet blijkt volgens een gebruiker op het CentOS forum dat een normale installatie maximaal 30 minuten duurt. In deze situatie is het niet duidelijk wat het installatiemedium (bijv. PXE, USB of DVD) is en of het ge\u00efnstalleerde systeem een HDD of SSD heeft. In hetzelfde topic benoemd iemand dat een geautomatiseerde installatie maximaal ongeveer 10 minuten duurt.\nBronnen:\n\nComputer simulation\nOm te achterhalen hoe snel een installatie plaatsvind in een realistischere situatie is gemeten hoe snel een installatie duurt op het netwerklab. De specificaties van de test virtuele machine zijn:\n2x CPU (1 socket, 2 cores)\n4GB RAM\n80GB Disk\nDe installatie vond plaats met CentOS 8 en de standaard Server with GUI instellingen. Het resultaat zal in de praktijk afwijken doordat dan niet alle standaard pakketten ge\u00efnstalleerd dienen te worden en de post-installation nog moet plaatsvinden. \nIn de test is gemeten vanaf het moment dat de installatie start, het handmatig invoeren van de gegevens is van het eindresultaat afgetrokken omdat het straks \u2018in bedrijf\u2019 automatisch zal plaatsvinden. Uit de test kwam als resultaat dat een installatie 14 minuten en 2 seconden duurt.\nConclusie\nUit het onderzoek komt naar voren dat een installatie van een kassa ongeveer 15 minuten zal duren. Omdat het op het moment van het onderzoek de soft- en hardware nog niet bekend is kan het daadwerkelijke resultaat afwijken van de gegeven 15 minuten.\n\n\n \nHoe worden technische problemen opgelost?\nInleiding\nOm technische problemen op te lossen moeten deze ergens gemeld kunnen worden. Daarna moet bepaald worden wie het probleem gaat oplossen.\nAntwoord\nDe beste oplossing is om een service desk op te zetten. Om de klanten optimaal te kunnen helpen is het beter om de service desk bij ons te vestigen (voor de klant dus extern). De klanten kunnen bij problemen een melding maken in een service-management tool (bijvoorbeeld TOPdesk) waarna de prioriteit en de vereiste kennis van het probleem bepaald kan worden. Bij kleine problemen kan een eerstelijnsmedewerker het probleem oplossen, bij problemen die meer technisch van aard zijn of een grotere prioriteit hebben wordt doorverwezen naar een tweedelijnsmedewerker. De medewerker zal bepalen hoe de klant het beste geholpen kan worden om problemen op te lossen.\n\nWelke Linux distributie kunnen we het beste gebruiken?\nToegepaste methodes\nInleiding\nBinnen het project gaat er onder andere Linux gebruikt worden. Omdat er veel distributies zijn met verschillende doelgroepen is het belangrijk om \u00e9\u00e9n goede keuze te maken om een ratjetoe aan verschillende distributies te voorkomen.\nResultaten\nAvailable product analysis\nUit mijn Available product analysis is gekomen dat de meeste bronnen het gebruik van CentOS, Debian of Ubuntu LTS aanbevelen om te gebruiken als serverbesturingssysteem. De reden hiervoor is de goede support vanuit Canonical  bij Ubuntu en de open-source community.\nCentOS is afgeleid van het commerci\u00eble Red Hat. CentOS is een besturingssysteem richt zich vooral op servers en door de Red Hat basis een goede ondersteuning heeft en zeer stabiel is. CentOS wordt 5 jaar voorzien van volledige updates en daarna nog 5 jaar van onderhoudsupdates waardoor een CentOS-versie in totaal 10 jaar ondersteund wordt.\nDebian is ook een populair (server)besturingssysteem. Debian staat vooral bekend om zijn stabiliteit. Packages worden eerst uitgebreid getest voordat deze in Debian stable opgenomen worden. Hierdoor kan het voorkomen dat de nieuwste software niet altijd ondersteund wordt. Doordat er geen commercieel bedrijf achter Debian zit is de er minder ondersteuning beschikbaar vanuit bedrijven en meer vanuit de online community. Elke Debian versie wordt 5 jaar ondersteund.\nUbuntu is een besturingssysteem gebaseerd op Debian. Ubuntu wordt vooral geprezen door zijn populariteit en goede hardware support. Ubuntu wordt ook goed commercieel ondersteund door Canonical, het bedrijf achter Ubuntu. De LTS (Long Term Support) versie wordt 5 jaar ondersteund met een optie om bij Canonical 5 jaar extra ondersteuning te kopen.\nToegepaste bronnen:\n \n\nCommunity research\nUit het community research is naar voren gekomen dat gebruikers vooral CentOS en in mindere mate Debian adviseren als server besturingssysteem. CentOS wordt vooral geadviseerd omdat het afgeleid is van Red Hat, goede en lange ondersteuning heeft en als stabiel ervaren wordt. Debian wordt vooral aangeraden doordat het als zeer stabiel ervaren wordt. Ubuntu wordt ook niet afgeraden, het voordeel van Ubuntu is dat het namelijk snel voorzien wordt van nieuwe packages, maar het is afhankelijk van de situatie of dat belangrijk is.\nToegepaste bron:\n \nInterview\nBinnen de projectgroep is gevraagd wat de leden prefereren om te gebruiken als serverbesturingssysteem.\nDe resultaten zijn:\nConclusie\nUit het onderzoek is naar voren gekomen dat zowel CentOS als Debian als goede serverbesturingssystemen ervaren worden. Ook Ubuntu wordt als een goed besturingssysteem ervaren, maar is mogelijk minder stabiel dan CentOS en Debian doordat er sneller nieuwere packages uitgerold worden.\nOmdat CentOS en Debian beide goed beoordeeld worden en het lastig is om een objectieve conclusie te trekken is besloten om de groepsleden te interviewen om te achterhalen welk besturingssysteem het beste gebruikt kan worden. Hieruit is naar voren gekomen dat Debian als beste uit de bus komt. De voordelen van Debian zijn: de goede kennis binnen de groep, Raspberry Pi\u2019s draaien standaard op een Debian-based besturingssysteem en de goede stabiliteit.\nNog wel onbekend bij de groepsleden is hoe Ansible op Debian draait. Momenteel hebben de groepsleden vooral gewerkt met Ansible i.c.m. CentOS. Mogelijk gebruiken we dus voor de Ansible server toch CentOS en voor de rest Debian."
        }
      ]
    },
    "TECforSocietyEndoftheyearpresentation": {
      "hand-ins": [
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDigitale aansluiting\n\nDigitale aansluiting\n\nMartin Ederveen & Elmira Drost\n\n\n\nKleine introductie van de naam van het project en een kort welkom praatje.\n1\n\n\nInhoud\nInleiding\nProbleem\nQR-codes\nVragen\n\n\n\nKort doornemen alle punten waar we het over gaan hebben\n2\n\n\n\n\n\n\nInleiding\nWijEindhoven / Buurt in bloei\nLector Sociale studies\nFontys ICT\nVitalis Kronehoef\n\n\n\nHoe is dit project tot stand gekomen?\nDoormiddel van het lectoraat en WijEindhoven met de afdeling Buurt in bloei. Gekeken naar probleem en vraag stukken en toen kwam \nVitalis om de hoek kijken\nWie werkt er allemaal mee aan dit project?\n3\n\n\nProbleem\n\n\n\n\nProbleem:\nOuderen die vinden het gebouw vaak onbekend en zien het als doolhof\nHierdoor kunnen zij in sommige gevallen de huisarts bijvoorbeeld niet vinden en willen ze daardoor niet naar de huisarts\nOuderen gaan simpelweg niet naar activiteiten doordat ze de weg niet weten.\n4\n\nGebouw vaak onbekend\n\n\nFaciliteiten kunnen niet gevonden worden\n\n\nOuderen gaan niet meer naar activiteiten\n\n\n\n\n\n\n\nOplossing\nRondleiding met digitale ondersteuning\nQR-codes\nWebsite met informatie\nVideo\u2019s\n\n\n\nDe oplossing:\nDit probleem gaan wij tackelen door middel van een 'digitale rondleiding'. Met deze rondleiding leren de bewoners het gebouw beter kennen en zullen zij sneller naar activiteiten gaan waardoor de eenzaamheid wordt verminderd.\nDe rondleiding bestaat uit een QR-code route, deze route lopen de ouderen met een buddy. Deze buddy helpt de ouderen met het scannen van de QR-codes. Achter de verschillende codes staat informatie: dit varieert van openingstijden van de huisarts tot een leuke video van het caf\u00e9 en meer.\n5\n\n\nVragen?\nBedankt!\n\n\n\n\n\nAfsluiting en een vragemoment.\n6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/docProps/thumbnail.jpeg\n\n"
        }
      ]
    },
    "TECforSocietyTechnicalplan": {
      "hand-ins": [
        {
          "text": "\nVersiebeheer\n\n\n\nInleiding\nDit document is opgesteld naar aanleiding van het project Tec For society digitale aansluiting. Dit document dient een duidelijk beeld te geven op het gebied van de functies binnen het technische stuk van het project.\nDoel\nHet doel van het project is dat (nieuwe) bewoners het gebouw beter kunnen leren kennen. Dit doormiddel van een tablet waarmee zij een QR-code kunnen scannen en daarbij doorverwezen worden naar een website met alle benodigde informatie.  \nHieronder een voorbeeld hoe dit in zijn werk kan gaan:\n\nOntwerp\nTablet\nDe tablet die zal zo ingericht worden dat gebruikers gemakkelijk QR-Codes kunnen scannen. Dit gaat doormiddel van de camera applicatie die standaard op de tablet ge\u00efnstalleerd staat.\nMomenteel is alleen de voorkant camera zichtbaar en de achterkant camera niet doordat het hoesje ervoor zit. De QR-codes kunnen doormiddel van de voor camera gescand worden als de gebruiker de camera goed voor de QR-code houden.\nAls er andere hoesjes aangeschaft worden dan kunnen de QR-codes ook via de achter camera gescand worden wat het wellicht makkelijker maakt. De gebruiker heeft dan zelf de mogelijkheid om te kijken op het tablet scherm waar de camera naar toe richt. \nQR-code\nVoor ieder punt waar informatie opgevraagd moeten worden is een QR-code nodig. De QR-code is per plaat specifiek met de juiste link.\nNetwerk tekening\nHieronder is de netwerktekening te zien van hoe het systeem ongeveer in elkaar zit. \nDe gebruiker scant de QR-code met de tablet (of telefoon)\nDe tablet die verbinding heeft met Wi-Fi ziet dat de QR-code gescand wordt\nDe tablet vraagt via het internet informatie op\nHet internet die vraagt bij de website de juiste pagina op\nDe website geeft de informatie terug aan het internet\nHet internet stuurt via de Wi-Fi de informatie naar de telefoon\n\n\nWebserver\nDe informatie die opgevraagd wordt doormiddel van de QR-code moet op een website komen te staan. Deze website met informatie komt op een webserver. Deze webserver moet gehost worden, dit kan misschien binnen Vitalis en anders extern.\nExterne hosting\nAls de website op een externe locatie gehost gaat worden zitten daar kosten aan vast, hieronder een simpele vergelijking tussen de verschillende hostingsplatforms waarbij Wordpress wordt gehost:\n* Eenmalige setup kosten van \u20ac5\nDomein naam\nAls het van belang is dat de website ook een eigen domein naam heeft moet deze gekocht worden. \nHostnet\nHieronder is een schema met de verschillende kosten van hostnet:\n* Gratis bij een hostnet abonnement voor de hosting\nYourhosting\nHieronder is een schema te zien met de verschillende kosten van yourhosting:\nStrato\nHieronder is een schema te zien met de verschillende kosten van Strato:\n"
        }
      ]
    },
    "WifiAttendanceAnsibleMonitoringscript": {
      "hand-ins": [
        {
          "text": "\n\nVersion control\n\n\n\nIntroduction\nThis document is set up for the description of the Ansible Zabbix roles script, this document can make sure that the purpose of the script is clear, as well as it functionality. The script can be used to setup a Zabbix server.\nI chose Ansible roles, this is a functionality inside Ansible where you can import extra functionality (roles).  Ansible roles are new to me and I was very curious how this works, I wanted to learn how to import roles and how to use them.\nThis document will contain the following items:\nWhy is it needed?\nWhat is the functionality of the script?\nThe code of the script\nThe result when running the script\nConclusion\n\nWhy is the script needed?\nThis script makes it easy to install the Zabbix server, Zabbix will be installed in a very short time through Ansible. Without any knowledge of Zabbix you can install it.\n\nWhat is the functionality?\nEach script has different functionality which is needed for Zabbix to work:\nInstallation of the Database\nThe installation script of the Database will make sure that MYSQL is being installed. It looks at what kind of  operating system is being used. Based on the operating system it will use different variables when running the script. \nIt will install MYSQL on the correct database and it sets the default username and password for the database server. \nInstallation of the Zabbix server & Apache\nThe installation script of the Zabbix server and Apache makes sure that both are installed. The apache script will install apache with its minimal requirements, no configuration is being add.\nThe installation script will also install Zabbix server, which uses the previously installed database (MYSQL). It will also install the Zabbix-web, which means a Zabbix web interface. \nConfiguration of Apache\nThe script of the configuration of apache makes sure that the Zabbix web interface is linked to the Apache server. This means that apache will be configured, after the script has run you can access the web interface of Zabbix.\n\nCode\nInstallation of the Database\n\nThe link to the Github repository of the installation of the Database:\n \n\nInstallation of the Zabbix server & Apache\n\nThe link to the Github repository of the installation of Zabbix & Apache:\n \n\n\n\nConfiguration of Apache\nThe link to the Github repository of the configuration of Apache:\n \n\nResult\nBefore we can actually run the Ansible scripts we need to install Ansible roles. When you want to use Ansible roles they need to be added in the Docker container in the AWX server. You need to go into the docker container of AWX by the following command:\ndocker exec -it awx_task bash\n\u00a0When you are logged into the container you need to run the following commands:\nansible-galaxy collection install geerlingguy.mysql\nIn our case we need to add the following roles:\u00a0\ngeerlingguy.mysql\u00a0\ndj-wasabi.zabbix-server\u00a0\ndj-wasabi.zabbix-web\u00a0\nWhen the roles are being installed you can now run the scripts.\n\n\nDatabase install\n\n\nZabbix install\n\n\nApache config\n\n\nDashboard end result:\n\n\nConclusion\nI got to learn how to work with Ansible roles which was new to me. I think it is a really cool concept to easily install different parts of software. \nIn the future I would like to use Ansible roles again since it gave me new knowledge, I would love to see everything I can do with Ansible roles and I take this new knowledge with me to the next semester! "
        }
      ]
    },
    "WifiAttendanceInfrastructureDocumentation": {
      "hand-ins": [
        {
          "text": "\n\nVersion control\n\n\nIntroduction\nThis document is meant to be a user manual. If new students are going to work on this project, this document can be used as deploy guide as well as system documentation.\nIn this document will be described, how the network is setup and servers configured. Not only will the current configuration be addressed but several recommendations will be made.\n\nNetwork design\nDesign\nNetwork diagram\n\nFiguur 1 networkdiagram\nDesign considerations\nIn this part the different considerations and choices that led to this design. The full .vsdx file can be found in either the teams channel or canvas.\nSeclab\nThe choice to host the network on seclab was made because it offers a lot of control on how the network could be implemented, and its built-in tools for snapshotting and redeploying are very useful in development stage.\nSingle network design\nIn developing the network design some of the following arguments were taken in consideration. Since it is very likely that in some point this project will be further developed without a team-member with infrastructure-specializations. It would be beneficial to keep the design relatively simple, and easy to maintain. This has as a result that a network was developed with a single local network. While behind a single easy to maintain Firewall, which would keep the network secure. \nSeclabvpn\nThe choice to include a network interface connected to the network on seclab was made for the following reason. Since one of the design requirements was that it should be simple to manage for less experienced people, the choice for an existing vpn managed by the organization was an easy choice. This means that in the event of an error or an outage support can be requested from the IT-department. But by including the vpn in the design, multiple managing interfaces can be port forwarded to a network that can only be accessed by vpn, thus increasing ease of use and security.\nProxy\nSince a proxy machine has been deployed, this means that the responsibility for enabling ssl support has been taken away from the software developers. This means that they can focus more on functionalities and have less work on security. However, to ease troubleshooting for the software team the cluster can still be reached using the insecure http protocol. Use of this in production is not recommended.\nDocker\nAfter meeting with the software-development team, it was agreed upon to have the applications natively installed since at that time it was the belief that this would be easier for the software team than a containerized solution. However, after some development time, the decision was made to run all software in docker-containers. Since this would minimize the effect of configuration disparity between development and production environments, hence making it less time consuming to deploy and develop.\nRecommended changes\nChanges that could be implemented to increase security would be to switch to a split network design. Using a separate network here the proxy can be deployed which means that it becomes easier to limit undesired traffic between proxy server and the rest. An extra change that could be made is. To scale the single docker machine to a docker cluster. This would increase the capacity of the system and would improve redundancy. Also, deployment of a complete second cluster could be used in conjunction with the reverse proxy to increase failover capacity and add extra options for a more staggered updated of newer versions, allowing to prevent downtime in case of a failed version.\nFirewall configuration\nFirewall choice\nPFsense was chosen as firewall software, since it is not only free and open source, but it also has a large userbase, in case something goes wrong and support needed to be found. And since it has a lot of modules which can be added to increase functionality in the future, Pfsense was chosen.\nFirewall install\nCreate new virtual machine\nFirst create a new virtual machine in the esxi environment seclab.\nSelect desired resources: minimum of 2 cores, 4GB RAM and 20GB storage is recommended\nAdd network interface used as WAN: in our configuration this would be: 0131-DMZ131\nAdd network interface used as LAN: in our configuration this would be: 0522_PROF49_PVlanA\nAdd network interface used as OPT2: in our configuration this would be: 0051-INTERNET-STATIC\nInstall Pfsense\nThe default guided installation can be followed\nAdd correct interfaces to PFsense\nThe interfaces must be added to Pfsense for it to function\nAdd network interface used as WAN: in our configuration this would be: 0131-DMZ131\nAdd network interface used as LAN: in our configuration this would be: 0522_PROF49_PVlanA\nAdd network interface used as OPT2: in our configuration this would be: 0051-INTERNET-STATIC\n\nFirewall configuration\nAfter PFsense has been installed any further configuration can be done using the web interface.\nPortforwarding\n\nFiguur 2: port forwarding rules\nTo make sure all servers can be reached when needed port forwarding is required. The current configuration differs from a fully production configuration in a few places. For example, the port forwarding to database would be disabled. As well as the direct port forwarding to docker-cluster. However, for development purpose on the software side this configuration was chosen.\n\nOrchestrating using AWX \nIn this chapter there will be a little manual on how to use Ansible AWX. This done because students who are new to AWX can have a quick and basic understanding of AWX. \nAfter running the  you can go to local host and see the following page:\n\nHere, you can login using the default login credentials which is the following:\nUsername: admin\nPassword: password\nDue to security issues, it is suggested to change the default login credentials.\nAfter you have logged in you see the dashboard which looks something like this:\n\nHere you can see the added host, the last scripts that you ran. The graph shows the failed (red) and  the succeeded (green) scripts.\n\nYou can change the default user login credentials under the tab users which is in the right. You can add new users or change the details of the user info, which will look something like this:\n\nYou can add hosts by the tab Hosts, when you click on add you can add host through the IP-address.\n\nAfter adding the host you can also add them to inventories, by clicking on the inventories tab on the left, then on the specific inventory, then add the hosts on the tab host. \n\n\n\nBefore you can use Ansible, you need to add credentials. This can be credentials to the VM the username and password. As well as the tokens for Gitlab/Github. You can add these under the tab credentials and then Add.\n\nAfter the credentials are added you can make projects. You can see projects as a kind of repository where all the scripts are being located. To add a project you need to go to the tab Projects, then click on add.\n\nYou can add different kind of projects but in this tutorial we are going to use Git. When adding a project you need to add the source type, which is in our case Git. You also need to add the source URL. When adding the URL, you need to make sure that it ends with .git , you can also choose a source branch in that Git repository, in our case we used the branch Development. Since it is a private repository on Gitlab, we need to add the source credentials which we filled in earlier. After that it is created and finished.\n\n\n\nAfter adding the credentials, and adding the projects we can create templates! These templates will make sure that you can run the YML-scripts. You can Add templates, by the button Add under the section templates. \n\nYou need to fill in the job type:\nRun  will actually run the script\nCheck  will check of the script can be run\nYou can set the organization to default, the inventory to the set of hosts that you need in our case the WiFi-attendance inventory, and the project where the scripts will be taken from. After that you can chose the playbook, which is the YML script what will be run. \n\n\nWhen the YML script is saved you can launch it. After it is being launched you can check the output with the results. Down below you can see that it has been successful. You can see that 8 points are ok (no change needed) and 2 points changed.\n\n\n\nServers\nGeneral information\nAfter meeting with the software-development team, it was agreed upon to have the applications natively installed since at that time it was the belief that this would be easier for the software team than a containerized solution. However, after some development time, the decision was made to run all software in docker-containers. Since this would minimize the effect of configuration disparity between development and production environments, hence making it less time consuming to deploy and develop.\nCentOS is used because they offer very long support, up to 10 years per release. Besides the long support CentOS is well integrated with the Red Hat ecosystem since it has been derived from Red Hat enterprise Linux(RHEL for short). Red Hat has a controlling interest in the CentOS project and help it develop with both funding and developers. Red Hat is the biggest open-source company in the enterprise world and is known for its stability, and support. \nHowever, since the start of development of this project EOL of centos8 has been announced. Possibilities to migrate to are RHEL8 which is almost identical but requires a subscription. Or the use of centos-stream, which will start to differ more and more from centos8 but at time of writing, most if not all steps described will be identical. \n\n\nManaging server \nThe managing server is being set up for the automation and orchestration of other servers. This server is being served as an centralized point from where you can install, configure and update all the other servers. \nFor the orchestration and configuration Ansible is being used. Why Ansible? Why not Puppet? Why not any other tool?\n is being used since it is well known inside the working group, besides that there is a lot of good documentation out there for the usage of Ansible. \nThe managing server uses a feature called . Ansible AWX is an open-source tool which can be used in combination with Ansible. AWX offers a dashboard which makes everything visual, because it is visual a lot of fist time users can find their way by navigating through the AWX. \nWhen only using the Ansible CLI it is also possible to run scripts, but this will have a steeper learning curve for new users. We used Ansible AWX instead of Ansible Tower since Ansible AWX is open-source and not paid.\nThe managing server is available when logging into the VPN, so you don\u2019t have to login on a local machine every time you want to use the AWX.\nThe managing server is set up on Netlab of Fontys ICT. There has been a request for the Netlab to have an account with a public IP-address. \nThe managing server has the following parameters:\n\nFor more info on how to install Ansible AWX, see the following . \n\n\nMonitoring server\nThe monitoring server is being setup to monitor all the other servers. This server is checking if all the other servers are online and if the services are online.\nFor the monitoring server Zabbix is being used. Why Zabbix? Why not any other monitoring software?\nZabbix is being used since Daan and I both wanted to learn Zabbix, how to use it, how we can automatically deploy it. Zabbix focuses more on hosts and not that much on the visualization. We do not need much visualization because we are mainly focusing on just the basics of monitoring and not that much on the looks.  \nSetup of the machine\nTo set up the server you need to add the following parameters to Netlab:\n\n\n\nWhen that step is done you can install Zabbix in 2 different ways with Ansible:\nBy using Ansible Roles\nWhen you want to use Ansible roles they need to be added in the Docker container in the AWX server\n\nIn our case we need to add the following roles:\ngeerlingguy.mysql\ndj-wasabi.zabbix-server\ndj-wasabi.zabbix-web\ngeerlingguy.apache\nAfter adding the roles you need to run the following Ansible scripts:\nThe install of mysql:\n \nThe install of Zabbix server:\n \nConfiguration of apache:\n \nBy using a default Ansible script\nAnsiblescripts/monitoring/install_zabbix.yml\n\n\nAdding hosts to Zabbix\nWe chose to use the function of Network discovery inside Zabbix. With this functionality Zabbix can scan the network and add hosts/servers to Zabbix hosts, when they machines are running Zabbix agent. Due to this function we don\u2019t need to add all the new hosts/server manually and saves us a lot of time.\nThe network discovery is running every 10 minutes on the Zabbix server. It is configured to check in the local network of 192.168.1.1 -254. It is checking if the Zabbix agent is running on every machine, with the following configuration:\n\nNotifications\nThe monitoring server is sending out messages when a message trigger severity is greater than or equals high. These messages are being sent out to Microsoft Teams, so anyone from Fontys can read the messages (only when they are in the Teams channel).\n\nTriggers | Templates\nZabbix is monitoring based on different templates. These templates are by default in the Zabbix server. We configured our Zabbix with the following monitoring templates:\nTemplate Module Linux block devices by Zabbix agent\nIt looks at the following triggers:\nThis trigger might indicate disk {#DEVNAME} saturation.\nTemplate Module Linux CPU by Zabbix Agent\nIt looks at the following triggers:\nLoad average is too high (per CPU load over {$LOAD_AVG_PER_CPU.MAX.WARN} for 5m)\nHigh CPU utilization (over {$CPU.UTIL.CRIT}% for 5m)\t\nTemplate Module Linux filesystems by Zabbix agent\nIt looks at the following triggers:\n{#FSNAME}: Disk space is critically low (used > {$VFS.FS.PUSED.MAX.CRIT:\"{#FSNAME}\"}%)\n{#FSNAME}: Disk space is low (used > {$VFS.FS.PUSED.MAX.WARN:\"{#FSNAME}\"}%)\t\n{#FSNAME}: Running out of free inodes (free < {$VFS.FS.INODE.PFREE.MIN.CRIT:\"{#FSNAME}\"}%)\t\n{#FSNAME}: Running out of free inodes (free < {$VFS.FS.INODE.PFREE.MIN.WARN:\"{#FSNAME}\"}%)\t\nTemplate Module Linux generic by Zabbix agent\nIt looks at the following triggers:\nSystem time is out of sync (diff with Zabbix server > {$SYSTEM.FUZZYTIME.MAX}s)\nSystem name has changed (new name: {ITEM.VALUE})\t\nConfigured max number of open filedescriptors is too low (< {$KERNEL.MAXFILES.MIN})\nGetting closer to process limit (over 80% used)\t\nOperating system description has changed\t\n/etc/passwd has been changed\t\n{HOST.NAME} has been restarted (uptime < 10m)\t\nTemplate Module Linux memory by Zabbix agent\nIt looks at the following triggers:\nHigh memory utilization ( >{$MEMORY.UTIL.MAX}% for 5m)\t\nLack of available memory ( < {$MEMORY.AVAILABLE.MIN} of {ITEM.VALUE2})\nTemplate Module Linux network interfaces by Zabbix agent\nIt looks at the following triggers:\nInterface High error rate (for 5m)\nInterface Link down\nInterface Ethernet has changed to lower speed than it was before\t\nNow we know which templates are being used, but why? We know the triggers, but we don\u2019t really know what they do and why we have them configured:\nTemplate Module Linux block devices by Zabbix agent\nIt looks at read and write requests per seconds, when it takes too long the device gets blocked\nTemplate Module Linux CPU by Zabbix Agent\nPer CPU load average is too high. Your system may be slow to respond.\nCPU utilization is too high. The system might be slow to respond.\nLooking at those items it may be useful to add some CPU cores or to reduce the number of applications that have high CPU usage.\nTemplate Module Linux filesystems by Zabbix agent\nTwo conditions should match: First, space utilization should be above\nSecond condition should be one of the following:\n- The disk free space is less than 5G.\n- The disk will be full in less than 24 hours. \nIt may become impossible to write to disk if there are no index nodes left.\nAs symptoms, 'No space left on device' or 'Disk is full' errors may be seen even though free space is available.\nWhen this happens the device might crash and the data will be useless\nTemplate Module Linux generic by Zabbix agent\nSystem local time of the host.\nSystem host name.\nNumber of users who are currently logged in.\nSystem uptime in 'N days, hh:mm:ss' format.\nThese values can be looked at, when they deviate, action needs to be taken\nTemplate Module Linux memory by Zabbix agent\nThe system is running out of free memory.\nWhen running out of memory the system may occur very slow and some processes might go wrong.\nTemplate Module Linux network interfaces by Zabbix agent\nPossible values of the network interfaces  are:\"unknown\", \"notpresent\", \"down\", \"lowerlayerdown\", \"testing\",\"dormant\", \"up\".\nWhen the network is down, they don\u2019t have access to the internet anymore and the service might be offline.\nThe templates gives a dashboard like this one:\nFor more info on Zabbix templates & triggers:  \n\nDatabase server\nChoices\nFor database software MariaDB was chosen since MariaDB is open source, free, reliable and compatible with legacy mysql commands. Also, MariaDB is very easily to install in most Linux distributions since it is often available in the default repositories.\nCreate new virtual machine\nFirst create a new virtual machine in the esxi environment seclab.\nSelect desired resources: minimum of 2 cores, 4GB RAM and 60GB storage is recommended.\nAdd network interface used as LAN: in our configuration this would be PVLANA\nInstall centos 8\nSelect minimal install and configure the installation with the desired hostname, IP and add ansible user with admin privileges. This user will be used to further configure the machine once the OS is installed.\nAdd hosts to Ansible/AWX\nAdd a new host to the wifi-attendance inventory in AWX and add to group database, centos and monitoring client.\nAdd machine credentials to awx credentials. \nThis will enable the ability to execute plays on the database server.\nRun plays\nInstall.yml\nThe following play installs mariadb on the servers and configures it for development use. Here is the script Install_mariadb.yml\n\nDocker server\nChoices\nThe choice for deploying docker is described here 2.1.1.5\nCreate new virtual machine\nFirst create a new virtual machine in the esxi environment seclab.\nSelect desired resources: minimum of 2 cores, 4GB RAM and 60GB storage is recommended.\nAdd network interface used as LAN: in our configuration this would be PVLANA\nInstall centos 8\nSelect minimal install and configure the installation with the desired hostname, IP and add ansible user with admin privileges. This user will be used to further configure the machine once the OS is installed.\nAdd hosts to Ansible/AWX\nAdd a new host to the wifi-attendance inventory in AWX and add to group docker, centos and monitoringclient.\nAdd machine credentials to awx credentials. \nThis will enable the ability to execute plays on the docker-servers.\nRun plays\nInstall.yml\nThe following play installs docker and docker compose on the servers.\nThis script with explanation can be found here Docker/install.yml\nProxy server\nChoices\nThe choice for which software was to be used was based on several considerations. Nginx was chosen for the following reasons: it is free and open source it is very easily obtainable. It has a very good reputation for being stable and more futureproof than its main competitor apache. Nginx can also be used as proxy. As a proxy Nginx has a lot of possibilities concerning load balancing and failover. Also, it is relatively easy to configure using ansible.\nCreate new virtual machine\nFirst create a new virtual machine in the esxi environment seclab.\nSelect desired resources: minimum of 2 cores, 4GB RAM and 40GB storage is recommended.\nAdd network interface used as LAN: in our configuration this would be PVLANA\nInstall centos 8\nSelect minimal install and configure the installation with the desired hostname, IP and add ansible user with admin privileges. This user will be used to further configure the machine once the OS is installed.\nAdd hosts to Ansible/AWX\nAdd a new host to the wifi-attendance inventory in AWX and add to group proxy, centos and monitoringclient.\nAdd machine credentials to awx credentials. \nThis will enable the ability to execute plays on the proxy-server.\nRun plays\nBasic proxy functionality\nThe following play installs nginx and configures it for proxy functions.\nProxy/configurenginx.yml\nConfigure ssl\nThe following play configures ssl certificates on the proxy server using letsencrypt and enables the use of ssl\nProxy/letsencrypt_proxy.yml\n\n\n\n\nAppendix\nScripts\nInstall_mariadb.yml\nHow does it work\nThis script first installs and starts mariadb. Once completed it will create a root user and configure password. The script also creates a new database with user and password. Finally the mysql firewallports are opened.\nThe script\n---\n- name: MariaDB_install\n  hosts: database\n  gather_facts: true\n  become: true\n  vars:\n    mysql_root_password: \"changeme\"\n    mysql_user_password: \"changemeto\"\n  tasks:\n    - name: install mariadb\n      dnf:\n        name:\n          - mariadb-server\n          - python3-PyMySQL\n        state: latest\n    - name: start mariadb\n      service:\n        name: mariadb\n        enabled: true\n        state: started\n    - name: mysql_root_password\n      mysql_user:\n        login_user: root\n        login_password: \"{{ mysql_root_password }}\"\n        user: root\n        check_implicit_admin: true\n        password: \"{{ mysql_root_password }}\"\n        host: localhost\n    - name: remove remote root\n      mysql_user:\n        check_implicit_admin: true\n        login_user: root\n        login_password: \"{{ mysql_root_password }}\"\n        user: root\n        host: \"{{ ansible_fqdn }}\"\n        state: absent\n    - name: Create database user with name 'bob' and password '12345' with all database privileges\n      mysql_user:\n        name: Noah\n        password: \"{{ mysql_user_password }}\"\n        priv: '*.*:ALL'\n        state: present\n    - name: open firewallport mariadb\n      become: yes\n      firewalld:\n        service: mysql\n        permanent: yes\n        immediate: yes\n        state: enabled\n\n\nDocker/install.yml\nHow does it work?\nFirst the script removes any packages that could conflict with the docker install. Next it ensures all existing packages are up to date. When the check if docker is installed has passed, docker will be downloaded and installed and the installation files deleted. The next step is enabling and starting the docker daemon. Next docker compose will be installed, after a check has been completed if it already exists. Finally the correct firewall ports will be opened.\nThe script\n---\n- name: Install docker and docker-compose\n  hosts: docker\n  become: true\n\n  tasks:\n    - name: remove conflicts\n      dnf: \n        name: podman \n        state: absent\n \n    - name: Upgrade all packages\n      dnf: name=* state=latest\n\n    - name: Check if Docker is installed\n      command: systemctl status docker\n      register: docker_check\n      ignore_errors: yes\n\n    - name: Download the Docker installer\n      get_url:\n        url: https://get.docker.com/\n        dest: /root/install_docker.sh\n        mode: 0700\n      when: docker_check.stderr.find('service could not be found') != -1\n\n    - name: Install Docker\n      shell: /root/install_docker.sh\n      when: docker_check.stderr.find('service could not be found') != -1\n\n    - name: Remove the Docker installer file.\n      file:\n        state: absent\n        path: /root/install_docker.sh\n\n    - name: Enable the Docker daemon in systemd\n      systemd:\n        name: docker\n        enabled: yes\n        masked: no\n\n    - name: Start the Docker daemon\n      systemd:\n        name: docker\n        state: started\n        masked: no\n\n    - name: Check if Docker Compose is installed\n      command: docker-compose --version\n      register: docker_compose_check\n      ignore_errors: yes\n\n    - name: Download and install Docker Compose\n      get_url:\n        url: https://github.com/docker/compose/releases/download/1.27.4/docker-compose-Linux-x86_64\n        dest: /usr/bin/docker-compose\n        mode: 0755\n      when:\n        - docker_compose_check.msg is defined\n        - docker_compose_check.msg.find('No such file or directory') != -1\n\n    - name: open firewallports docker deamon encrypted comunication\n      firewalld:\n        port: 2376-2377/tcp\n        permanent: yes\n        immediate: yes\n        state: enabled\n\n    - name: open firewallports container network discovery\n      firewalld:\n        port: 7946/tcp\n        permanent: yes\n        immediate: yes\n        state: enabled\n\n    - name: open firewallports container network discovery\n      firewalld:\n        port: 7946/udp\n        permanent: yes\n        immediate: yes\n        state: enabled\n\n    - name: open firewallports container ingress network\n      firewalld:\n        port: 4789/udp\n        permanent: yes\n        immediate: yes\n        state: enabled\n\n\nDocker_deploys/api.yml\nHow does it work?\nThis script first installs dependencies that are needed to deploy the api. Next it creates a directory where the repo can be stored. Next it clones the repository and builds the docker image. Finally a container will be started and the required firewall ports opened.\nThe script\n---\n- name: api deploy\n  hosts: docker\n  gather_facts: true\n  become: true\n\n  tasks:\n    - name: install dependencies\n      dnf: \n        name:\n          - git\n          - python3\n        state: latest\n    - name: install dependencies\n      pip: \n        name:\n          - docker\n        state: latest\n    - name: make sure app directory exists\n      file:\n        path: /home/repo/backend\n        state: directory\n    - name: clone repo\n      git:\n        repo: 'https://awx:c_9c6hBptAWrQeCYDSXB@git.fhict.nl/I433819/wifi-attendance-backend.git'\n        dest: /home/repo/backend\n      register: git_finished\n    - name: build image\n      command: docker build --pull --rm -f \"Dockerfile\" -t wifiattendancebackend:latest \".\"\n      args:\n        chdir: /home/repo/backend\n      register: result\n    - name: run container\n      docker_container:\n        name: api\n        image: wifiattendancebackend\n        state: started\n        ports:\n          - \"3000:3000\"\n    - name: open firewallport\n      firewalld:\n        port: 3000/tcp\n        permanent: yes\n        immediate: yes\n        state: enabled\n\n\nDocker_deploys/webapp.yml\nHow does it work?\nThis script first installs dependencies that are needed to deploy the webapp. Next it creates a directory where the repo can be stored. Next it clones the repository and builds the docker image. Finally, a container will be started, and the required firewall ports opened.\nThe script\n---\n- name:\n  hosts: docker\n  gather_facts: true\n  become: true\n\n  tasks:\n    - name: install dependencies\n      dnf: \n        name:\n          - git\n          - python3\n        state: latest\n    - name: install dependencies\n      pip: \n        name:\n          - docker\n        state: latest\n    - name: make sure app directory exists\n      file:\n        path: /home/repo/webapp\n        state: directory\n    - name: clone repo\n      git:\n        repo: 'https://awx:-oVjCQpQxdBKY2xffdje@git.fhict.nl/I433819/wifi-attendance-webapp.git'\n        dest: /home/repo/webapp\n      register: git_finished\n    - name: build image\n      command: docker build --pull --rm -f \"Dockerfile\" -t wifiattendancewebapp:latest \".\"\n      args:\n        chdir: /home/repo/webapp\n      register: result\n    - name: run container\n      docker_container:\n        name: webapp\n        image: wifiattendancewebapp\n        state: started\n        ports:\n          - \"8080:8080\"\n    - name: open firewallport\n      firewalld:\n        port: 8080/tcp\n        permanent: yes\n        immediate: yes\n        state: enabled\n\n\nGeneralstuff/installcockpit.yml\nHow does it work?\nThis script installs cockpit and enables it.\nThe script\n---\n- hosts: all\n  tasks:\n    - name: Make sure Cockpit is installed and install if not\n      dnf: name=cockpit state=latest\n      become: yes\n    - name: start Cockpit\n      become: yes\n      systemd:\n        name: cockpit.socket\n        enabled: yes\n        state: started\n\n\nGeneralstuff/prepare_awx_install.yml\nHow does it work?\nThis script prepares the install for an awx installation it first checks if docker and docker compose are installed next it will pull the awx repo and open all firewall ports.\nThe script\n---\n- name: prepare for Install awx\n  hosts: localhost\n  become: true\n  tasks:\n\n    - name: Check if Docker is installed\n      command: systemctl status docker\n      register: docker_check\n      ignore_errors: yes\n  \n    - name: Check if Docker Compose is installed\n      command: docker-compose --version\n      register: docker_compose_check\n      ignore_errors: yes\n    \n    - name: pull awx repo\n      git: \n        repo: 'https://github.com/ansible/awx.git'\n        dest: /etc/ansible/\n\n- name: Install awx\n  import_playbook: /etc/ansible/awx/installer/install.yml\n\n- name: open firewall ports\n  hosts: localhost\n  become: true\n  tasks:\n    - name: open http\n      firewalld:\n        service: http\n        permanent: yes\n        immediate: yes\n        state: enabled\n\n    - name: open https\n      firewalld:\n        service: https\n        permanent: yes\n        immediate: yes\n        state: enabled\n\n\nGeneralstuff/reboot_all.yml\nHow does it work?\nThis script reboots all hosts unconditionally\nThe script\n---\n- name: reboots all machines unconditionally\n  hosts: all\n  become: yes\n  tasks:\n    - name: reboot all machines\n      reboot:\n\n\nGeneralstuff/synctime.yml\nHow does it work?\nThis script first checks if ntpd is not installed next it will install and start chronyd\nThe script\n---\n- name: sync time\n  hosts: centos\n  gather_facts: true\n  become: true\n  tasks:\n    - name: make sure ntpd is not installed\n      dnf:\n        name: ntpd\n        state: absent\n    - name: install chrony if not installed\n      dnf:\n        name: chrony\n        state: latest\n    - name: start chronyd\n      service:\n        name: chronyd\n        enabled: true\n        state: started\n\n\nGeneralstuff/update_all.yml\nHow does it work?\nThis script updates all hosts.\nThe script\n---\n- name: update all\n  hosts: all\n  become: yes\n  tasks:\n    - name: update all hosts\n      dnf:\n        name: \"*\"\n        state: latest\n\n\nMonitoring/Install_zabbix.yml\nHow does it work?\nFirst this script will install dependencies, next it will configure php and install and start mariabd. Next mariadb will be configured with a root password and a Zabbix database will be created. A Zabbix user will be added to the database. Next the Zabbix repository will be added and Zabbix will be installed. Next a Zabbix sqlscheme will be imported into the database. Next password will be configured, and firewall ports opened. Also selinux will be disabled and the Zabbix-server and Zabbix-agent will be enabled and started.\nThe script\n---\n- name: zabbix_install\n  hosts: monitoring\n  gather_facts: true\n  become: true\n  vars:\n    mysql_root_password: \"changemeplease\"\n\n  tasks:\n    - name: install httpd\n      dnf:\n        name: httpd\n        state: latest\n    - name: start httpd\n      service:\n        name: httpd\n        enabled: true\n        state: started\n    - name: install php\n      dnf:\n        name: \n          - php\n          - php-cli\n          - php-common\n          - php-devel\n          - php-pear\n          - php-gd\n          - php-mbstring\n          - php-mysqlnd\n          - php-xml\n          - php-bcmath\n        state: latest\n        update_cache: true\n    - name: configure php\n      lineinfile:\n        path: /etc/php.ini\n        regexp: 'date.timezone ='\n        line: date.timezone = Europe/Amsterdam\n    - name: install mariadb\n      dnf:\n        name:\n          - mariadb-server\n          - python3-PyMySQL \n        state: latest\n    - name: start mariadb\n      service:\n        name: mariadb\n        enabled: true\n        state: started\n    - name: mysql_root_password\n      mysql_user:\n        login_user: root\n        login_password: \"{{ mysql_root_password }}\"\n        user: root\n        check_implicit_admin: true\n        password: \"{{ mysql_root_password }}\"\n        host: localhost\n    - name: Create zabbix database \n      mysql_db:\n        login_user: root\n        login_password: \"{{ mysql_root_password }}\"\n        name: zabbix\n        state: present\n        encoding: UTF8\n        collation: utf8_bin\n      register: zabbixdatabase\n    - name: Create zabbix user in database\n      mysql_user: \n        login_user: root\n        login_password: \"{{ mysql_root_password }}\"\n        name: zabbix\n        password: Fontysproject2020\n        priv: '*.*:ALL'\n        state: present\n    - name: add zabbix repo\n      dnf:\n        name: \"https://repo.zabbix.com/zabbix/5.0/rhel/8/x86_64/zabbix-release-5.0-1.el8.noarch.rpm\"\n        state: present\n        disable_gpg_check: true\n    - name: install zabbix\n      dnf:\n        expire-cache: true\n        name: \n          - zabbix-server-mysql\n          - zabbix-web-mysql\n          - zabbix-agent\n          - zabbix-apache-conf\n        state: latest\n    - name: import zabbix sqlschema\n      mysql_db:\n        login_user: root\n        login_password: \"{{ mysql_root_password }}\"\n        name: zabbix\n        state: import\n        target: /usr/share/doc/zabbix-server-mysql/create.sql.gz\n      when: zabbixdatabase is defined and zabbixdatabase.changed == True\n    - name: configure dbpassword in zabbix\n      lineinfile:\n        path: /etc/zabbix/zabbix_server.conf\n        regexp: 'DBPassword='\n        line: DBPassword=Fontysproject2020\n    - name: open firewallport http\n      firewalld:\n        service: http\n        permanent: yes\n        immediate: yes\n        state: enabled\n    - name: open firewallport https\n      firewalld:\n        service: https\n        permanent: yes\n        immediate: yes\n        state: enabled\n    - name: open firewallports 10050 tot 10051 tcp\n      firewalld:\n        port: 10050-10051/tcp\n        permanent: yes\n        immediate: yes\n        state: enabled\n    - name: set SElinux to permissive\n      selinux:\n        policy: targeted\n        state: permissive\n    - name: enable zabbix-server\n      service:\n        name: zabbix-server\n        enabled: true\n        state: started\n    - name: enable zabbix-agent\n      service:\n        name: zabbix-agent\n        enabled: true\n        state: started\n    - name: restart httpd\n      service:\n        name: httpd\n        state: restarted\n\n\nMonitoring/install_zabbix on agents.yml\nHow does it work?\nFirst the script adds the Zabbix repository next it installs and configures Zabbix by pointing to the Zabbix server. Next firewall ports will be opened and the agent enabled and started.\nThe script\n---\n- name:\n  hosts: monitoredclient\n  gather_facts: true\n  become: true\n\n  tasks:\n    - name: add zabbix repo\n      dnf:\n        name: \"https://repo.zabbix.com/zabbix/5.0/rhel/8/x86_64/zabbix-release-5.0-1.el8.noarch.rpm\"\n        state: present\n        disable_gpg_check: true\n    - name: install zabbix\n      dnf:\n        expire-cache: true\n        name: zabbix-agent\n        state: latest\n    - name: configure zabbix-agent 1\n      lineinfile:\n        path: /etc/zabbix/zabbix_agentd.conf\n        regexp: 'Server='\n        line: Server=192.168.1.30\n    - name: configure zabbix-agent 2\n      lineinfile:\n        path: /etc/zabbix/zabbix_agentd.conf\n        regexp: 'ServerActive='\n        line: ServerActive=192.168.1.30\n    - name: open ports\n      firewalld:\n        port: 10050/tcp\n        permanent: yes\n        immediate: yes\n        state: enabled\n    - name: enable zabbix-agent\n      service:\n        name: zabbix-agent\n        enabled: true\n        state: started\n\n\nProxy/configurenginx.yml\nHow does it work?\nFirst it will set selinux to permissive and ensures nginx is installed and up to date. Next it will install git and clone a repository which contains an initial nginx.conf Proxy/nginx.conf next it will configure nginx with this configuration file. And it will open firewall ports and restart nginx.\nThe script\n---\n- name:\n  hosts: proxy\n  gather_facts: true\n  become: true\n\n  tasks:\n    - name: Put SELinux in permissive mode, logging actions that would be blocked.\n      selinux:\n        policy: targeted\n        state: permissive\n    - name: ensure nginx is at the latest version\n      dnf: name=nginx state=latest\n      become: yes\n    - name: start nginx\n      service:\n          name: nginx\n          state: started\n      become: yes   \n    - name: install git\n      dnf: name=git state=latest\n    - name: clone repo\n      git:\n        repo: 'https://awx:76C2A7yBGYNUFP5Ukn6A@git.fhict.nl/I440164/ansiblescripts.git'\n        dest: /home/repo/\n      register: git_finished\n    - name:\n      file:\n        path: /etc/nginx/nginx.conf\n        state: absent\n    - name: configure nginx\n      copy:\n        src: /home/repo/proxy/nginx.conf\n        dest: /etc/nginx/nginx.conf\n        remote_src: yes\n        owner: bin\n        group: wheel\n        mode: '0644'\n\n    - name: open firewallport http\n      become: yes\n      firewalld:\n        service: http\n        permanent: yes\n        immediate: yes\n        state: enabled\n    - name: open firewallport https\n      become: yes\n      firewalld:\n        service: https\n        permanent: yes\n        immediate: yes\n        state: enabled\n    - name: restart nginx\n      service:\n        name: nginx\n        state: restarted\n      become: yes\nProxy/letsencrypt_proxy.yml\nHow does it work?\nFirst the script ensures a successful nginx install. Next it will make sure http and https ports are opened and nginx restarted. After python3 is successfully installed and the epel repo enabled. Certbot will be installed and used to create and install a ssl certificate on the correct domain.\nThe script\n---\n- hosts: proxy\n  tasks:\n  - name: ensure nginx is at the latest version\n    dnf: name=nginx state=latest\n    become: yes\n  - name: start nginx\n    service:\n        name: nginx\n        state: started\n    become: yes\n  - name: open firewallport http\n    become: yes\n    firewalld:\n      service: http\n      permanent: yes\n      immediate: yes\n      state: enabled\n  - name: open firewallport https\n    become: yes\n    firewalld:\n      service: https\n      permanent: yes\n      immediate: yes\n      state: enabled\n  - name: restart nginx\n    service:\n      name: nginx\n      state: restarted\n    become: yes\n\n  - name: Install Python Package\n    become: yes\n    dnf: name=python3 state=latest\n\n  - name: enable epel repo\n    become: yes\n    dnf: name=epel-release state=latest\n  \n  - name: install certbot\n    become: yes\n    dnf:\n      name:\n        - certbot\n        - python3-certbot-nginx\n      state: latest\n\n  - name: Create and Install Cert Using nginx plugin\n    become: yes\n    command: \"certbot --nginx -n -m 440164@student.fontys.nl --agree-tos -d wifi-attendance.fhict.nl\"\n\n\nProxy/nginx.conf\n# For more information on configuration, see:\n#   * Official English Documentation: http://nginx.org/en/docs/\n#   * Official Russian Documentation: http://nginx.org/ru/docs/\n\nuser nginx;\nworker_processes auto;\nerror_log /var/log/nginx/error.log;\npid /run/nginx.pid;\n\n# Load dynamic modules. See /usr/share/doc/nginx/README.dynamic.\ninclude /usr/share/nginx/modules/*.conf;\n\nevents {\n    worker_connections 1024;\n}\n\nhttp {\n    log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n                      '$status $body_bytes_sent \"$http_referer\" '\n                      '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n\n    access_log  /var/log/nginx/access.log  main;\n\n    sendfile            on;\n    tcp_nopush          on;\n    tcp_nodelay         on;\n    keepalive_timeout   65;\n    types_hash_max_size 2048;\n\n    include             /etc/nginx/mime.types;\n    default_type        application/octet-stream;\n\n    # Load modular configuration files from the /etc/nginx/conf.d directory.\n    # See http://nginx.org/en/docs/ngx_core_module.html#include\n    # for more information.\n    include /etc/nginx/conf.d/*.conf;\n    \n    server {\n        listen       80 default_server;\n        listen       [::]:80 default_server;\n        server_name  wifi-attendance.fhict.nl;\n        proxy_redirect      off;\n        proxy_set_header    X-Real-IP $remote_addr;\n        proxy_set_header    X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header    Host $http_host;\n\n        # Load configuration files for the default server block.\n        include /etc/nginx/default.d/*.conf;\n\n        location / {\n        proxy_pass http://192.168.1.10:8080;\n        }\n\n        error_page 404 /404.html;\n            location = /40x.html {\n        }\n\n        error_page 500 502 503 504 /50x.html;\n            location = /50x.html {\n        }\n    }\n# Settings for a TLS enabled server.\n#\n#    server {\n#        listen       443 ssl http2 default_server;\n#        listen       [::]:443 ssl http2 default_server;\n#        server_name  _;\n#        root         /usr/share/nginx/html;\n#\n#        ssl_certificate \"/etc/pki/nginx/server.crt\";\n#        ssl_certificate_key \"/etc/pki/nginx/private/server.key\";\n#        ssl_session_cache shared:SSL:1m;\n#        ssl_session_timeout  10m;\n#        ssl_ciphers PROFILE=SYSTEM;\n#        ssl_prefer_server_ciphers on;\n#\n#        # Load configuration files for the default server block.\n#        include /etc/nginx/default.d/*.conf;\n#\n#        location / {\n#        }\n#\n#        error_page 404 /404.html;\n#            location = /40x.html {\n#        }\n#\n#        error_page 500 502 503 504 /50x.html;\n#            location = /50x.html {\n#        }\n}\n"
        }
      ]
    },
    "WifiAttendanceLawofprivacyEthicalconsiderations": {
      "hand-ins": [
        {
          "text": "Algemene verordening gegevensbescherming (AVG)\nMet wifi-tracking worden persoonsgegevens verzameld en daarmee valt dit onder de Wet bescherming persoonsgegevens (Wbp). De persoonsgegevens mogen pas verwerkt worden als er wettelijke grondslag voor is. Hierbij is te denken dat mensen zelf toestemming geven of als de gegevens nodig zijn om diensten te kunnen verlenen.\nArtikel 6 grondslagen\nIn artikel 6 van de AVG staan 6 grondslagen genoemd. Hiervan komen 3 grondslagen mogelijk in aanmerking voor wifitracking en bluetoothtracking door private partijen: toestemming, overeenkomst en gerechtvaardigd belang.\nDe 3 grondslagen die passen binnen wifi tracking zijn de volgende:\nde betrokkene heeft toestemming gegeven voor de verwerking van zijn persoonsgegevens voor een of meer specifieke doeleinden;\nde verwerking is noodzakelijk voor de uitvoering van een overeenkomst waarbij de betrokkene partij is, of om op verzoek van de betrokkene v\u00f3\u00f3r de sluiting van een overeenkomst maatregelen te nemen;\nde verwerking is noodzakelijk voor de behartiging van de gerechtvaardigde belangen van de verwerkingsverantwoordelijke of van een derde, behalve wanneer de belangen of de grondrechten en de fundamentele vrijheden van de betrokkene die tot bescherming van persoonsgegevens nopen, zwaarder wegen dan die belangen, met name wanneer de betrokkene een kind is.\nPersoonsgegevens wifi tracking\nBij wifitracking en bluetoothtracking worden doorgaans iemands MAC-adres (het unieke nummer van een telefoon of ander mobiel apparaat), de signaalsterkte van het geregistreerde wifi- of bluetoothsignaal, het serienummer en/of de locatie van de sensor en het tijdstip van de waarneming gebruikt.\nMAC-adres\nEen MAC-adres is een persoonsgegeven op het moment dat dit wordt gecombineerd met andere (persoons) gegevens die te herleiden zijn naar een persoon. Die herleiding is mogelijk via locatiegegevens van de telefoon die ontvangen wordt wanneer het apparaat verbonden is met wifi. In het geval van het project van de wifi-attendance is het MAC-adres gekoppeld aan een student/medewerker, dit omdat gebruikers worden gevraagd om op deze manier in te loggen op het netwerk.\nZelfs met MAC-spoofing is de gebruiker te herleiden aangezien hij/zij is ingelogd op het netwerk met zijn of haar gegevens. Zoals in ons geval het mailadres of PCN nummer.\n\nProject Wifi attendance\nVoor het project wifi attendance kan alleen gebruikt gemaakt worden van grondslag dat is op basis van toestemming. Dit komt doordat het in deze situatie niet is dat het noodzakelijk wordt geacht voor het onderwijs. Verder kan het ook niet op basis van gerechtvaardigde belangen aangezien het niet nodig is voor bescherming. \nDit project kan alleen op basis van toestemming, echter zitten aan de toestemming ook nog haken en ogen aan. Er mag alleen gebruik worden gemaakt van toestemming als het volgende in acht kan worden genomen:\nDe betrokkene die ervoor kiest om wel toestemming te geven die mag geen voordelen ontvangen omdat hij/zij toestemming geeft\nDe betrokkene die ervoor kiest om geen toestemming te geven mag geen nadelen ontvangen omdat hij/zij geen toestemming geeft. \nEr moet sprake zijn van een duidelijke actieve handeling. Bijvoorbeeld een schriftelijke of een mondelinge verklaring. Het moet in elk geval volstrekt helder zijn dat er toestemming is verleend. Er mag niet vanuit worden gegaan van het principe \u2018wie zwijgt, stemt toe\u2019. Het gebruik van voor-aangevinkte vakjes is niet toegestaan.\nDe betrokkene worden ge\u00efnformeerd voordat ze toestemming geven over het volgende:\nDe identiteit van de organisatie\nHet doel van elke verwerking waar toestemming gevraagd voor wordt\nWelke persoonsgegevens er verzameld worden en gebruikt worden\nHet recht dat betrokkene de toestemming in kunnen trekken\n\nDe gegevens die verzameld worden en verwerkt worden bij de grondslag van toestemming moeten per doel en per verwerking duidelijk worden beschreven. Daarnaast moet er ook beschreven worden waar de data eventueel bewaard wordt en voor hoelang.\n\n\nEthische overwegingen\nBij de ethische overwegingen wordt er in dit geval gekeken naar aanwezigheid. Dit komt doordat dit het grote onderdeel en doel is van het project. \nAanwezigheid\nIs het vanuit een ethisch oogpunt goed om te kijken hoe vaak en hoe lang studenten aanwezig zijn op school?\nHet is goed vanuit een oogpunt dat er gekeken kan worden hoe studenten presteren. Hier kan dan onderzoek worden gedaan of de aanwezigheid invloed heeft op de prestaties van de student.\nHet is niet goed als er gekeken wordt naar de aanwezigheid dat studenten worden afgekraakt als ze niet aanwezig zijn. Hierbij is te denken dat een docent heel boos wordt of neerwaardig gaat kijken naar de studenten.\n\nIs het vanuit een ethisch oogpunt goed om te kijken hoelang studenten pauze nemen/in de aula zitten?\nHet is goed om te kijken hoe vaak studenten pauzes nemen. Het pauze nemen kan namelijk soms helpen met het verbeteren van de resultaten.\nEchter zal dit niet altijd de oplossing zijn om te oordelen over hoe vaak studenten pauzes nemen. \n\nVerplichten van studenten om wifi aan te hebben staan?\nAls er tracking gebeurd op basis van wifi, worden studenten indirect verplicht om wifi aan te hebben. Dit gebeurd dan zonder dat zij dit wellicht willen, ergens is dit niet goed.\nStudenten die een oude Nokia hebben en geen wifi op de telefoon aan te kunnen zetten, zij kunnen zich buiten gesloten voelen. \n\n\nBronvermelding\n\nhttps://www.hrpraktijk.nl/topics/hr-analytics/nieuws/wifi-tracking-medewerkers-valt-onder-privacywetgeving#:~:text=Met%20wifi-tracking%20worden%20persoonsgegevens,AP)%20naar%20aanleiding%20van%20onderzoek.&text=Maar%20deze%20persoonsgegevens%20mogen%20volgens,een%20wettelijke%20grondslag%20voor%20is \n \n \n "
        }
      ]
    },
    "WifiAttendanceProjectproposal": {
      "hand-ins": [
        {
          "text": "\n\n\nVersion control\n\n\nProject Assignment\nProblem to solve\nThe current problem is that there so much students inside FHICT with a lot of knowledge. Currently, when students have questions they go to teachers instead of other students for specific questions. It would be nice to see what each student have for skills and that they kind find each other through Wi-Fi tracking. \nProject goals\nThe goal of this project consists of two parts:\nCreate a solution to enable the localisation of skills and experience in the building.\nHelp the student quantify their performance by having automated attendance as an extra metric in the canvas.\nResearch Questions & Sub questions\nHow can we localize people?\nHow can we integrate the skills of people when localizing?\nHow can we automate attendance?\n\nProject members\nThe project consists of the following members:\nElmira Drost (Project lead)\nDaan de Weirdt \nNoah Greff\n\nStakeholders\nInside the project we have different stakeholders. We have the following stakeholders:\nEric Slaats (The client)\nMaartje Hoop-Houben (GDPR Officer)\nRens van der Vorst (Fontys ICT services)\nTarget audience (users)\nThe people who are going to use this project:\nStudents of Fontys ICT\nLocation of the student itself are going to be displayed as well as a link to canvas for their knowledge\nTo see the location of other students and their knowledge\n\nMoSCoW\nMust have\nThe ability to localize people by tracking their Wi-Fi-devices.\nShould have\nVisualise location of people in a dashboard\nSee the basic data of people \nName\nMail\nStudent/ teacher\nList canvas courses per person.\nCould have\nIntegrate with the Ellie project\nGetting the data from canvas to see the skills & knowledge of each per person\nWon't have\nThe AR integration to see which knowledge students have\nProject Deliverables\nProject proposal                                                                                                                                \nLaw of privacy & Ethical considerations \nDashboard\nVisualisation of the location of people\nCanvas course information\n\n\nResearch Strategies\nLibrary\nAvailable product analysis\nTalking to the previous groups who were working on the project Wi-Fi-Attendance. Looking into the work that they made and see if we can use this work.\nBest good and bad practices\nLooking at experiences that other have with Wi-Fi tracking. Analysing their work, what worked and what not.\nCommunity research\nLooking into what the online community have done with Wi-Fi tracking and their problems. With the needed peer knowledge we can see if there is a good start for this project.\nExpert interview\nHaving an interview with the expert of Fontys ICT services. He is using Wi-Fi tracking for the measurements of people in the building due to COVID-19.\nLiterature study\nSearching for relevant information, using Google, using other people who work on this kind of project.\nField\nDocument analysis\nAnalysing the documentation that has been made by the previous groups for this project. Analysing the documentation of an intern who has worked inside the company on a similar project.\nDomain modelling\nMaking a network design for the relations between the servers.\nExplore user requirements\nHaving a conversation with Eric Slaats where the project is all about. Getting the requirements based on MoSCoW.\nInterview\nAn interview with Maartje, she handles the GDPR inside Fontys. With her we can look at the potential that our project have.\nProblem analysis\nGetting the real problem from the stakeholder. This way we can focus that the end product of  the project really is what the client wants.\nStakeholder analysis\nThe analysis with the different stakeholders concerning the GDPR. This project is really important for the GDPR because a lot of user data is being used.\n\nLab\nComponent test\nTesting the individual servers.  Testing of the application is working on the local machine and on the servers.\nComputer simulation\nSimulating the servers through virtual machines. Simulating the application on a local machine.\nHardware validation\nChecking if the current network devices supports the concept of Wi-Fi-tracking.  Besides that, looking if we can get access to the hardware.\nNon-functional test\nTesting the servers with other applications to check if the servers can handle the application. Testing the performance of the servers.\nSecurity test\nFinding and prioritizing the vulnerabilities inside the system and services. After that securing the vulnerabilities. \nSystem test\nStaging our application instead of going into production instantly. This way we can check if the system meet our requirements.\nUsability testing\nWhen it is working testing with a small group of users if they can use the software.\nShowroom\nEthical check\nChecking multiple times with Maartje from the GDPR if our decisions remains ethical on the privacy side of users.\nGuideline conformity analysis\nMaking the guidelines of privacy and security. This way the project can start and it can be done in the correct way.\nPeer review\nPeer reviews with the group members to check if all the work is clear and if it is done in a correct way.\nProduct review\nMaking sure that we get feedback from the stakeholders, prepare good demonstrations of our product. Improving with the feedback we get back.\n\nStatic program analysis\nUsing tools to check our program on vulnerabilities. After checking the application, look for solutions and apply them. \nWorkshop\nBrainstorm\nGenerating and developing new ideas for this project. With these ideas we can improve our project.\nCode review\nReviewing the code with many people can avoid problems. With many code reviews you can prevent bugs being implemented.\nDecomposition\nBreaking the IT system into smaller pieces for maintenance. Hereby we can update and work for instance on the webserver only instead of every server.\nPrototyping\nMaking a prototype so we can test and see if it is working, with the prototype we can communicate with the stakeholders, improve our product and get it to production.\nRequirements prioritization\nUsing the MoSCoW-method to get all the requirements prioritized.\nRoot cause analysis\nUnderstanding why a problem occurs, checking where it comes from and look at prevention. Document it for other students so they can fix and prevent this problem again.\n\u00a0\n\n\nActivities and Planning\nFor this project we use the approach of scrum. However inside this chapter we try to give a rough inside in the planning.\nDuring this project, several activities will be undertaken to ensure success. While more detailed planning will be available in this projects Microsoft Teams environment, a rough timeline will be sketched here. \n\n\nRisks\nIn the following section are listed probable underlying problems (and some probable solutions) that may arise throughout the project.\nGeneral\nInternal risk\nThe feasibility of the assignment:\nDue to too little knowledge there can\u2019t be any assurance that the project is 100% feasible. \nThe project have members  that are very busy with a lot of other projects, this needs to be taken into account with the division of tasks\nDefinitive deadline\nIf products or services are being delayed it can have effect on the definitive deadline.\nExternal risk\nCOVID-19\nDue to these uncertain times, there is a chance that physical meetings and team meetings can be delayed/cancelled. This would delay the progression of the project. \nUnclear boundaries:\nThere needs to be concrete agreements on the project. What is relevant and what not. We tried to give that insight with the MoSCoW-method\n\n\nDetailed risks\nWhat can go wrong in this project based  the CIA-classification:\nAvailability: \nThe machines need to update \nNo internet connection to and from the servers\nThe servers of Natlab crash \nThe servers of Natlab don\u2019t receive any power\nNatural disasters like earthquakes or fire happens at the Fontys Building\nIntegrity:\nMariaDB server is being hacked\nMariaDB receives wrong information\nWebserver displays wrong information\nThe canvas API gives the wrong information\nThe localization API from swagger receives the incorrect information\nConfidentiality:\nServers having the same logins\nPeople without the non-disclosure agreement see the data\n\n\nRisk matrix\nDown below there is a risk matrix based on the named points.\nLegenda: RED = Veel  ORANGE = Gemiddeld  GREEN = weinig\nThe risks matrix is based on the following:\n\n\nFor the highest risks there will be taken measurements into account:\nMariaDB server is being hacked\nMariaDB receives wrong information\nWebserver is hacked\nWebserver displays wrong information\nPeople without the non-disclosure agreement see the data\n\nMariaDB server is being hacked:\nThe data for the users who are being tracked is in the database server. If the server is being hacked the information could go on the streets.\nSolution: Monitor a lot, close all the unused ports, use anti-virus software, only use the database in the intern network with the seclab VPN and use strong passwords.\nMariaDB receives wrong information\nWhen the database receives the wrong information the webserver will receive the wrong info which cause that people can\u2019t be localized.\nSolution: Monitor the input of the database, setting parameters in the database that only the correct form of data can be saved.\nWebserver displays wrong information\nThe data where people are is being showed on the webserver. If the webserver displays the wrong information people can\u2019t be localized anymore. Therefore it could also display the wrong information about the knowledge of people.\nSolutions: Monitor the webserver 24/7, check if the connection between the database server and the webserver is ok. \nPeople without the non-disclosure agreement see the data\nWhen working with a lot of data there is a little change that people can see the data on your screen. Which cause that some who don\u2019t have a non-disclosure agreement see the data.\nSolution: Working on this project in a little room alone or with people who have a non-disclosure agreement."
        }
      ]
    }
  },
  "17661": {
    "AdviceNLPsolutions": {
      "hand-ins": [
        null
      ]
    },
    "BeehiveBeeresearch": {
      "hand-ins": [
        {
          "text": "Requirements\nAuthor: Sabrina Hooijmans\nDate: 26/10/20\n\n(M)\tTrack Health of the beehive\n(M) \tRecognise/track where the bees are coming from\n(S)\tNice representation on the window\n\nResearch Questions\nWhat are the key indicators of a healthy beehive?\nHow can we recognise/track where the bees are coming from?\nHow can we get the measured data to a remote location?\nHow can we make the beehive look representable on the window without effecting the health of the beehive?\n\n\n\nIntroduction\n\n\nVersion control\n\n\nResearch method\nThe research methods that I applied for this document are community research and literature studies. In order to get started with the key bee indicators, I went on several forums to garner information from beekeepers. The most frequent forum posts I visited were about how to keep bees alive, and how to see whether or not your bees are healthy. A list with the community forums I went on can be found at the bottom of this page. \n\nFor literature studies, I searched on ResearchGate and google scholar to find applicable research that had been done before. Sadly enough, a lot of the documents I found were not concretely helpful, but they did give me a better foundational understanding of the entire situation. The documents talked about the theory behind the bee dance and how bees kept their population standing. I was hoping to find more \u2018concrete\u2019 applications on how tracking of bees has been done, sadly enough most of the concrete examples were unavailable to us or with cutting-edge technology. \n\nWhat are the key indicators of a healthy bee hive?\nTraffic / population\nThe most obvious indicator to use on determining whether or not a beehive is healthy is the population. Strong hives have an abundance in worker bees, which can show itself in forms of traffic at the entrance of the hive. This traffic isn\u2019t an infallible indicator, as there are many outside factors which can reduce the traffic and make it seem less populated than it actually is. These outside factors can be: time of day, weather, wind speed and warmth. Because of all of these factors, getting a good overview of the actual traffic should be done by averaging the traffic, as single measurements will result in a distorted view on the situation.  \nAnother \u2013 but more complex way \u2013 of checking the hive is physically opening it up and checking. By incorporating this technique we remove the beforementioned outside factors, and are no longer dependant on the fluctuating traffic.  Strong beehives will, when opened up, have bees hanging from the top of the frames, combined with activity on the frame itself. Besides checking the population, this is also a good way to physically check the brood pattern, which is another factor to take into account. The brood pattern should be covered with nurse bees, which are using their bodies as a blanket, covering the brood pattern and retaining the warmth. Next to those nurse bees should also be bees stationed in the honey supers, which are working on protecting the honey from moths and beetles. This area is less populated than other locations in the hive, but the more you see, the healthier the colony.  \nBrood pattern\nA healthy colony has a healthy brood pattern. A brood pattern is a straightforward indicator since there is an objective \u2018good\u2019 and \u2018bad\u2019 brood pattern. When the queen starts laying her eggs, she will put the brood in a specific pattern. This pattern is usually close together in a group, in such a shape that it\u00b4s called a brood pattern. It is the easiest to see when the brood are capped with wax, but it can be viewed earlier. \nThe brood should look like a solid concrete \u00b4block\u00b4 of brood. There should be areas of eggs, larvae and the capped brood, grouped together to retain heat. There are a handful of reasons to this pattern being neglected, most of which are prominent indicators of disease. One of those diseases would be the \u2018Spotty\u2019 brood pattern, which is, as the term indicates, full of spots without brood (figure 1) . This is almost the opposite of what a healthy brood pattern (figure 2) should look like. By knowing what a healthy brood pattern looks like, we can easily test the beehive against a set of healthy beehive pictures and check for similarities. \n\n\n\n\n\n\n\nFigure 1: 'Spotty' brood pattern\t\t\tFigure 2: Healthy brood pattern\n\n\nInsects in the brood\nBesides the patterns, there is more to the brood. Insects are constantly looking to annex part of the beehive brood area for themselves. This doesn\u00b4t necessarily pose a concrete threat, as long as the hive has enough guards to support the brood, all is well. However, there is a chance that the amount of brood becomes too large to protect by the guards. There are a heap of insects waiting for this to happen, so that they can move in and grow their population in the under-guarded brood. \nOnce brood starts to get out of the comb and leave their unused comb behind, the insects start using it as free real estate. This can pose a serious threat to the colony, if the guards cannot fend off these insects there is a chance they will be overrun. As long as the guards are able to fend off the insects there is nothing to worry about, this means that there is always a possibility to find small numbers of beetles and moths. Their presence alone does therefore not define a weak hive.  \n\nPollen\nPollen are a major part of a bees\u2019 diet, which creates a strong correlation between a healthy hive and the abundance of pollen stored in the hive. The collected pollen are taken by the bees throughout the hive to the pollen stores, which are commonly found near the entrance of the hive. A good way of checking whether or not the bees have enough food intake would be to scan the entrance for bees with pollen on them (figure 3) and count how many of these we see a day. Based on this we will be able to create an average trend of the amount of food consumed by the hive per day/week. \n\nFigure 3: Bee with sidebags storing pollen \nHeat of the hive\nIf a beehive gets too hot, the bee brood dies and the honey gets dehydrated too quickly. If a beehive gets too cold, broods die off and the nectar cannot dehydrate fast enough to make honey. 35\u00b0C is the exact right temperature for a hive.\nWarmth\nA key outside factor which has to be taken into account, is the warmth of the beehive. According to studies, beehives need to be kept at a consistent 32-35\u00b0C in order to produce honey. If the temperature rises above 35\u00b0C, not only will the honey production stop, but the larvae will die from hyperthermia. In order to prevent causalities, the bees will strategically position themselves throughout the beehive and start flapping their wings, which will act as a \u2018natural air conditioning\u2019. Besides being lethal for brood, a warm hive also dries out the honey, which would result in no food for the young brood. \n\nCold\nBesides the danger of hyperthermia, there is also the lethal risk of hypothermia. If the brood temperature gets below 32\u00b0C, there is a chance they will decease. Luckily bees are skilled insulators, and can manage to keep the brood at a constant 35\u00b0C, even at temperatures of absolute 0 (-40F, -40\u00b0C). At this temperature, the bees will cluster together and use their bodies as insulating blankets to keep the brood warm. The bees on the inside of the cluster will move toward the outside of the cluster and vice versa, creating a constant flow of warm/cold bees. \n\nHow can we check the key indicators of a healthy beehive?\n\nHow can we recognise/track where the bees are coming from?\nBees communicate in two ways: releasing pheromones and dancing. In order to analyse where the bees are coming from, we will be focussing on the latter. \n\nBee dance detection algorithm\nThe waggle dance\nWhen communicating resource stores in the vicinity to its peers, bees incorporate a specific dance to transmit the location. The bees have a very distinct dance which is executed each time one tries to communicate the location of a resource store.  This dance has a few simple parameters which need to be taken into account in order to comprehend the dance. The dance goes in two parts:\n\nThe bee \u2018waggles\u2019 in a straight line for x amount of seconds ,every second the bee moves forward indicates one extra km. This line is angled against the azimuth, and indicates how many degrees the target is, relative to the hive. \nAfter it has indicated the distance, the bee makes a turn and returns back to the original starting position, this is repeated for a supposedly amount of iterations\n\nIdea 1: bee location plotting\n\nOne idea to track the bees is by plotting their locations on a canvas once every 0.1 seconds (10fps) (figure 4). Every frame we also check whether or not any of the bees are plotting something which looks similar to a waggle dance, and compare it to an actual waggle dance (figure 5). If the comparison validates that this is indeed a waggle dance, we can analyse the dance by looking at the previously plotted points and see what location the bee is trying to communicate.  \n\nFigure 6: Bee dances for different locations in the field\n\nT.B.D.\n\nHow can we get the measured data to a remote location?\nThere are many ways to transfer data to a remote location. The main four types that are used in IoT are Cellular, LAN/PAN, LPWAN and mesh networks \nCriteria :\nRange to the gateway (mid range)\npower consumption (low)\nSecurity (encryption/authentication)\nData rate\nQuality of service and reliability\nSimplex or duplex\nSuitable and available spectrum (Licensed or unlicensed)\nHow can we make the beehive look representable on the window?\n\n\n\nSources:\nBeeculture. (23-04-2018). A closer look \u2013 waggle dances. \nWikipedia. (n.d.). Waggle Dance. \nCyborgAntorphology (22-39-12). Bee dance. \nThe Management Agency National American Foulbrood Pest Plan. (n.d.). \u201cSpotty\u201d brood pattern. \nAllison\u2019s Apiaries. (13-03-2018). Can a beehive get too hot or too cold?. \nBackYardHive. (09-10-2017). Honey Comb Identification \u2013 Brood nest \nHoneyBeeSuite. (n.d.). What is a brood pattern?. \n\n\nScienceDirect. (02-06-2014). Dancing bees communicate a foraging preference for rural lands. "
        }
      ]
    },
    "CompetenceDocument": {
      "hand-ins": [
        {
          "text": "Version control\n\n\nIntroduction\nNiek van Dam\n433010\n\nMy name is Niek van Dam, I am a 19-year-old student from the FHICT Delta course studying the software trajectory.\nBefore coming to Fontys, I studied software engineering on a vocational level for 3 years, where we created responsive web apps. During this study, my teachers let me take a deeper dive into algorithms and design patterns, after which I got interested in software engineering.\nRight now, I am still very interested in algorithms and started working with neural networks, which is my main goal to master right now, combined with mathematics.\u00a0 Besides programming, I am also interested in trading on the stock market and crypto markets. I have been trading on the crypto market for about two years now, and the stock market for a few months. My goal is to create a trading algorithm - if possible, with neural networking - that will automate most of the trades for me.\n\u00a0\nWithin Delta, I am working on the following projects:\nDataWall\nASAM\nOpenRemote\nIntelligent Beehive\nDeX\n\n\n\nProjects\nDataWall\nDatawall is a project that aims to identify and visualize the identify of all FHICT students and employees and improve the social cohesion between all the different study routes. The project will be an \u2018interactive art\u2019 setup, in which every student will be visualized by a \u2018node\u2019. The more nodes, the clearer the pictures become and the more complex pictures can be created. \nWithin DataWall, I am working on the API and Unity back-end, which has been a struggle so far. I am planning on getting some good progress in the coming few weeks, if everything goes according to plan. \n\nASAM\nAsam is a relatively new project, in which we have not done much work yet. The project is about upgrading the internship system for Fontys, by adding new innovative features. \n\nOpenRemote power prediction\nThe OpenRemote project initially began as the \u2018smart streetlights on Strijp-s\u2019 project. After the first meeting it became clear that this project was already taken by another group, which meant that we were free to take another project. In that moment, our team decided to take \u2018wind power prediction\u2019, which is a project where we must create a RNN neural network which will predict wind power soon. The biggest challenge in this project is finding a dataset, needed to create a proof of concept. After that we must start creating a network based on real life data, which is also a challenge. \nWithin this project I tried to take the \u2018lead\u2019 and get the others to work with me. After a few weeks, we also decided to implement scrum into the project. We are working in sprints of two weeks and have a delivery at the end of each one of them. Within this project I have been working on the research document and the proof of concepts for predicting wind power.  \n\nIntelligent Beehive\nIntelligent Beehive is a delta project in which the goal is to create a \u2018smart\u2019 beehive, which is being monitored by an array of sensors. In this project we are building upon an already existing piece of software, which already has some realized features like the BroodPatternDetection. This piece of software analyses the breeding patterns of the honeybees and checks whether those patterns are considered \u2018healthy\u2019. \nThe first concrete piece of software which I want to realize is the tracking of the honeybees\u2019 communicative ways, of which the goal is to map where all the bees have been. \n\n\nDeX\nDeX (Digital Excellence) is a big archive of all projects and ideas throughout Fontys stored in one place. The goal of this project is to increase cooperation and popularity of projects since most remain unseen currently. \nBy the time I joined DeX, it was already a running project with a functioning front- and back-end codebase. Because it was already set up, I was tasked with making the website more \u2018discoverable\u2019 by improving the SEO (Search Engine Optimization). Besides, SEO I am also working as full-stack developer, meaning that I can work on both codebases if I see fit. This gave me the opportunity to also implement other miscellaneous tasks, like CI/CD and pipelining. \n\n\nKPI-table with proof\nProject abbreviations:\nOR: OpenRemote Wind Power Prediction\nIBH: Intelligent Beehive\nASAM: ASAM\nDeX: Digital Excellence\nDW: Data Wall\n\n\nKPI Table\n\n\n\nKPI-Matrix\nEvaluation and Reflection\nEvaluation mid-term\nIn the semester so far, there have been a lot of interesting things happening. In the beginning we started out with little to no structure in the projects, which was mostly due to the fact that almost none of my projects had a dedicated leader. After realising that the projects were not going anywhere, I decided to ask Wilrik for advice on how to structure the projects better. His advice was: implement a type of project management like Scrum and get a project lead. After this meeting I decided to incorporate this into the projects which were progressing a bit \u2018rocky\u2019. A few days after implementing this, I started noticing more structure within the project, goal achieved. \nBy now, most of my projects have improved greatly in comparison to the beginning, which makes working within delta a lot more comfortable and less stress-invoking. \n\nFinal evaluation\nThis semester has turned out to be a lot differently than I initially thought, this is mostly due to the covid situation and the fact that we are unable to physically work together. In the beginning of the semester I was sceptical about Delta in general, as most projects were not working smoothly and I felt like there was little productivity within the entire course. When structure finally appeared in most of the projects I was able to get a lot more joy out of working within delta, as I felt like I was finally doing something productive.\nWorking at a lot of projects at the same time definitely was not the best approach to this semester, as it made things a bit more stressful than they should have been for me. This wasn\u2019t the case at the start of the semester, as in the beginning I was only working on DeX. At the time, I did a lot of research for SEO and trying to implement more SEO features into the project. As soon as the OpenRemote project started rolling, my attention shifted to that project and I had no time left to still work as hard on DeX as I did in the beginning. After all of my other projects starting up as well, it started to become a daily juggle between my projects and how to spend most of my time. \nThe projects DeX and OpenRemote are definitely the ones that I am the most proud of working on, I was really invested in these projects. Sadly enough we were not able to finish the OpenRemote project in one semester, as that was my main goal. I am still really proud of our accomplishments within this project, as we will still get a turbine up and running in the near future, accompanied by the great load of AI skills that I learnt. DeX was all around a good project to be in and I felt that the work I was doing there actually helped the website. This was also something that could concretely be checked with SEO tools, further proving to myself that my actions were actually doing something. \n\n\nReflection  mid-term\nDuring the semester, I have grown mostly in terms of personal leadership and researching. These are two things that I have not done in such a dedicated manner as I have in this semester. In the beginning the projects were not really progressing as much as I would have hoped, so I picked up the initiative to try and \u2018lead\u2019  the projects and get structure going within the team. After a painstaking amount of awkward meetings with no clue what to discuss next, I finally got the hang of it. Now I\u2019m able to more comfortably take the lead in a meeting when I feel that it is going nowhere, which is a huge improvement in comparison to last year.\nBesides leading a team, I have also done a good amount of research on the topics I was about to work on, which helped more than imaginable. I have the feeling that the writing of a research document gives you a better understanding of what you have just researched. I think this because you can immediately try and apply your new knowledge by creating examples for your readers which makes you think about the topics more as well. By writing research documents for the more complicated topics I was about to approach, I felt like I had better preparation for what I was about to get myself into. \n\nFinal reflection\nIn the beginning of the semester I was mostly working on getting the projects to run smoothly and increasing my PO skills in the project. This changed in the second half, when the projects finally started to run smoothly. \nWithin OpenRemote, I was usually presenting the deliveries with Elmira, to the point where I felt like I was gradually improving my way of presenting to OpenRemote. This has been a goal of mine for a while, as I would like to improve my presentation skills. At OpenRemote I have also gained a lot more opportunities to have contact with the stakeholders, which also helped me in figuring out how to properly do so in the future. \nWe also had to incorporate a lot of research in order to buy to buy/install a wind turbine for OpenRemote. Due to concretely researching what the company needed, I gained a lot of research skills, mostly because of the broad array of research questions we had to solve: how to install a wind turbine/how safe is it/is it realistic/ etc. This really made us think about all possible edge cases when installing such a thing. I feel like I still have a lot to gain on this subject, as this didn\u2019t go as smoothly as expected and gave me some stress now and then.  \nThere were times where the teamwork within OpenRemote did not go as expected, to the point where it was difficult to work with my teammates in the project. At first I discussed this with Elmira, who had noticed the same thing. Afterwards we decided to bring it up in a meeting, in which everyone agreed that teamwork wasn\u2019t going well. After this meeting, however, nothing concretely changed. After this going on for another while we decided it was time to undertake action, which is when we kicked Daniel out of the project.\nIn general this semester lead me to learn a lot about presenting to the customers, communicating with them, trying to manage a team and researching \u2018abstract\u2019 questions. On the other hand, it showed me that I need to improve my scheduling skills, as this gave me a lot of stress.  \n\n\nAttachments:\nAttachment  1:\n\n\nAttachment 2:\n\n\nAttachment 3:\n\n\nAttachment 4:\n\nAttachment 5:\n"
        }
      ]
    },
    "DataWallAnalyseexistingcode": {
      "hand-ins": [
        {
          "text": "\n\n\n\n\n\nTo-Do\n\nGetters and Setters of objects\nFor some reason, the objects which are being referenced in the BackendSpring project do not contain getters and setters. This causes the IDE to fall over and report up to 100+ problems. This is uncalled for and should be able to be fixed by simply creating getters and setters. \nCreate postman tests for the API\nTo test the API, we can use postman, this is a program which executes simple web requests to a specified address. These web requests can be filled up with a json body in order to add a request body. Because of this we are able to use this for testing the API, and checking whether or not everything returns in an appropriate manner. \n\nA small list of things that we can test with Postman are listed below, including further explanations:\nCreate users / roles (possibly not necessary)\nThis could possibly not be necessary, as we are going to use the FHICT API to be logging in. The assumption is that using the FHICT API to log in would automatically create a user/role\nAuthenticate users\nAuthenticating users goes with the FHICT API as said before, this procedure however does need to be researched. \nManage modules\nAdd\nUpdate\nDelete\nCreate visualizations from modules\nEach visualisation consists of a set of modules, we need to find out how these are linked and passed to the API and Unity project\nGet OAuth working\n\nUpdate recipes to match unity\u2019s recipe model\nAccording to last meeting with the original developers, Unity had a different object for recipes in comparison to the API. This needs to be updated from the API side, since the Unity side shouldn\u2019t be tweaked with if possible.  \n\n\n\n\n\n\n"
        }
      ]
    },
    "DeXCICDLighthousecheck": {
      "hand-ins": [
      ]
    },
    "DeXSEOIntroduction": {
      "hand-ins": [
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\nDeX\u00a0\nSEO\u00a0 \u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n05.10.2020\u00a0\n\nVersion 1.0\u00a0\n\n\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\nVersion History\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0 \u00a0\n\n1\u00a0\n\nVersion\u00a0 Date\u00a0 Author\u00a0 Changes\u00a0\n\nV0.1\u00a0 05-10-2020\u00a0 Niek van Dam\u00a0 Initial document structure\u00a0\u00a0\n\nV0.2\u00a0 06-10-2020\u00a0 Niek van Dam\u00a0 Addition of future SEO plans\u00a0\n\nV0.3\u00a0 09-10-2020\u00a0 Niek van Dam\u00a0 Refactor of the document\u00a0\n\nV1.0\u00a0 12-10-2020\u00a0 Niek van Dam\u00a0 Added examples and improved\u00a0\ndocument structure\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\nTable of Content\u00a0\n\n\u00a0\n\nVersion History 1\u00a0\n\nTable of Content 2\u00a0\n\n1. Introduction 3\u00a0\n\n2. SEO Basics 4\u00a0\n\n3. What DeX has done to improve SEO 5\u00a0\n\nTag updates 5\u00a0\n\nFiles to increase SEO 6\u00a0\n\nRobots.txt 6\u00a0\n\nSitemap.xml 8\u00a0\n\nUser-friendly pages 10\u00a0\n\n4. Future SEO plans 11\u00a0\n\nEnhanced google search results 11\u00a0\n\nEnhance URLs 11\u00a0\n\nEnhance tags and websites 12\u00a0\n\nT.B.D 12\u00a0\n\n5. Testing 12\u00a0\n\n\u00a0 \u00a0\n\n2\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\n1. Introduction\u00a0\n\nA lot of employees and students of FHICT create a lot of nice products. As a hobby, for their\u00a0\n\nwork, or because of a school project. These can be software solutions, small scripts, research\u00a0\n\npapers, thesis but possibly also project ideas and proposals for others to pick up. Unfortunately,\u00a0\n\nmost of them remain private and are not known by the majority of the population. The goal of the\u00a0\n\nDeX-platform (short: DeX) is to make all this work more findable and thus improving collaboration.\u00a0\n\n\u00a0\n\nTo get people on the platform, we need to edit the website in such a way that the platform\u00a0\n\nbecomes more discoverable on SERPs (Search Engine Result Pages). A few common examples of\u00a0\n\nSERPs are Google, Bing, and Yahoo. These SERPs get their results from the respective web\u00a0\n\ncrawlers, which are constantly indexing new webpages and ranking these for specific keywords.\u00a0\n\nThe goal with SEO is to reach the highest possible index on these SERPS. This can be reached\u00a0\n\nby modifying your pages, adding metadata, and guiding the crawlers.\u00a0\u00a0\n\n\u00a0\n\nIn this document, we will go more into depth about how we approached SEO in DeX.\u00a0 \u00a0\n\n3\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\n2. SEO Basics\u00a0\n\nDeX is a project that has been launched pretty recently at the time of writing. Because of its\u00a0\n\nnovelty, it does not have a lot of organic visitors yet, which means that we have to garner them\u00a0\n\nourselves. To get these visitors, we need to update the site in such a way that google will relate it\u00a0\n\nto specific search \u2018keywords\u2019.  For example, DeX is a site mainly focussed on\u00a0\n\nsharing/collaborating on ideas and projects, this means that when someone is looking up:\u00a0\n\n\u201csoftware collaboration\u201d, we would want our site popping up in the SERPs. To achieve this, we\u00a0\n\nhave to organize our site in such a way that the crawlers will link our website to keywords as\u00a0\n\n\u2018software\u2019, \u2018collaboration\u2019, \u2018sharing\u2019 etc.\u00a0\u00a0\n\nBy getting our site linked to a few keywords we\u00a0\n\nare far from done, there are a lot of optimizations\u00a0\n\nto be made to get a higher ranking on the SERPs\u00a0\n\n(which is something to always strive for). Although\u00a0\n\nthat might seem obvious, I still want to stress how\u00a0\n\nbig of a role the SERP index plays. The SERP\u00a0\n\nindex is based on how well-designed and \u2018crawler\u00a0\n\nfriendly\u2019 your page is. The more information (good\u00a0\n\nmetadata, updated tags, informative alt tags, etc.)\u00a0\n\na crawler can get from your website, the more\u00a0\n\nlikely you are to get a higher ranking.  Besides\u00a0\n\nthat, the ranking is also based on monthly traffic\u00a0\n\nand engagement. The image below shows the\u00a0\n\nCTR (Click Through Ratio) based on the ranking in\u00a0\n\nthe SERP. This ratio is used in expressing how\u00a0\n\nmany people click the link to the website when\u00a0\n\nthey are posed with it. For example, 43.2% of all\u00a0\n\npeople seeing the red result would have clicked\u00a0\n\nthis. From this picture, we can see that a sub-top 5\u00a0\n\nalready reduces your potential traffic down to 15% of all people looking for \u2018your tag\u2019.\u00a0\u00a0\n\n\u00a0\n\nIn conclusion, to grow in organic clicks we have to create a website that has good and relatable\u00a0\n\ntags so that we appear in the SERPs, besides which we also have to create such an SEO-friendly\u00a0\n\nsite that we get ranked in the top 5 results.\u00a0\n\n4\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\n3. What DeX has done to improve SEO\u00a0\u00a0\n\nThe issues that DeX has worked on in terms of SEO can be found \u200bhere\u00a0\u00a0\n\nTag updates\u00a0\n\nThe first thing we did to improve the SEO is updating \u2018tags\u2019. This is described vaguely on purpose\u00a0\n\nsince there is a broad array of different kinds of tags to be updated.\u00a0\u00a0\n\nWe started by updating the meta description and title tags since these are easily modified and\u00a0\n\nhave a decent impact on the SEO already. The meta description (figure 1) is usually shown by\u00a0\n\nSERPs underneath the search results, which plays a big role in convincing users to click on your\u00a0\n\nwebsite. The best practice is to give each page a good and unique meta description, which will\u00a0\n\nrank you higher in the SEO index.\u00a0\n\n\u00a0\n\n\u00a0\n\nBesides updating the meta description, it is also important to add some \u2018quality of life\u2019 tag\u00a0\n\nupdates to the website. For example the lazy loading of images, alt image tags for text-only\u00a0\n\nbrowsers, and a dynamically updating title tag. All of these small additions will end up boosting\u00a0\n\nyour SEO indexing by a significant amount if applied properly.\u00a0\n\n\u00a0 \u00a0\n\n5\u00a0\n\nhttps://github.com/DigitalExcellence/dex-frontend/issues?q=is%3Aissue+label%3ASEO\n\n\n\u00a0\n\n\u00a0\n\nFiles to increase SEO\u00a0\n\nBesides the updating of web content, there is also the possibility of increasing the SEO by adding\u00a0\n\nSEO-specific files to the root folder of the website. These files are meant for guiding the\u00a0\n\nrespective web crawlers to the right files (or restricts them from entering certain directories)\u00a0\n\nRobots.txt\u00a0\n\nThe robots.txt is an important text file that instructs web crawlers to crawl certain pages on your\u00a0\n\nsite. This file gives site-wide instructions on how to handle links to the web crawler. By guiding\u00a0\n\nthe web crawlers through your website you gain more control over the pages which are being\u00a0\n\nindexed. For example, denying the web crawlers from accessing the staging site, which could be\u00a0\n\nfull of SEO-unfriendly pages.\u00a0\u00a0\n\nThe markup of a robots.txt file is for the most part quite simple and direct, making the document\u00a0\n\nclean and easily understandable. The only part that can get quite complicated is the markup for\u00a0\n\nURLs, but we\u2019ll get into that later.\u00a0\u00a0\n\nFor now, the basic syntax is as follows:\u00a0\n\n\u00a0\n\nIn the robot.txt above we can see most of the syntax, I\u2019ll describe it once again below:\u00a0\n\n- User-agent\u00a0\n\n- The user agent is the entity that is currently crawling the webpage, this could be\u00a0\n\nDiscoBot, GoogleBot, BingBot, etc. You can either specify one of those or use a\u00a0\n\nwildcard (*) to target all of them\u00a0\n\n- Below the User-agent tag, the rules for that agent are listed\u00a0\n\n- A new user agent is defined by a line break underneath the rule list\u00a0\n\n- Disallow\u00a0\n\n- This is a rule for a specific user-agent, which is why it is listed underneath the\u00a0\n\nuser-agent. This rule tells the crawler where to go, or where to stay away from.\u00a0\n\n6\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n- Allow\u00a0\n\n- This is the opposite of disallowing and is mostly used for specific user agents that\u00a0\n\nare allowed to crawl sites that others are not allowed to\u00a0\n\n- Sitemap\u00a0\n\n- This is a generic tag, used to tell the crawlers where the sitemap is. The usage\u00a0\n\nand importance of a sitemap will be covered later.\u00a0\u00a0 \u00a0\n\n7\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\nSitemap.xml\u00a0\n\nThe second file is the sitemap.xml, which is also used to guide the crawlers, but differently. This\u00a0\n\nfile is a blueprint containing each site you have on your website. This is especially useful when\u00a0\n\nthere is a lot of content that may not be well-linked to each other. In this case, the crawler can\u00a0\n\nrequest the sitemap and check which web pages still need to be indexed.\u00a0\n\nEvery sitemap has a few basic required tags. Those are as follows:\u00a0\n\n- A sitemap must begin with an opening <urlset> tag and end with a </urlset>\u00a0\n\n- Include an <url> entry for every child\u00a0\n\n- You must include a <loc> child tag for each <url> parent tag\u00a0\n\nBelow you can see these rules applied in a basic sitemap document.\u00a0\n\n \n\nOptionally, there are two more tags to add, used for manipulating the frequency of your page\u00a0\n\nbeing crawled. These are optional, if none are provided the crawler will implement these values\u00a0\n\nitself. The two tags are as follows:\u00a0\n\n- lastmod\u00a0\n\n- On every update of a webpage, you can update the lastmod. This will let the\u00a0\n\ncrawlers know whether or not the page content is outdated or not. The lastmod\u00a0\n\ntag can go hand in hand with the changefreq tag since both are related to page\u00a0\n\nupdates.\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n8\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\n- Changefreq\u00a0\n\n- The changefreq is a tag used to indicate how many times a period the page is\u00a0\n\n\u2018updated\u2019. Crawlers have the option to memorize this changefreq and will come\u00a0\n\nback once the website has changed based on the changefreq. This is not\u00a0\n\nnecessarily always the case, since a page marked \u200bhourly\u200b could be crawled daily.\u00a0\nThis decision cannot be influenced and comes from the crawler itself\u00a0\n\n- The list of change freqs is as follows:\u00a0\u00a0\n\n- Always\u00a0\n\n- Hourly\u00a0\n\n- Daily\u00a0\n\n- Weekly\u00a0\n\n- Monthly\u00a0\n\n- Yearly\u00a0\n\n- Never\u00a0\n\n- This value should be used for archived URLs, which will no longer\u00a0\n\nbe updated\u00a0\n\n\u00a0 \u00a0\n\n9\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\nUser-friendly pages\u00a0\n\nAnother small issue that has been worked on is the creation of user-friendly page URLs. This\u00a0\n\nmeans that all of the web page URLs need to have human-readable text in them instead of, for\u00a0\n\nexample, ids. We did this by replacing the ID\u2019s with the project name in the router and in certain\u00a0\n\nHREFs, which made it \u2018human-readable\u2019.\u00a0\n\nThis updating of links is not only useful for crawlers but can also be used to improve CTR from\u00a0\n\norganic users. This is because a semantically accurate URL will look a lot more \u2018trustworthy\u2019, in\u00a0\n\ncomparison to a poorly structured URL with almost \u2018random\u2019 numbers in it.\u00a0\n\n\u00a0\n\n\u00a0 \u00a0\n\n10\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n4. Future SEO plans\u00a0\n\nEnhanced google search results\u00a0\n\nWith enhanced google search results we can make the website more appealing to the public,\u00a0\n\nwhich would therefore increase the CTR. There has not yet been a fully-fledged brainstorm\u00a0\n\nsession about this idea, but some inspiration has been drawn from the google carousel.\u00a0\n\n\u00a0\n\nEnhance URLs\u00a0\n\nThere are still plenty of URL enhancements to be\u00a0\n\nmade to increase SEO even more. We can add\u00a0\n\nmore keywords to the URL (but not too many!).\u00a0\n\nAnother way to enhance the URLs is by shortening\u00a0\n\nthem. As shown in the diagram to the left, there is a\u00a0\n\ncorrelation between the average URL length and\u00a0\n\nthe google position.\u00a0\u00a0\n\n\u00a0\n\n11\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\nEnhance tags and websites\u00a0\n\nThere are a handful of additional tags to update and implement to increase SEO. These tags or\u00a0\n\nwebsite updates don\u2019t have a huge impact itself, but 10 of these small updates already have the\u00a0\n\npotential to boost your SERP ranking. I listed the most prominent 5 below:\u00a0\n\n1. Use your keyword in H1, H2, or H3 tags\u00a0\n\n2. Use your keyword once in the first 150 words\u00a0\n\n3. Update your page content with LSI keywords\u00a0\n\n- LSI keywords are a special set of keywords used by Google to determine the\u00a0\n\noverall topic of your webpage. A detailed description of LSI can \u200bbe found here\u00a0\n4. Use trusted external links (5-8 are recommended). This way google sees that the content\u00a0\n\nis well-referenced and trustworthy.\u00a0\u00a0\n\n5. Use internal links with keyword-rich anchor text in these links. This means that you should\u00a0\n\nuse keywords of the referenced page in the \u200banchor text\u200b.\u00a0\n\n\u00a0\n\nT.B.D\u00a0\n\nThere are still loads of things to do to increase SEO. The checklist that I am currently using is the\u00a0\n\nblacklinko SEO checklist\u200b. Blacklinko is a good resource for more SEO inspiration, so I recommend\u00a0\nchecking this out when all other things are finished.\u00a0\u00a0\n\n5. Testing\u00a0\n\nTesting is done by using the Google Search Console. In this tool, we can easily get an overview\u00a0\n\nof all data that Google has gathered about the website.\u00a0\u00a0\n\nThe console has a broad array of SEO tools, which can help you a lot in analyzing whether or not\u00a0\n\nyour website needs to be upgraded. There has been a discussion about applying Google\u00a0\n\nAnalytics, but this idea has been rejected due to the privacy policy that would have to be\u00a0\n\nupdated.\u00a0\u00a0\n\n\u00a0\n\n\u00a0\n\n12\u00a0\n\nhttps://backlinko.com/hub/seo/lsi\nhttps://backlinko.com/seo-checklist\n\n"
        }
      ]
    },
    "DeXSEOResearch": {
      "hand-ins": [
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearch: SEO \nDigital Excellence \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n\n \n\n\n\nVersion history \n \n\n  \n\n1 \n\nVersion Date Author Changes \n\nV1.0 13-10-2020 Niek van Dam Initial document structure \n\n   -  \n\n\n\nTable of contents \nVersion history 1 \n\nTable of contents 2 \n\nReason for this research 3 \n\nResearch method 3 \n\nResearch questions 3 \nMain questions 3 \nSub questions for each SEO improvement 3 \nSub questions for checking if SEO is working 3 \n\nWhat is SEO? 4 \n\nWhere can I find information on how to improve SEO? 5 \nGoogle SEO Starter guide 5 \nBlacklinko 6 \nNeilpatel blog 7 \n\nHow to test whether your SEO is functional? 8 \nGoogle analytics 8 \nGoogle Search Console 8 \nSEMRush 8 \n\nFinal remarks 9 \nKeywords 9 \n\nConclusion 10 \n \n\n  \n\n2 \n\n\n\nReason for this research \n \nWithin DeX, I took the responsibility to increase the SEO on the website. Before this, I had no real \nexperience with SEO besides some small posts I read about this here and there. In this document \nI will be looking for some good sources I can use in order to start working on SEO, combined with \ntools that can be used in order to test whether or not I have done a good job.  \n \n\nResearch method \nThe research method that I have applied in this research document is community research. I \nstarted off by looking online for any threads about SEO which asked for checklists with SEO \nimprovements. After a while I gathered a big collection of \u2018seo checklists\u2019. Most of these were \nsmall and irrelevant, until I got a collection of extensive SEO improvements which were all unique \nin their own way. The collections with SEO improvements that remain will be discussed in the \ndocument below, as one of the main research questions.  \n\n \n\nResearch questions \nBefore the start of the research, we have to set several questions we would like to answer. These \nquestions are as follows: \n\nMain questions \n- What is SEO? \n- Where can I find information on how to improve SEO? \n- How to test whether or not SEO is working? \n\nSub questions for each SEO improvement \n- Complexity, how difficult is it to implement the SEO improvement? \n- Impact, how impactful is the SEO improvement? \n\n \n\nSub questions for checking if SEO is working \n- Complexity \n- Usability \n- Cost \n\n \n\n3 \n\n\n\nWhat is SEO?\u00a0\nA lot of employees and students of FHICT create a lot of nice products. As a hobby, for their work,\u00a0\n\nor because of a school project. These can be software solutions, small scripts, research papers,\u00a0\n\nthesis but possibly also project ideas and proposals for others to pick up. Unfortunately, most of\u00a0\n\nthem remain private and are not known by the majority of the population. The goal of the\u00a0\n\nDeX-platform (short: DeX) is to make all this work more findable and thus improving collaboration.\u00a0\n\n\u00a0\n\nTo get people on the platform, we need to edit the website in such a way that the platform\u00a0\n\nbecomes more discoverable on SERPs (Search Engine Result Pages). A few common examples of\u00a0\n\nSERPs are Google, Bing, and Yahoo. These SERPs get their results from the respective web\u00a0\n\ncrawlers, which are constantly indexing new webpages and ranking these for specific keywords.\u00a0\n\nThe goal with SEO (Search Engine Optimization) is to reach the highest possible index on these\u00a0\n\nSERPS. This can be reached by modifying your pages, adding metadata, and guiding the crawlers.\u00a0\u00a0\n\n\u00a0 \u00a0\n\n4 \n\n\n\nWhere can I find information on how to improve \nSEO? \n\nGoogle SEO Starter guide \nAnother trusted resource is the \u200bGoogle SEO Starter Guide\u200b. Since this is made by Google, we can \nassure that it covers most of the requirements which the GoogleBot has.  \n \nGoogle offers a good starting guide in order to improve basic SEO. The tutorial is written in an \naccessible way, without any prior knowledge needed to get started. It starts off with basics like \nadding robots.txt, unique title tags, description meta tags, basic markup and url optimizations.  \n \nThe tutorial goes into depth on the content markup on the websites. It emphasises the importance \nof a well structured and user-friendly website. It also slightly references the usage of \u2018trusted \nsources\u2019, which is an additional parameter google uses for ranking.  \n \nOverall, this SEO Starter Guide is a very good foundation to get a basic understanding of SEO. In \nterms of complexity, this guide has a lot of accessible topics and I would recommend it as a \nstarter guide. The impact of applying the SEO tips noted in this guide will be enough for you to get \nmore organic clicks on your website.  \n \n \n\n  \n\n5 \n\nhttps://support.google.com/webmasters/answer/7451184?hl=en\n\n\nBlacklinko \nBlacklinko is a highly praised website for SEO which I saw coming by quite a few times while \nresearching. This website has tons of information about SEO and thanks to the common blog \nposts it has a lot of resources about SEO knowledge.  \n \nBlacklinko offers extensive in-depth tips and tricks about how to improve your SEO. Due to how \nin-depth it is, it can be hard to grasp some concepts which are applied in the blogs. Whereas \nGoogle was mostly practical examples with not a lot of technical jargon, Blacklinko has the \ntendency to go into more technical language pretty quickly.  \n \nThe ways on how to improve SEO are also completely different than what Google offers. \nBlacklinko goes into the ways that crawlers work, and modify their content so that it \u2018optimizes\u2019 for \nthe crawlers (which is \u200bsurprisingly \u200benough what SEO stands for). That doesn\u2019t mean that \nBlacklinko doesn\u2019t go into the basics as well - since it does -, but the majority of the blog posts are \nabout really specialized topics.  \n \nOverall, Blacklinko has a broad array of ticks and trips, but I would not recommend it as a starter\u2019s \nguide because it has too many specialised topics. It would be better to start with basic SEO then \nto immediately jump the gun and do some highly specialised SEO tricks.  \n \n\n  \n\n6 \n\n\n\nNeilpatel blog \nNeil patel\u2019s blog is comparable to blacklinko, but this blog does not only feature SEO. Neil Patel\u2019s \nblog is about a broad array of business and ecommerce related topics, that\u2019s where SEO comes \nin.  \n \nThe blogs which are about SEO are a huge source of information, and are usually more than just \na few lines of information. Most guides I found from Neil concerning SEO have over 300+ pages \nof information, all for free. The information that Neil delivers is comprehensible and easily \nreadable, with a lot of visualisations to explain foreign concepts.  \n \nI do however feel like Neil\u2019s blog is usually more oriented about the business side of SEO as well, \nwhich is certainly an interesting point of view, but not really what I want to achieve with DeX. \nOverall the advice from neil is doable and accessible, but it would not be my go-to choice.  \n \n\n  \n\n7 \n\n\n\nHow to test whether your SEO is functional? \n\nGoogle analytics \n \nThe most prominent one being Google Analytics, which gives you extensive information into your \nvisitors like: bounce percentage, time spent per page, conversions per page etc. In short, this \nlibrary is ideal for analysing user behaviour on your website.  The problem that google analytics \nbrings with it is the fact that we would also have to update our privacy policy, since google garners \nquite some user data which we can\u2019t do without the users\u2019 permissions. \n \nFurthermore, Google analytics is free and easy to implement/use, if privacy was not a problem \nthen this would be a go-to tool. \n \n\nGoogle Search Console \nThis tool is the little brother of Google Analytics, which is more focussed on SEO. Google Search \nConsole shows the sites from which your visitors came and also shows the amount of \nimpressions you get a day. You can also request indexing of your website, and see where you \nrank on google with each Besides analysing where your visitors are coming from, this tool also \noffers website analytics which shows web pages that aren\u2019t optimised for SEO.  \n \nIn order to implement the tool you only need to confirm that the website is yours in the DNS \nrecords, after which you can access all data that google search console has gathered of your \nwebsite, no payment required.  \n\nSEMRush \nThis is a sophisticated SEO tool, which is probably too overkill for a project like DeX. SEMrush \noffers on-demand SEO audits, analytics of your competitor\u2019s SEO strategies, pre generated \nkeywords and new potential keywords. It has a built in dashboard for your website in which you \ncan see all kinds of automated reports. The \u2018crawlability\u2019, \u2018site performance\u2019, \u2018internal linking ratio\u2019, \nand the \u2018keyword ranking\u2019 are just a few examples from the extensive list of power tools that \nSEMrush has to offer.  \n \nThis tool sounds really powerful and could really well be incorporated in an enterprise \nenvironment. This, however, is not the case for DeX and would probably be over the top for this \nproject.  \n \n \n\n8 \n\n\n\nFinal remarks \n\nKeywords \nOne thing that all of the guides for SEO had in common was the importance of keywords, which is \na topic that I haven\u2019t really touched yet in this research document.  \n \nThe importance of keywords is one which I overlooked often during my research, but it still plays a \nvery prominent role in the increase of SEO. The theory behind it is that once you fill your website \nwith certain \u2018keywords\u2019, crawlers will recognize it and add a \u2018general topic\u2019 to a website. This \ngeneral topic will then also be added to your website. As a result, your website has the potential to \nalso show up when the \u2018general topic\u2019 is searched for.  \n \nThe more keywords you have, the broader your general topics are, the more potential you have to \nappear in the SERPs. Therefore it\u2019s always a good idea to do some research into keywords and \nimplement them on your website.   \n\n9 \n\n\n\nConclusion \nIn conclusion, the Google SEO getting started tutorial is probably the best resource to get started \nwith SEO, in combination with the Google Search Console. This is because Google offers \nentry-level resources with a lot of additional options to go more in-depth into specific subjects. \n \nThe other blogs are definitely as useful if not more, but not for starting out with SEO, since it is too \nspecialized in my opinion.  \n\n10 \n\n\n"
        }
      ]
    },
    "DeXSSRAngularXtoAngularUniversal": {
      "hand-ins": [
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\nDeX\u00a0\nConversion to\u00a0\nAngular Universal\u00a0\n\n\u00a0\n\n\u00a0\n\n12.11.2020\u00a0\n\nVersion 0.1\u00a0\n\n\u00a0 \u00a0\n\n\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\nVersion History\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0 \u00a0\n\n1\u00a0\n\nVersion\u00a0 Date\u00a0 Author\u00a0 Changes\u00a0\n\nV0.1\u00a0 12/11/2020\u00a0 Niek van Dam\u00a0 Initial document structure\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\nTable of Content\u00a0\n\n\u00a0\n\nVersion History 1\u00a0\n\nTable of Content 2\u00a0\n\n1.Introduction 3\u00a0\n\n2.Pros and Cons to Angular Universal 4\u00a0\n\nPros 4\u00a0\n\nSSR 4\u00a0\n\nResponse time 4\u00a0\n\nCons 5\u00a0\n\nJavaScript 5\u00a0\n\nDOM 5\u00a0\n\nDocumentation 5\u00a0\n\n3. How to convert angularX app to Angular Universal 6\u00a0\n\n4. Conclusion 7\u00a0\n\n\u00a0 \u00a0\n\n2\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\n1.Introduction\u00a0\n\nThis research document has been made after a discussion with some people within DeX, to see\u00a0\n\nwhether or not the updating of AngularX to Angular Universal would positively improve the\u00a0\n\nproject. The pros of Angular Universal are mostly SEO related, which is why I decided to pick up\u00a0\n\nthis research document. Firstly I am going over the pros of converting from AngularX to Angular\u00a0\n\nUniversal, and afterwards I will go over some cons which appear when converting to Universal.\u00a0\u00a0\n\n\u00a0 \u00a0\n\n3\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\n2.Pros and Cons to Angular Universal\u00a0\n\nPros\u00a0\n\nFirst we will list all the pros of converting from Angular to Angular Universal\u00a0\n\nSSR\u00a0\n\nThe main pro to using Angular Universal over Angular is the support for SSR(\u200bS\u200berver \u200bS\u200bide\u00a0\nR\u200bendering). This gives us the ability to improve SEO, by returning a fully rendered HTML page to\u00a0\nthe WebCrawlers which can be instantly read.\u00a0\u00a0\n\nBecause the content is rendered on Server Side, we are able to manipulate the website with\u00a0\n\njavascript before it is actually crawled by bots. This enables us to create dynamic OG and meta\u00a0\n\ntags, in comparison to default angular, where the javascript is only executed if the crawler\u00a0\n\nsupports it. This way we can ensure that every netizen gets the same page served to them.\u00a0\u00a0\n\n\u00a0\n\nResponse time\u00a0\n\nThis goes hand in hand with SSR, the response time of Angular Universal is faster because of the\u00a0\n\nway it handles loading webpages. Angular Universal sends the information in \u2018stages\u2019, it loads\u00a0\n\nHTML/CSS first, which shows the user a basic \u2018shell\u2019. After this is sent, it starts sending the\u00a0\n\nJavaScript. This way the user gets a seamless instant loading page, as long as they do not\u00a0\n\ninteract with any buttons with javascript handlers in the first second of loading as this will not be\u00a0\n\nloaded at that time.\u00a0\u00a0\n\nThe faster loading time also improves site traffic, as 53% of all mobile site visits are abandoned if\u00a0\n\npages take over three seconds to load. As of right now, the loading time is 2.3 seconds\u00a0\n\naccording to LightHouse reports.  The loading time will be reduced when using Universal, as\u00a0\n\neverything is done by the server which removes the slow loading time for users with bad\u00a0\n\nhardware.\u00a0\u00a0 \u00a0\n\n4\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\nCons\u00a0\n\nOf course, not all that glitters is gold, which is also the case for Angular Universal. We will now be\u00a0\n\ndiscussing the Cons that appear when switching from Angular to Angular Universal\u00a0\u00a0\n\nJavaScript\u00a0\n\nAngular Universal generates a static version of web apps when loading, called the \u2018shell\u2019.As I said\u00a0\n\nbefore, this shell appears once the HTML/CSS is done loading, after which it starts loading the\u00a0\n\nJavaScript of the web page. This usually is not a big deal, unless when working with big\u00a0\n\nJavaScript files, since the user will have to wait until the javascript is finished loading in order to\u00a0\n\nactually have a functioning webpage. One way to tackle this problem is by implementing lazy\u00a0\n\nloading. This is still an inconvenient way of getting things done with javascript.\u00a0\u00a0\n\nDOM\u00a0\n\nBecause Angular Universal generates a static webpage, the DOM (\u200bD\u200bocument \u200bO\u200bbject \u200bM\u200bodel) will\u00a0\nnot work as expected. As a result, DOM manipulation becomes a tricky situation, e.g. setting\u00a0\n\nclasses of elements or updating ID\u2019s become unavailable to you. According to angular\u00a0\n\nconventions, you are not recommended to be working with the DOM regardless. As long as this\u00a0\n\nis already the case, there is nothing to worry about with the DOM.\u00a0\u00a0\n\nBut there is a pitfall to this, even if you are compliant to all the conventions, you can still get\u00a0\n\nscrewed over by this constraint. The developer might not be using the DOM, but libraries sure\u00a0\n\nwill. A big load of libraries are currently working with DOM manipulation. Many libraries have\u00a0\n\nbeen working on this issue since SSR came out, but there are still some libraries that do not\u00a0\n\nsupport \u2018out of the box\u2019 functionality.  This issue only applies to libraries which offer direct\u00a0\n\ncomponents or directives. Libraries with business logic should have no issues with functioning as\u00a0\n\nexpected.\u00a0\u00a0\n\nDocumentation\u00a0\n\nThe documentation that I could find for Angular is limited, there are some basic tutorials. Besides\u00a0\n\nthis there were little to no big comprehensive guides or documentations which you can use as a\u00a0\n\nreference point. If you stumble into any exceptions while working you are on your own.\u00a0\u00a0\n\n\u00a0 \u00a0\n\n5\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\n3. Conclusion\u00a0\n\nAfter trying to convert the AngularX project to Angular Universal, I found out that this is definitely\u00a0\n\nnot a worthwhile conversion. I tried to use a few different tutorials, but none of them seemed to\u00a0\n\nwork as expected. When I finally did manage to convert it to Angular Universal, I was presented\u00a0\n\nwith a broad array of broken libraries. I tried fixing these libraries, but most of the issues were too\u00a0\n\ncomplex to solve in a matter of hours and would require us to rewrite parts of certain libraries.\u00a0\u00a0\n\nIn summary, it is better to stay with AngularX and not convert to Angular universal as it would\u00a0\n\nheavily impact the project in a negative way.\u00a0\n\n6\u00a0\n\n\n"
        }
      ]
    },
    "EerstewerkdagEvaluatie": {
      "hand-ins": [
        {
          "text": "Evaluatie en Reflectie:  les geven\n\nInleiding\nNa twee jaar geleden afgestudeerd te zijn aan het Scalda College voor Techniek en Design in Vlissingen ben ik door mijn oude opleidingshoofd (Bram)  van Software Development teruggevraagd om studenten te begeleiden als onderwijsassistent. Binnen de opleiding zitten 3 klassen, waarvan klassen 2 en 3 bezig zijn met groepsprojecten en het maken van webapps. Jaar 1 is nog bezig het met exploreren van software development en databases.\nOrigineel was het idee om online begeleiding te geven, dit mocht dankzij de nieuwe maatregelen ook op locatie. \n\nEvaluatie\nMijn eerste dag als onderwijsassistent was een leuke en interessante ervaring, in het algemeen beter dan verwacht. \nDe dag begon met een stand-up, waar ik een aantal groepen moest overzien om te kijken of de stand-up volgens plan ging, en of er geen problemen waren. Na de stand-ups ben ik bij de groepen langsgegaan waar problemen werden besproken tijdens de stand-ups en heb ik gekeken of er behoefte was aan extra uitleg/hulp.  \nDe verdere invulling van de ochtenduren bestond voornamelijk uit het sparren met studenten over waarom bepaalde implementaties wel/niet handig zouden zijn binnen het project. Na bij een handvol aan studenten rond te zijn gegaan kwam ik er wachter dat de leesbaarheid van de code laag lag, waardoor ze vaak ook zelf verward waren over hun code. Dit was goed te merken als je vragen stelde over de locatie van sommige variabelen en methoden. Toen ik dit zag heb ik het besproken met Bram en gevraagd of er profijt kon worden gehaald uit een kleine workshop over clean code schrijven. \nZelf leek het mij leuk en leerzaam om een workshop te geven, wat een van de redenen was voor dit aanbod. Ook de docenten waren blij met dit aanbod en gaven mij een half uur om kort iets voor te bereiden. In dit half uur heb ik snel een kleine presentatie bij elkaar gegooid over code smells en het maken van clean code, de overige tijd heb ik gevuld met het uitleggen seperation of concerns.  Aan het einde van de presentatie heb ik veel positieve reacties ontvangen, wat mij motiveert om vaker in de toekomst zulke kleine workshops te geven indien nodig. \nDe middag heb ik besteed aan het helpen van eerstejaars, zij bezig waren met het vak Databases. Ze waren vooral bezig met het exploreren van `INNER JOIN` statements, waar ik samen met ze een handvol aan opdrachten ben langsgelopen. In het begin legde ik zelf de vraag uit en ging dan met de studenten de approach ontleden. Het doel was dat ik de eerstejaars zelf het verband liet leggen tussen het gebruiken van bepaalde syntax en bepaalde problemen.  \nIk heb de dag enorm positief ervaren en heb veel leuke feedback ontvangen over mijn manier van lesgeven/uitleggen. Dit heeft mij enorm gemotiveerd om het volgende week weer te doen. \n\nReflectie\nTijdens het lesgeven heb ik wel een paar keer het idee gehad dat ik erg chaotisch bezig was. Zo vergat ik om terug te komen naar bepaalde studenten zoals ik had gezegd, en was ik i.p.v. iets aan het uitzoeken voor 1 student opeens weer een andere student aan het helpen. Dit is iets waar ik in de toekomst beter op zou moeten letten om het voor iedereen soepel te laten lopen. \nNa een kleine brainstorm over hoe ik dit zou kunnen oplossen kwam ik op het idee om de volgende keer een klein lijstje te maken met namen van studenten waar ik nog langs moet. Als ik langs een student ben geweest kan ik deze dan weer afstrepen en kan ik door naar de volgende. \n\nConclusie\nAan het einde van de dag kan ik heel positief terugkijken op hoe mijn eerste werkdag is verlopen en ben ik enthousiast om  volgende week weer terug te gaan. Ik vind het fijn dat de studenten open staan voor mijn input en er wederzijds respect is, wat een goede werksfeer maakt. Wel heb ik gemerkt dat ik mijn taken wel goed op orde moet hebben zodat andere hier niet de dupe van worden. \n"
        }
      ]
    },
    "JetsonNano2GBfoutmeldingen": {
      "hand-ins": [
        {
          "text": [
            "---------- Tools/environment_install/install-prereqs-ubuntu.sh start ----------\n",
            "Get:1 file:/var/cuda-repo-10-2-local-10.2.89  InRelease\n",
            "Ign:1 file:/var/cuda-repo-10-2-local-10.2.89  InRelease\n",
            "Get:2 file:/var/visionworks-repo  InRelease\n",
            "Ign:2 file:/var/visionworks-repo  InRelease\n",
            "Get:3 file:/var/visionworks-sfm-repo  InRelease\n",
            "Ign:3 file:/var/visionworks-sfm-repo  InRelease\n",
            "Get:4 file:/var/visionworks-tracking-repo  InRelease\n",
            "Ign:4 file:/var/visionworks-tracking-repo  InRelease\n",
            "Get:5 file:/var/cuda-repo-10-2-local-10.2.89  Release [574 B]\n",
            "Get:6 file:/var/visionworks-repo  Release [2001 B]\n",
            "Get:7 file:/var/visionworks-sfm-repo  Release [2005 B]\n",
            "Get:8 file:/var/visionworks-tracking-repo  Release [2010 B]\n",
            "Get:5 file:/var/cuda-repo-10-2-local-10.2.89  Release [574 B]\n",
            "Get:6 file:/var/visionworks-repo  Release [2001 B]\n",
            "Get:7 file:/var/visionworks-sfm-repo  Release [2005 B]\n",
            "Get:8 file:/var/visionworks-tracking-repo  Release [2010 B]\n",
            "Hit:9 http://ports.ubuntu.com/ubuntu-ports bionic InRelease\n",
            "Hit:10 http://ports.ubuntu.com/ubuntu-ports bionic-updates InRelease\n",
            "Hit:11 http://ports.ubuntu.com/ubuntu-ports bionic-backports InRelease\n",
            "Hit:12 https://repo.download.nvidia.com/jetson/common r32.5 InRelease\n",
            "Hit:13 http://ports.ubuntu.com/ubuntu-ports bionic-security InRelease\n",
            "Hit:14 https://repo.download.nvidia.com/jetson/t210 r32.5 InRelease\n",
            "Reading package lists...\n",
            "1\n",
            "##############################################\n",
            "Add user to dialout group to allow managing serial ports\n",
            "##############################################\n",
            "Done!\n",
            "python-wxgtk3.0 - Python interface to the wxWidgets Cross-platform C++ GUI toolkit\n",
            "python-wxgtk3.0-dev - Development files for wxPython\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "build-essential is already the newest version (12.4ubuntu1).\n",
            "ccache is already the newest version (3.4.1-1).\n",
            "gawk is already the newest version (1:4.1.4+dfsg-1build1).\n",
            "libtool is already the newest version (2.4.6-2).\n",
            "libtool-bin is already the newest version (2.4.6-2).\n",
            "make is already the newest version (4.1-9.1ubuntu1).\n",
            "python-dev is already the newest version (2.7.15~rc1-1).\n",
            "python-numpy is already the newest version (1:1.13.3-2ubuntu1).\n",
            "python-pyparsing is already the newest version (2.2.0+dfsg1-2).\n",
            "python-serial is already the newest version (3.4-2).\n",
            "python-setuptools is already the newest version (39.0.1-2).\n",
            "python-yaml is already the newest version (3.12-1build2).\n",
            "gcovr is already the newest version (3.4-1).\n",
            "lcov is already the newest version (1.13-3).\n",
            "libcsfml-audio2.4 is already the newest version (2.4-2).\n",
            "libcsfml-dev is already the newest version (2.4-2).\n",
            "libcsfml-graphics2.4 is already the newest version (2.4-2).\n",
            "libcsfml-network2.4 is already the newest version (2.4-2).\n",
            "libcsfml-system2.4 is already the newest version (2.4-2).\n",
            "libcsfml-window2.4 is already the newest version (2.4-2).\n",
            "libsfml-audio2.4 is already the newest version (2.4.2+dfsg-4).\n",
            "libsfml-dev is already the newest version (2.4.2+dfsg-4).\n",
            "libsfml-graphics2.4 is already the newest version (2.4.2+dfsg-4).\n",
            "libsfml-network2.4 is already the newest version (2.4.2+dfsg-4).\n",
            "libsfml-system2.4 is already the newest version (2.4.2+dfsg-4).\n",
            "libsfml-window2.4 is already the newest version (2.4.2+dfsg-4).\n",
            "python-matplotlib is already the newest version (2.1.1-2ubuntu3).\n",
            "python-scipy is already the newest version (0.19.1-2ubuntu1).\n",
            "python-wxgtk3.0 is already the newest version (3.0.2.0+dfsg-7).\n",
            "g++ is already the newest version (4:7.4.0-1ubuntu2.3).\n",
            "g++-arm-linux-gnueabihf is already the newest version (4:7.4.0-1ubuntu2.3).\n",
            "git is already the newest version (1:2.17.1-1ubuntu0.8).\n",
            "libpython2.7-stdlib is already the newest version (2.7.17-1~18.04ubuntu1.6).\n",
            "libxml2-dev is already the newest version (2.9.4+dfsg1-6.1ubuntu1.3).\n",
            "libxslt1-dev is already the newest version (1.1.29-5ubuntu0.2).\n",
            "python-psutil is already the newest version (5.4.2-1ubuntu0.1).\n",
            "wget is already the newest version (1.19.4-1ubuntu2.2).\n",
            "pkg-config-arm-linux-gnueabihf is already the newest version (4:7.4.0-1ubuntu2.3).\n",
            "python-opencv is already the newest version (3.2.0+dfsg-4ubuntu0.1).\n",
            "python-pip is already the newest version (9.0.1-2.3~ubuntu1.18.04.4).\n",
            "xterm is already the newest version (330-1ubuntu2.2).\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  apt-clone archdetect-deb bogl-bterm busybox-static cryptsetup-bin\n",
            "  dpkg-repack gir1.2-timezonemap-1.0 gir1.2-xkl-1.0 grub-common\n",
            "  kde-window-manager kinit kio kpackagetool5 kwayland-data kwin-common\n",
            "  kwin-data kwin-x11 libdebian-installer4 libkdecorations2-5v5\n",
            "  libkdecorations2private5v5 libkf5activities5 libkf5attica5\n",
            "  libkf5completion-data libkf5completion5 libkf5declarative-data\n",
            "  libkf5declarative5 libkf5doctools5 libkf5globalaccel-data libkf5globalaccel5\n",
            "  libkf5globalaccelprivate5 libkf5idletime5 libkf5jobwidgets-data\n",
            "  libkf5jobwidgets5 libkf5kcmutils-data libkf5kcmutils5 libkf5kiocore5\n",
            "  libkf5kiontlm5 libkf5kiowidgets5 libkf5newstuff-data libkf5newstuff5\n",
            "  libkf5newstuffcore5 libkf5package-data libkf5package5 libkf5plasma5\n",
            "  libkf5quickaddons5 libkf5solid5 libkf5solid5-data libkf5sonnet5-data\n",
            "  libkf5sonnetcore5 libkf5sonnetui5 libkf5textwidgets-data libkf5textwidgets5\n",
            "  libkf5waylandclient5 libkf5waylandserver5 libkf5xmlgui-bin libkf5xmlgui-data\n",
            "  libkf5xmlgui5 libkscreenlocker5 libkwin4-effect-builtins1 libkwineffects11\n",
            "  libkwinglutils11 libkwinxrenderutils11 libqgsttools-p1 libqt5designer5\n",
            "  libqt5help5 libqt5multimedia5 libqt5multimedia5-plugins\n",
            "  libqt5multimediaquick-p5 libqt5multimediawidgets5 libqt5opengl5\n",
            "  libqt5quickwidgets5 libqt5sql5 libqt5test5 libxcb-composite0 libxcb-cursor0\n",
            "  libxcb-damage0 os-prober python3-dbus.mainloop.pyqt5 python3-icu python3-pam\n",
            "  python3-pyqt5 python3-pyqt5.qtsvg python3-pyqt5.qtwebkit python3-sip\n",
            "  qml-module-org-kde-kquickcontrolsaddons qml-module-qtmultimedia\n",
            "  qml-module-qtquick2 rdate tasksel tasksel-data\n",
            "Use 'sudo apt autoremove' to remove them.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 157 not upgraded.\n",
            "Collecting future\n",
            "  Using cached https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz\n",
            "Collecting lxml\n",
            "  Using cached https://files.pythonhosted.org/packages/e5/21/a2e4517e3d216f0051687eea3d3317557bde68736f038a3b105ac3809247/lxml-4.6.3.tar.gz\n",
            "Collecting pymavlink\n",
            "  Using cached https://files.pythonhosted.org/packages/1b/59/34e07d7049e0cded0e7ffbae3e44c36d4a182c85c2655dea5ca4cf1d9d9a/pymavlink-2.4.14.tar.gz\n",
            "Collecting MAVProxy\n",
            "  Using cached https://files.pythonhosted.org/packages/40/e5/827562a3f2db94621ecd375edd79da367ec8a7740993087d8ccb3da7ecc2/MAVProxy-1.8.34.tar.gz\n",
            "Collecting pexpect\n",
            "  Using cached https://files.pythonhosted.org/packages/39/7b/88dbb785881c28a102619d46423cb853b46dbccc70d3ac362d99773a78ce/pexpect-4.8.0-py2.py3-none-any.whl\n",
            "Collecting flake8\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/b0/3b5820728d687f2c000476216a3fccc7a03baac1034afc0284ccde25e26d/flake8-3.9.1-py2.py3-none-any.whl (73kB)\n",
            "Collecting pygame\n",
            "  Using cached https://files.pythonhosted.org/packages/c7/b8/06e02c7cca7aec915839927a9aa19f749ac17a3d2bb2610b945d2de0aa96/pygame-2.0.1.tar.gz\n",
            "    Complete output from command python setup.py egg_info:\n",
            "    \n",
            "    \n",
            "    WARNING, No \"Setup\" File Exists, Running \"buildconfig/config.py\"\n",
            "    Using UNIX configuration...\n",
            "    \n",
            "    sh: 1: sdl2-config: not found\n",
            "    sh: 1: sdl2-config: not found\n",
            "    sh: 1: sdl2-config: not found\n",
            "    \n",
            "    Hunting dependencies...\n",
            "    \n",
            "    ---\n",
            "    For help with compilation see:\n",
            "        https://www.pygame.org/wiki/CompileUbuntu\n",
            "    To contribute to pygame development see:\n",
            "        https://www.pygame.org/contribute.html\n",
            "    ---\n",
            "    \n",
            "    Traceback (most recent call last):\n",
            "      File \"<string>\", line 1, in <module>\n",
            "      File \"/tmp/pip-build-Ddcj_7/pygame/setup.py\", line 318, in <module>\n",
            "        buildconfig.config.main(AUTO_CONFIG)\n",
            "      File \"buildconfig/config.py\", line 221, in main\n",
            "        deps = CFG.main(**kwds)\n",
            "      File \"buildconfig/config_unix.py\", line 194, in main\n",
            "        DependencyProg('SDL', 'SDL_CONFIG', 'sdl2-config', '2.0', ['sdl']),\n",
            "      File \"buildconfig/config_unix.py\", line 39, in __init__\n",
            "        self.ver = config[0].strip()\n",
            "    IndexError: list index out of range\n",
            "    \n",
            "    ----------------------------------------"
          ]
        }
      ]
    },
    "ORProjectstructure": {
      "hand-ins": [
        null
      ]
    },
    "PoCObjectDetection": {
      "hand-ins": [
      ]
    },
    "PoCTextgrading": {
      "hand-ins": [
      ]
    },
    "ResearchNavigationmethods": {
      "hand-ins": [
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDrone_brainstorm\n\n\n------------------------------------brainstorm-Sieuwe------------------------------------------------------------\n\nProof of concept\nTo prove the hardware pipeline from sensor input to motor control a proof of concept can be\nmade. This proof of concept will also be a practise on working with the different modules.\nThe proof of concept is to create an algorithm that can autonomously follow a moving\nperson. To keep the proof of concept simple the drone does not need to take into account\nobstacles. The drone only has to keep following a person in an open area.\n\nA possible approach for this can consist of the following pipeline.\n\nHardware\nA RGB camera connected with a csi interface to the jetson nano\nA solid state lidar (tf luna) connected to the Jetson nano with i2c\nThese two sensors have been calibrated so that the 5*5cm square area of the solid state\nlidar is in the middle of the RGB camera feed at a distance of 5m.\nThe jetson nano is connected to the pixhawk using a usb interface.\nThe drone actuators will be controlled by the pixhawk.\n\nSoftware\nThe jetson nano will run a python script which follows the following steps:\n1- Using jetson utils library a mobilenet model will produce bounding boxes at 20fps. Only\nbounding boxes containing people will be passed to SORT.\n2- A tracker like SORT will track bounding box id\u2019s between frames.\n\nDuring each tracker update (also 20 times a second) the jetson nano will:\n1- Calculate the centerpoint of the bounding box with ID=1 given by SORT. (other\nbounding boxes will be ignored)\n2- Use the centerpoint of the incoming image to calculate the distance between the\ncenterpoint and the bounding box centerpoint. This distance will feeded into either a\nPID controller if possible or else a deque of 10. The average of this deque is than\ncalculated and a factor is applied to create two values ranging from -1 to 1. The first\nvalue is the X axis and the second value is the Z axis.\n3- To generate the Y axis (depth), the lidar sensor is used. However the data from the\nlidar sensor is only used when the centerpoint of the image is in a small area around\nthe centerpoint of the bounding box. This is to stop the lidar from getting a distance\nother than distance between the drone and the person. This distance is fed inside a\nseparate PID controller or deque like explained above to keep the distance 5m.\n4- all values coming from the 3 PID controllers, for x, y and z, ranging -1 to 1 will be\nconverted into usable commands for the dronekit library. Using the dronekit library\nthe commands are send to the pixhawk. The pixhawk will in its turn update the\nmotors.\n\nThe following steps are performed 20 times a second to create smooth motion.\n\nIf the drone loses track of the person the drone will hover in its place for 20 seconds. If in the\ntime SORT gives a new bounding box then the script starts following this new boundingbox.\nOtherwise the drone will start beeping with the internal buzzer. After another 20 seconds the\ndrone will land automatically if the user has not taken control.\n\n\n\nLimitations to this approach are that the drone can follow persons good in straight lines.\nForward backwards left right. But if the persons starts walking in for example a circle then\nthe drone will lose track since there is no yaw control. Only pitch (X axis), height (Z axis) and\nroll (Y axis) are controller. However using the X and Z axis a possible Yaw can be calculated.\n\n\n\nFull navigation\n\nDirections\nWhen looking at full navigation for an autonomous quadcopter, there are 4 axis of movement\nwith 7 directions combined\n\n1- up and down\n2- forward or backward (roll)\n3- left right (pitch)\n4- rotate (yaw)\n\n3 of these four actions can when performed hit an obstacle. Yaw does not since it is done in\nplace. Only when dynamic objects are in the area, it could hit the drone.\n\nFor each movement direction the drone has to check whether it is possible or not without\nhitting any obstacles. One direct possibility would be to mount sensors on each side of the\ndrone to account for obstacles in all movable directions. Some directions are however\nperformed more during flight than others. And to keep the system as simple as possible, this\nis not a beneficial approach. For example backwards is something that could be eliminated\nas movement because the drone could just rotate 180 degree to make a the backwards\nmovement just a forward movement.\n\nThis is one of the main ideas of this navigation approach. While the drone is capable of\nmoving in all directions, making the drone movible on only the necessary directions to reach\nall possible locations will make the sensor package easier to use.\n\nThese directions are:\n1 - up and down, to fly over or under obstacles\n2 - forward, to have a horizontal movement\n3- yaw, to make the drone change its heading.\n\nThis will make the drone slower since it has to adjust its yaw frequently. But this does make\nthe navigation task easier since there are only 4 possible directions instead of 7.\n\nVision\nTo check if movement is possible concerning obstacles, it is important that the drone\n\nEnd to End full navigation approach.\nWhen looking at\n\n\n\nBrainstorm Niek\n\nPoC - Object detection\nIn order to determine whether or not the drone can properly recognise all possible\nobstacles/threats while flying, we can make the drone fly a predefined path with already\nknown obstacles. Once the flight is done, we can analyse the list of detected objects the\ndrone has seen on its way, and validate whether or not the object detection script detects all\nnecessary obstacles.\n\nHardware requirements\n- RGB camera\n\n- Used for object detection\n- Jetson nano\n\n- Used to run the object detection script on\n- Pixhawk\n\n- Flight controller for the drone\n\nSoftware requirements\nThe software required for this PoC consists mainly of the object detection script to be\nexecuted on the jetson nano, as the GPS coordinates can already be set using the dronekit.\n\nThe flowchart of this system would look as follows, assuming you have already mapped out\nthe course which the drone is going to fly. As well as having a list of all objects which the\ndrone should detect.\n\n1. Start image detection algorithm on drone\n2. Send coordinates through dronekit\n3. Let the drone fly to location x on the map\n4. List all objects which are detected along the way\n5. Save all detected objects to a local file\n6. Compare detected objects to the actual objects\n\nWhy\nThe main reason why I think this is an important PoC to realise is because of the fact that\nthe follow-up actions of the drone will most likely depend on the detected objects within the\ntracks. If the objects detected along its way are not really what it should be detecting we\ncould expect to see the drone execute redundant/unusual maneuvers, which heightens the\nchance of crashing.\n\n\n\nPoC - Simulation\nWhen flying the drone outside while working on PoC\u2019s, there is a real chance that the drone\nmay crash, resulting in damage. To counter this we can attempt to simulate the drone in a\ncontrolled environment first like: a simulated environment.\n\nWithin this simulated environment we can control the amount of detectable objects, as wel\n\nSoftware Requirements\nThe software requirements for this PoC require only one thing, which is the software on\nwhich we can run the simulation. Some additional research still has to be done regarding\nwhether or not we are using Unity or a custom made solution like this quadcopter controller\n\nWhy\nWhen simulating a drone in a safe environment like Unity, we lose the risk of the drone\ncrashing, and increase the rate at which we can test approaches. When having to implement\na new algorithm onto the drone, taking it outside and testing physically, it can take 15-20\nminutes, whereas simulating this would take up to 5.\n\nhttps://github.com/simeonradivoev/Quadcopter-Controller\n\n\nPOC - Decision making (Bas)\nIn order to make correct and reliable decisions we need a Algorithm that controls the\nmovements of the drone based on input. The main objective of this POC being: training an\nalgorithm to make its own decisions not based on a pre set course. Proving the POC could\nbe done by analysing the flight path over a preset course and observing the drones decision\nmaking.\n\nAlgorithm specifics\n\nThe chosen algorithm would be Deep Reinforcement Learning (preferably we would like to\nstart training in a simulated environment being for example a Unity3D sim. The drone would\nbe rewarded points whenever the drone comes closer to the predefined target. Everytime\nthe drone \u201ccrashes\u201d points would be detracted. A crash would be simulated by sonars which\nwould intervene whenever the drone would be actually close to crashing.\n\nHardware requirements\n\n\u25cf RGB Camera\n\u25cf Jetson Nano\n\u25cf The drone actuators will be controlled by the pixhawk\n\u25cf 4 Sonars TBD.\n\nSoftware requirements\n\nThe software requirements for this POC would be the followup of Niek\u2019s POC meaning most\nof the software components remain the same.\n\n\u25cf Start image detection algorithm on drone\n\u25cf Send coordinates through dronekit\n\u25cf Decision making algorithm to fly as fast as possible to the provided coordinates\n\u25cf Sonar intervenes when the sonar detects an object at a forward distance off 30 cm or\n\na z-axis detection at a distance off 15 cm ( deducting points ).\n\u25cf Let the drone fly to location x on the map\n\n\n\nBrainStorm\n- eroverheen eromheen wanneer?\n- hoe bepaal je de ideale vlieghoogte als die niet defined it.\n- 3D bounding boxes annotations\n- Controlling the drone with quick object detection + reinforcement learning model.\n\nObject det.\n\n- YOLO\n\nStereo vision\n- https://www.sciencedirect.com/science/article/pii/S2590005620300011 Focused op\n\nhet object voor zich.\n\navg mean\nhttps://stackoverflow.com/questions/13728392/moving-average-or-running-mean\n\npid\nhttps://pypi.org/project/simple-pid/\n\nhttps://www.sciencedirect.com/science/article/pii/S2590005620300011\nhttps://stackoverflow.com/questions/13728392/moving-average-or-running-mean\n\n"
        }
      ]
    },
    "ResearchProficiencyanalysis": {
      "hand-ins": [
        {
          "text": "Version Control\n\n\n\n\n\nIntroduction\nWithin the Quantified Student project, we thought of a new feature where we would analyse the written text from the students and detect whether their proficiency level in that language is up to standards. Before we can go into creating PoCs about this new feature, we first must research whether this is even feasible. \nThe research will be conducted by incorporating Literature study as main research method. \nResearch questions\nThe main question I want to get answered in this research is the following:\nIs it possible to analyse someone\u2019s language level based using NLP?\nIs the approach that is proposed feasible?\nIs the approach able to be implemented for the target languages (Dutch and English)?\n\n\nPaper 1:\n\nPredicting proficiency levels in learner writings by transferring a linguistic complexity model from expert-written coursebooks | \n\nThis paper covers the measuring of proficiency levels by comparing the linguistic complexity to that of a textbook at the given level. The language used in the textbooks is recorded per proficiency level, measured by the CEFR (Common European Framework of Reference for languages). This level goes from A to C, respectively being ranked from basic to proficient user. These grades can further be subdivided according to the needs of the local context. The idea behind this research method is that the model can read the input data from the student, normalize it to its basal form and compare the formatted text to the proficiency level.\nThe focus within this research paper was put on the correcting and normalizing of grammar mistakes, which led to greater performance within the network. This is not something which I think we should be focussing on right now, as this is supposed to be a secondary feature, not something which we should direct all our time and resources towards. However, it is good to keep in mind that these things can still improve performance within NLP. \nWhen asking the question whether this is feasible to implement into our own systems, I would have to say no. There are multiple reasons for this, the main one being that this method takes up a lot of resources and time to properly implement, which seems over-the-top for a small feature. Besides, this application has only been tested on one language (Swedish), which has an easier grammar system than the languages we would have to implement this software into. In our implementation, we would have to teach the network two languages (assuming people fill in their FeedPulse in either English or Dutch), this adds another dimension of difficulty as these languages have a more complex grammar system than the language used in the paper. \nIn conclusion, I do not think this is a viable approach to use, however, it did explain a lot about the data cleaning process going on behind the scenes in NLP. \n\n\nAlternative methods to analysing text proficiency\nDuring my research I saw a lot of instances where the NLP model used was way over the top for our implementation, which, although interesting, did not meet the requirements needed (mostly since the approach was not feasible and too avant-garde). There were some more basic approaches which were covered throughout my research, which seemed more in-line with what we are trying to achieve. The most promising methods have been noted down below.\n\nAnalysing proficiency based on grammar mistakes\nThe analysing of proficiency based on grammar mistakes is a more feasible goal than the methods which are proposed in the research papers. Mostly because the analysing of grammar mistakes is a lot easier to do and does not require the same amount of preparation to be done beforehand. One of the easiest ways to implement this metric is by calculating the average rate of grammar mistakes per sentence. We can measure these against a predefined scale, which can later be given back to the student as feedback. \nThe reason this is easier to implement than one of these research papers is because we can incorporate pre-existing software for this, and do not require us to build our own solution from scratch.\n\nCalculating the ARI (Automated readability index)\nThe readability index is a simple but powerful formula which rates the readability of the given text. The formula only requires basic metrics, which can easily be extracted from the data. The formula is as follows:\n4.71 (\nWhere c is the number of letters and numbers, w is the number of spaces, and s is the number of sentences (not necessarily the number of periods present in a piece of text). \nThis formula outputs a number between 1-14, which can be used to calculate the grade level at which the text has been written. The scores can be a decimal number, in which case they will be rounded up; causing a 10.1 to be interpreted as an 11. The meaning of the scores is in the table to the left, taken from the  regarding ARI.\nThe ARI looks like a good metric to use to determine proficiency of the writer, without having to train a model to predict it for us. Since this is only a basic formula, it can more easily be implemented into the system, requiring basic text analysis at best. \n\n\nGunning fog index\nIn linguistics, the Gunning fog Index is a readability formula for English writing which estimates the years of formal education needed to understand the text on first reading. This method has been developed in 1952 by Robert Gunning and has been used since. The fog index is commonly used to confirm that text can be read easily by the intended audience and can also be used to indicate the level (proficiency) of your writing skills. \nThe Gunning fog index is calculated with the following formula: \n\nIn comparison to the ARI, we do not round this number up or down when checking the end score, if the number is a decimal, it should be treated as \u2018in between\u2019 the levels. The results of this index can be seen in the table to the left. \nIn conclusion, this index can be used within our application, besides one critical point. This index is only applicable to the English language, which goes against our target languages (Dutch and English). Therefore, we would only be able to apply this to the English side of canvas. \n\nFlesch-Kincaid grade\nThe Flesch-Kincaid grading system is a widely used readability formula which assesses the approximate reading level of a given text. It has been developed by the US Navy who worked with the Reading Ease formula first but has been converted to the analysis of reading levels instead of reading ease. The obsolete version still exists; however, it is not as clear in its grading as the current formula.\nThe formula uses the same principle as the Gunning fog index but uses different variables and therefore also outputs a different result. The result which this grade gives is more generalised, whereas the fog index is specific enough to classify writer grades. \nThe formula for this grade is as follows: \n \nWhere all variables describe the count of a property within the given text (words -> total words, syllables -> total syllables). This formula results in a number between 0-18 and displays the data in 6 different sections, in value pairs of 3. \nThis grade level has a lot more potential than the Gunning fog index, as it can be widely applied to every language for as far as I can tell. This gives it significant pros over the other linguistical grading formulas.\n\n\ntRANSLATION OF TEXT FOR GRADING PURPOSES\nThe readability indexes discussed above all give a clear and direct grading of the text to be analysed, but has one prominent downfall: the language. The readability indexes are all made with the mindset of grading English texts. In theory these readability indexes can be modified to a desired language, as the majority of these readability indexes solely require the amount of: syllables, words and sentences. Before we can reliably apply this to every language, I decided to validate it myself with the following pieces of text, found on the \u2018\u2019:\n\n\u201cIk ben makelaar in koffi, en woon op de Lauriergracht. Het is mijn gewoonte niet, romans te schrijven, of zulke dingen, en het heeft dan ook lang geduurd, voor ik er toe overging een paar riem \npapier extra te bestellen, en het werk aan te vangen, dat gij, lieve lezer, in de hand hebt genomen, en dat ge lezen moet als ge makelaar in koffie zijt, of als ge wat anders zijt. Niet alleen dat ik nooit iets schreef wat naar een roman geleek, maar ik houd er zelfs niet van, iets dergelijks te lezen.\u201d (Max Havelaar - Multatuli) \n\n\n\n\n\u201cDe volle maan, tragisch dien avond, was reeds vroeg, nog in den laatsten dagschemer opgerezen als een immense, bloedroze bol, vlamde als een zonsondergang laag achter de tamarindeboomen der Lange Laan en steeg, langzaam zich louterende van hare tragische tint, in een vagen hemel op. Een doodsche stilte spande alom als een sluier van zwijgen, of, na de lange middagsi\u00ebsta, de avondrust zonder overgang van leven begon.\u201d  (De Stille Kracht \u2013 Louis Couperus) \n\n\n\n\n\n\u201cOnbegrijpelijk veel mensen hebben familiebetrekkingen, vrienden of kennissen te Amsterdam. Het is een verschijnsel dat ik eenvoudig toeschrijf aan de veelheid der inwoners van die hoofdstad. Ik had er voor een paar jaren nog een verre neef. Waar hij nu is, weet ik niet. Ik geloof dat hij naar de West gegaan is. Misschien heeft de een of ander van mijn lezers hem wel brieven meegegeven. In dat geval hebben zij een nauwgezette, maar onvriendelijke bezorger gehad, als uit de inhoud van deze weinige bladzijden waarschijnlijk duidelijk worden zal.\u201d (Camera Obscura \u2013 Hildebrand)\n\nWhen running these samples through the grading indexes we see that the result from best/worst grading fluctuates by a lot. Besides this we added one additional grade, the Flesch Reading Ease grading, which shows The fact that the data is not uniform in it\u2019s best/worst grading shows me that Dutch is not made for the grading indexes. In order to use the  grading indexes we will have to convert it to a uniform language so that all students will get the same results. \n\n\n\n\nConclusion\nAfter reading a handful of academic papers and researching a wide array of language grading systems I can safely say that the academic papers are above our reach. The methods which are being used are advanced enough to be branched into their own little project, which is over the top for the small feature we are trying to implement.\nThe grading systems and readability indexes, however, show a more promising result. These grading systems are more generic, which respectively, leads to an easier implementation. Besides that, the data which is required to work for these formulas is significantly lower than what would be needed for the NLP approaches. Where the NLP approaches would require several batches of textbook examples, combined with data cleaning; the formulas only require primitive parameters which can be extracted from the text with relative ease. Therefore, I think that we should incorporate the grading systems into this feature, instead of using an NLP solution.\nWhen choosing between the indexes, there is one which significantly stands out above the rest in terms of usefulness in my opinion: the Flesch-Kincaid grade. Where the other tools output a grade, which is specifically focussed on the American schooling system, the Flesch grade can be represented in more \u2018global\u2019 terms of difficulty. In order to make the grades work as expected we will have to find a translation service which will allow us to convert the given text to English, as these algorithms seem to be prominently language-based. \nTo counter the point of being focussed on the American schooling system, we could convert these values and turn them into a more Europeanised value system. This, however, would still give us the issue of these values being \u2018class\u2019 specific. Because it is so specific, it can more easily be prone to flaws of grading a student one rating too high-low, whereas the Flesch score puts you in a broader category.\n\nIn conclusion, I would like to implement the Flesch-Kincaid grading into this system, accompanied by the ARI grades. Due to the constraints which these grading systems put on the language, we do have to translate the to-be analysed text to English . Once converted, these two metrics combined give a clear and concrete overview of the level of writing, without having to write a big amount of software, which makes them ideal for this solution.\n\n\n"
        }
      ]
    },
    "WindPowerCreateprototype": {
      "hand-ins": [
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlan of attack\n\n\n15-12-2020 \n\n \n  \n\nPlan of attack \nMeasurements wind speed \n\nNiek & Elmira \n\n\n\nPlan of attack  Niek & Elmira \n\n \n1 \n\nIndex \nVersion control ........................................................................................................................................ 1 \n\nSituation ................................................................................................................................................... 2 \n\nThe setup ................................................................................................................................................. 3 \n\nComponents list: .................................................................................................................................. 3 \n\nThe data to be gathered .......................................................................................................................... 4 \n\nPlanning ................................................................................................................................................... 5 \n\n \n\nVersion control \n \n\nVersion Author Date Changes \n\nV0.1 Niek & Elmira 15-12-2020 Setup document \nV0.2 Elmira Drost 21-12-2020 Added the Situation \n\n \n\n\n\nPlan of attack  Niek & Elmira \n\n \n2 \n\nSituation \nSince the last press conference on the 14th of December the Netherlands has declared a country-\n\nwide lockdown. This caused us to be unable to actively work on the wind turbine project as planned. \n\nDue to this series of unfortunate events, we started looking for alternative wind turbine setups \n\nwhich can be worked at from home. After brainstorming with Eric, we came up with a plan to use an \n\nArduino in Elmira\u2019s and Niek\u2019s backyard to simulate a wind turbine.   \n\n\n\nPlan of attack  Niek & Elmira \n\n \n3 \n\nThe setup \nWe looked at two different approaches for this experiment, using two different tutorials. In both \n\ntutorials, wind speed is measured using an anemometer. The main difference in both is that they use \n\na different methodology for measuring the RPM. The RJ-11 meter uses a RJ-11 adapter to export the \n\nwind speed data, whereas the inspeed anemometer uses a power cable, where you have to measure \n\nthe amount of rotations per second manually.   \n\nParts needed RJ-11 Anemometer Parts needed Inspeed Anemometer  \n\nArduino Uno Wifi 47,90 Arduino Uno Wifi 47.90 \nRJ-11 Connector 24,88  0 \nAnemometer RJ-11 25,90  0 \n 0  Inspeed classic \n\nAnemometer \n59,00 \n\n4.7K Resistors 9,61 4.7K Resistors 9,61 \nM2M wires 6,49 M2M wires 6,49 \nPower(bank) source 14,95 Power(bank) source 14,95 \n\nBreadboard 6,50 Breadboard 6,50 \n\nTotal cost 136,23  233,55 \n\n \n\nAfter comparing feasibility and effectivity, we decided to choose the RJ-11 Anemometer, and are \n\ngoing to continue developing the prototype with that. \n\nComponents list: \nThe setup will be the following: \n\n \n\n \n\n\u2022 Arduino Uno WiFi  \n\n\u2022 Anemometer \n\n\u2022 RJ-11 Connector for anemometer \n\n\u2022 4.7K Resistor \n\n\u2022 External 5V power source \n\n  \n\nhttp://forcetronic.blogspot.com/2016/12/measuring-wind-speed-with-anemometer.html\nhttp://cactus.io/hookups/weather/anemometer/davis/hookup-arduino-to-davis-anemometer\nhttps://www.bol.com/nl/p/arduino-uno-wifi-rev2/9200000114847338/\nhttps://www.bol.com/nl/p/arduino-uno-wifi-rev2/9200000114847338/\nhttps://nl.rs-online.com/web/p/chip-programming-adapters/6677889/\nhttps://www.amazon.nl/gp/product/B07BMVYBW9/ref=ox_sc_act_title_1?smid=A2ES8YDVVOD63L&psc=1\nhttps://www.store.inspeed.com/Inspeed-Classic-Anemometer-Sensor-Only-WS.htm\nhttps://www.store.inspeed.com/Inspeed-Classic-Anemometer-Sensor-Only-WS.htm\nhttps://www.amazon.nl/Projects-10EP5124K70uk-4-7k-weerstanden-Pack/dp/B07YNVLCRX/ref=sr_1_2?__mk_nl_NL=%C3%85M%C3%85%C5%BD%C3%95%C3%91&crid=2WF723DXMLXN0&dchild=1&keywords=4.7k+ohm&qid=1608218400&sprefix=4.7%2Caps%2C313&sr=8-2\nhttps://www.amazon.nl/Projects-10EP5124K70uk-4-7k-weerstanden-Pack/dp/B07YNVLCRX/ref=sr_1_2?__mk_nl_NL=%C3%85M%C3%85%C5%BD%C3%95%C3%91&crid=2WF723DXMLXN0&dchild=1&keywords=4.7k+ohm&qid=1608218400&sprefix=4.7%2Caps%2C313&sr=8-2\nhttps://www.bol.com/nl/p/dupont-jumper-kabels-40-stuks-10cm-voor-breadboard-arduino/9200000112732180/?bltgh=lyoMWCofCzeEMT5ANBFBIg.1_28.39.ProductTitle\nhttps://www.bol.com/nl/p/dupont-jumper-kabels-40-stuks-10cm-voor-breadboard-arduino/9200000112732180/?bltgh=lyoMWCofCzeEMT5ANBFBIg.1_28.39.ProductTitle\nhttps://www.bol.com/nl/p/mophie-power-boost-xl-powerbank-10-400-mah-zwart/9200000080445342/?bltgh=peqX4kc0h17VwWWy5CpMDw.r1tPn425oYeScLM8YiRMCg_0_45.49.ProductTitle\nhttps://www.bol.com/nl/p/mophie-power-boost-xl-powerbank-10-400-mah-zwart/9200000080445342/?bltgh=peqX4kc0h17VwWWy5CpMDw.r1tPn425oYeScLM8YiRMCg_0_45.49.ProductTitle\nhttps://www.bol.com/nl/p/breadboard-170-contactpunten-arduino-compatible-wit/9300000008153326/?bltgh=s6mMzyeveRG7KugrjO9GQQ.nsY9ZSFS6Cxau5nA-6tDJA_0_23.30.ProductImage\nhttps://www.bol.com/nl/p/breadboard-170-contactpunten-arduino-compatible-wit/9300000008153326/?bltgh=s6mMzyeveRG7KugrjO9GQQ.nsY9ZSFS6Cxau5nA-6tDJA_0_23.30.ProductImage\n\n\nPlan of attack  Niek & Elmira \n\n \n4 \n\nThe data to be gathered \nThe Arduino will be set up to record the current data every x minutes. Once these x minutes have \n\npassed, we will start the gathering of data. The anemometer will record the current wind speed in \n\nm/s, accompanied by an OpenWeather API call which returns the current weather status. \n\nThe data from the anemometer will be converted to what the power output should have been, had \n\nwe used a TESUP turbine.  We can calculate this output by referencing the spec sheet of the turbine, \n\nwhich shows the output in W (watts) at the given wind speed. This will be stored as the \u2018current \n\npower output\u2019. Once the data collection is done, we will use this current power output as output \n\nvariable in the prediction model. This means that this data will be the \u2018expected output\u2019 of the \n\nprediction model.   \n\nThe OpenWeather API will return a collection of weather data statistics, e.g.: Type of weather, \n\ntemperature, the angle of the wind. This weather data will be used as the prediction parameters, \n\nthese are parameters that the prediction model will use as input to predict the output variables.   \n\n \n\n  \n\n\n\nPlan of attack  Niek & Elmira \n\n \n5 \n\nPlanning \nPlanning for delivery time of the parts: \n\nParts needed RJ-11 Anemometer Delivery time \n\nArduino Uno Wifi 47,90 Circa 2 werkdagen \nRJ-11 Connector 24,88 Circa 2 werkdagen \nAnemometer RJ-11 25,90 Maandag, 21 dec \n -  \n\n4.7K Resistors 9,61 dinsdag, 29 dec \nM2M wires 6,49 21 december \nPower(bank) source 14,95 1 werkdag \n\nBreadboard 6,50 2 werkdagen \n\n \n\nPlanning for setup of the windspeeldcalculator2.0: \n\nActivities Days \nSetup physically 2 days \n\nProgram time Arduino 1 day \n\nProgram time link tesup specsheet + opweather \napi \n\n3 days \n\n \n\nhttp://forcetronic.blogspot.com/2016/12/measuring-wind-speed-with-anemometer.html\nhttps://www.bol.com/nl/p/arduino-uno-wifi-rev2/9200000114847338/\nhttps://nl.rs-online.com/web/p/chip-programming-adapters/6677889/\nhttps://www.amazon.nl/gp/product/B07BMVYBW9/ref=ox_sc_act_title_1?smid=A2ES8YDVVOD63L&psc=1\nhttps://www.amazon.nl/Projects-10EP5124K70uk-4-7k-weerstanden-Pack/dp/B07YNVLCRX/ref=sr_1_2?__mk_nl_NL=%C3%85M%C3%85%C5%BD%C3%95%C3%91&crid=2WF723DXMLXN0&dchild=1&keywords=4.7k+ohm&qid=1608218400&sprefix=4.7%2Caps%2C313&sr=8-2\nhttps://www.bol.com/nl/p/dupont-jumper-kabels-40-stuks-10cm-voor-breadboard-arduino/9200000112732180/?bltgh=lyoMWCofCzeEMT5ANBFBIg.1_28.39.ProductTitle\nhttps://www.bol.com/nl/p/mophie-power-boost-xl-powerbank-10-400-mah-zwart/9200000080445342/?bltgh=peqX4kc0h17VwWWy5CpMDw.r1tPn425oYeScLM8YiRMCg_0_45.49.ProductTitle\nhttps://www.bol.com/nl/p/breadboard-170-contactpunten-arduino-compatible-wit/9300000008153326/?bltgh=s6mMzyeveRG7KugrjO9GQQ.nsY9ZSFS6Cxau5nA-6tDJA_0_23.30.ProductImage\n\n\tVersion control\n\tSituation\n\tThe setup\n\tThe data to be gathered\n\tPlanning\n\n"
        }
      ]
    },
    "WindPowerDatacollectionflowchart": {
      "hand-ins": [
        {
          "text": "Version Control\n\nTable of Contents\n\n\n\nIntroduction\nAt first, the goal of this semester was to install a wind turbine on location of Strijp-TQ. Due to Covid-19 we are unable to physically the wind turbine as expected. The goal of this wind turbine was to gather data on how much energy is being generated relative to the current wind status. Because of this we had to look for alternative ways to simulate a wind turbine and still gather the required data. \n\nAfter spending some time looking for alternatives, we decided to go for a \u2018diy turbine\u2019. This turbine consists of an Anemometer and an ESP32 WiFi to gather the required data. As we cannot install this on Strijp TQ, we decided to order the items to our homes and set them up there. As I am currently not in Eindhoven, it would be a feasible plan to install 2 unique wind turbine locations to gather more data. This would not have been possible were I to stay in Eindhoven, as that would just result in duplicate data entries. \n\n\nDiagram\nThis diagram shows how the data will be collected from the ESP32. Once the script is loaded, a piece of code will be executed every n (defined at startup) minutes. This code measures the wind speed and converts it to a hypothetical output. This output would have been the actual output in Watts, were the specified wind turbine to be installed. \nAfter getting the current \u2018generated power\u2019, we call the OpenWeather API and get the real time weather data. This way we can create one entry containing the current weather and generated power. This data will then be sent to our API, where it will be merged into a bigger dataset. \nIn case of failure during the transmission of data, we store the variables in a temporary list, from which the call can be executed again at the next iteration. \nWhen the process is done, the script will wait for n minutes, and repeats the previously explained code. \n"
        }
      ]
    },
    "WindPowerDocumentationIntroductiontoproject": {
      "hand-ins": [
        {
          "text": "Version control\n\n\nIntroduction\nThe goal of the project was to create a reliable wind power prediction model based on historical weather and power data. It was initiated in partnership with a company named OpenRemote. They plan to create two new buildings in Strijp-S, Eindhoven including wind power solution. This means that turbines are going to be installed on the rooftops of these buildings in order to generate power for OpenRemote. If the power generated for a certain time in the future can be predicted based on the weather forecast, the power consumption of the building can be managed so that it relies more or less on the power generated directly from its wind turbines Therefore, it is important to create a precise machine learning model to predict the power generated from a wind turbine.\n\nCurrent situation\nThe first phase of the project was to create a Proof of Concept of the model. After an extensive research what data we need for the wind power prediction model and which algorithm to use to train it, we started building the Proof of Concept with a relatively small demo dataset with weather and power data that we found in Kaggle. After showing it to OpenRemote we concluded that we could start the second phase of the project and work with a real wind turbine.\n\nWind Turbine\nWe need to install a real wind turbine to create a \u201cdynamic\u201d prediction model based on the turbine specifications and real-time data. We have carefully researched and selected a small wind turbine that suits our needs for testing. After talking with OpenRemote about it they gave us green light to purchase the turbine (). It was meant to be installed on the rooftop of the Strijp-TQ building at the beginning of January 2021, but because of the current lockdown, this is not possible at the time of writing this document. However, the building manager of TQ gave us a green light to install it, so in the future, the turbine could be installed, and the testing with a real wind turbine could be started. \nThe data for the power generated from this model of wind turbine could be read with a small module from the company that manufactured it, TESUP. They also provide a cloud solution where all this data could be stored and consumed in the future. This data is an important part of the dataset that is needed to create a precise power prediction model.\nAnemometer\nIn order to gather real-time precise wind data to be used by the power prediction model, we decided that at least one anemometer should be installed close to the location of the Wind Turbine. The anemometer will be connected to an Arduino which will send the wind measurements to an API made to store the data in a database. This data could be used to train the model at first, but later also make real-time predictions based on it.\nAfter a few weeks of data gathering, we plan to have enough data to create a large enough dataset, the training of the model could start.\n\n\nTarget Approach\nThe library we have decided to work with is called Keras, an easy-to-use Machine Learning library, built on top of TensorFlow. It is a beginner friendly library with a lot of readily documentation and examples for implementation. This makes it a good tool to use in order to create a basic working example.\nKeras to Tribuo \nIn the future OpenRemote would like to switch to a Java-based machine learning library (Tribuo). When using java, they can port the program to their own OpenRemote service and use it as one of the \u2018smart sensors\u2019. For testing purposes however, we are still able to use Keras. \nA conversion to Tribuo would not have a big impact on the design, as both libraries support Linear Regression. However, the Tribuo library from Oracle is newer, and therefore has a smaller amount of documentation/community surrounding it. Hypothetically speaking this shouldn\u2019t be a problem, as the documentation is probably up-to-date enough to develop the library without a hassle. When running into any unexpected errors, it would be nice to have a community to speed up the development process.  \nSupervised learning\nThe target approach is to use a machine learning strategy called \u2018Linear Regression\u2019, which is a form of supervised learning. In this case, we train the prediction algorithm on a bunch of different inputs and also give it the output variable. By giving the algorithm the input and expected outcome, it can analyse its own predictions and adjust which parameters to prioritize (also called the weights of the network).  By giving it enough examples of these in- and outputs, the network can finetune the weights in such a way that it will also be able to start accurately predicting output values for input values on which it has not been trained. \nA prominent risk to take into account when programming a linear regression model is under- or overfitting the model. The first occurs when there is too little data, causing the model to create very \u2018broad\u2019 and \u2018unspecific\u2019 predictions. The latter means that the model is trained too often on the data, meaning that it will start to memorize the data, instead of predicting it. This causes the model to overperform on the given model, but broadly underperform on any other model. \n\n\nProof of Concepts\nIn the past, we have tried to approach the problem with a handful of other methods. The findings on those are listed below:\nARIMA\nArima (Auto Regressive Integrated Moving Average) is a type of model that tries to \u2018predict\u2019 the future trend based on its own past values. As weather prediction also comes in time series, we thought that this could be easily translated to an ARIMA model. This went easier than expected but did not yield excellent results. Mostly because the data was not cleaned in a proper way, which made the time series on which the ARIMA model was applied have major inconsistencies. \nBesides this we also found out that it did not help us as you cannot use it to predict any new or \u2018dynamic\u2019 situations, as it is only applied to one dataset. The moment that any outside factor was to change, the model would not be able to pick this up and continue with outdated data. These are reasons why we decided to drop the ARIMA forecasting model and decided to switch to other approaches.\n\nLSTM\nLSTM (Long Short-Term Memory) is a type of Recurrent Neural Network mainly used in deep learning, time series forecasting and text analysis. This type of network is unlike most neural networks, as the previously predicted values can be stored, and used as an additional parameter in the next prediction.  This can be extremely useful in, for example, the generation of pieces of text. In this case, the network will remember the previously outputted word, and can this way make sure that the following word to be predicted matches. \nThis approach can be used in time series predictions as well but requires a different method of creating the X and Y datasets. To prepare the datasets for the training with LSTM, the Y values need to be set back a certain number of steps, called timesteps.  from Machine Learning Mastery covers the use of timesteps well.\nAfter discussing this technique with OpenRemote, they commented on the fact that this method would not be efficient, as it is only able to predict the future of a series. This means that if we have the weather prediction for the coming 5 days, it would only be able to continue predicting from there on out, meaning from day 5+. \n\n\nTraining\nAs we discussed above, we will be training the model with supervised linear regression. We will be using a collection of input variables to train the network. The input variables will be as follows:\nOpenWeather weather prediction \nTemperature\nHumidity\nPressure\nWind speed\nWind degrees\nWeather ID\nThe formula for approximating the wind power output\nP = \u03c0/2 * r\u00b2 * v\u00b3 * \u03c1 * \u03b7\nr = Radius of the blades (m)\nv = Wind speed m/s\n\u03c1 = air density (kg/m3)\n\u03b7 = efficiency factor (%)\nAlthough we will be training this network to predict the outcome for a time series (3-7 days), we are not going to give the neural network an array of n days of weather prediction and expect it to output a series for n days. Instead, we will let it train 1:1 with the OpenWeather prediction. This means that instead of predicting all n days at once in bulk, we are going to predict them one by one. This will result in several different outputs, one for each period for which we have OpenWeather data (probably 3 hours).  \n\nThe network will use these 7 unique values as input variables in the first Dense layer. After this first dense layer, the values will be put through 2 more Dense layers with each 21 nodes (this is still open for modification). We have settled on this as the initial combination, although it can still be modified to see whether more layers/nodes can yield a better result. The only thing to keep in mind, is that the last layer of the network needs only one node, as only one prediction will be made.  As for the loss function of this model, we use MSE (Mean Squared Error) as yields the best result in the case of Linear Regression. \n\n\n"
        }
      ]
    },
    "WindPowerProjectProposal": {
      "hand-ins": [
        {
          "text": "\n\n\nVersion control\n\n\nProject Assignment\nProblem to solve\nNo accurate prediction models for wind power generation without a subscription.\nThere are already several prediction models for this online, but these are hidden behind a subscription. \nThere are no Edge Gateways within OpenRemote at the moment.\nCheck the potential usage of Tribuo .\n\nProject goals\nSoftware\nA prediction model (preferably in Java) for wind power, based on outside factors like wind speed, direction and configuration. This model should be able to run on a RPI. \nAn easy to use dashboard which displays the wind power prediction \nInfrastructure \nThe ability to create and manage Edge Gateways from a central instance.\n\nProject scope\nWe will be researching the Tribuo library and any other potential libraries as a machine-learning candidate.  \nResearch and create a dataset for the wind power prediction model.\nCreating a dashboard on which the power prediction can be viewed.\nDeveloping a prediction model that will be able to run on an RPI3.\nCreating a solution for deploying edge gateways.\n\nResearch Questions & Sub questions\nHow to predict the generation of wind power?\nWhat type of prediction model would be beneficial?\nWhat ML library would be optimal for predicting wind power generation? (QoL, efficiency, community etc.)\nHow to deploy the OpenRemote software on edge gateways in a manageable fashion.\nHow to deploy?\nHow can it be kept manageable?\n\n\n\nProject Deliverables\nInfrastructure\nA solution for orchestrating deployments from OpenRemote software on edge gateways.\nDocumentation\nConfigurations\nUser install guide\nSoftware\nA prediction model for approximating wind power generation.\nA software solution which has the potential to display the predicted power yield on an OpenRemote dashboard. \nDocumentation\n\n\n\nTeam\nThe team consist of the following members:\nNiek van Dam\nDaan de Weirdt\nElmira Drost \nDaniel Vaswani\nMartin Markov\nInside the team there are 2 subgroups, one is based on the Software side and the other is based on the Infrastructure side.\nSoftware\nThe group of software will be focusing on the Software side of the project. They will be focusing on the prediction of the wind power. The group of Software consist of the following members:\nNiek van Dam\nDaniel Vaswani\nMartin Markov\nThere is 1 group leader in the Software group who is also responsible for the communication between the stakeholders. The group leader is: Niek van Dam.\nThey will be solving the following sub questions for the project:\nHow to predict the generation of wind power?\nWhat type of prediction model would be beneficial?\nWhat ML library would be optimal for predicting wind power generation? (efficiency, community etc.)\nInfrastructure\nThe group of Infrastructure will be focusing on the Infrastructure side of the project. They will be focusing on deploying software on edge gateways. The group of Infrastructure consist of the following members:\nDaan de Weirdt\nElmira Drost\nThere is 1 group leader in the Infrastructure group who is also responsible for the communication between the stakeholders. The group leader is: Daan de Weirdt.\nThey will be solving the following sub questions for the project:\nHow to deploy the OpenRemote software on edge gateways in a manageable fashion.\nHow to deploy?\nHow can it be kept manageable?\n\n \n\nActivities and Planning\nDuring this project, several activities will be undertaken to ensure success. While a more detailed planning will be available in this projects Microsoft Teams environment, a rough timeline will be sketched here. \n\n\nRisks\nIn the following section are listed probable underlying problems (and some probable solutions) that may arise throughout the project.\nGeneral\nCovid-19\nDue to these uncertain times, there is a chance that physical meetings and team meetings can be delayed/cancelled. This would delay the progression of the project. \nSoftware\nSelecting suitable technologies\nWhen choosing technologies, the software development team should be aware of the underlying risk of getting stuck with a technical problem that is not listed in the documentation of the used technology, or is not yet discovered as a bug/implemented feature from its maintainers. In case of a similar problem the development team should choose technologies that are widely used and have large enough community which would eventually be able to help\nWhile it could be tempting to use cutting-edge technology, the development team should take into consideration the risk of using It as the main one for the project.\nSelecting source of data\nWhen selecting data source, the development team should take into consideration the probability of getting misleading data from an external source, no matter how well the service is advertised\nEven though it may seem preferable to set up our sensors for gathering wind data from the buildings, there is always a chance of not setting them up correctly or using not sensitive enough devices\nBudgeting\nIn the case of any purchasing needs, we should check the costs and discuss them with OpenRemote\nWork organization\nThroughout the work process, any conflicts, blockers or miscommunication between team members can arise. Problems like this should be resolved in an \u201cagile\u201c manner by discussing them with the team.\n\nInfrastructure\nDelivery time\nWhen purchasing the Raspberry PI\u2019s online, there is a chance that online delivery will get delayed. This would delay the testing and implementation of OpenBalena. \n\n"
        }
      ]
    },
    "WindPowerProofofConcepts": {
      "hand-ins": [
      ]
    },
    "WindPowerResearchWhatlibrarytouse": {
      "hand-ins": [
        {
          "text": "Version history\n\n\n\n\nReason for this research\nTo get started on our project for predicting wind energy, we need to know what the potential options in front of us are so that we can make a well-considered decision.  We will look at what the best frameworks are which can be applied to our project, as well as potential prediction model types. \nBesides prediction models this research document will also go into detail about what method of data gathering to use. There are a handful of options to gather data for a dataset, but not all are viable nor cost effective. \n\nThis document will start with a list of all the research questions that we want to answer in this\ndocument. After this we will look through viable and possible answers to the research questions, and discuss \n\nThe target audience for this research is software engineers.  \nResearch questions\nBefore the start of the research, we have to define several research questions which we are going to answer while doing said research. The main questions are written down below, with each containing a subset of child questions. \n\nMain Questions\nWhich ML library has the most potential to deliver good results\nCan it run on an RPI?\nDoes it support continual learning?\nIs there a community behind the library?\nHow difficult is it to implement?\nWhat is the best way to test the results against real life data\nDIY Wind turbine\nWindstats\nOther API\n\n\nWhich ML library to use?\n\nTensorFlow\nTensorFlow is Google\u2019s library for dataflow programming, written in Python. It has a comprehensive set of tools to its disposal, both high and low-level. Keras is meant for getting high performance on big datasets and is used for things like object detection. \nTensorFlow has a large community, making it an accessible library to use for developing the prediction model. Furthermore, it is a well-documented and maintained library, which would really improve the speed at which we can develop the model. This library has full support for RPI and even has tutorials for setting it up on a Raspberry Pi. \nSince this is a high- and low-level library, I think the implementation of this library should be doable. From what I have read the implementation seems comprehensible (enough) to actually get started with it. \nTribuo\nTribuo is Oracle\u2019s latest machine learning library, with support from java 8+ until the latest versions. It is purposely written in an enterprise language like Java instead of traditional languages like Python or R. This way you do not need to handle two languages at the same time for simple ML operations, and instead keep everything tidy in the same package.  Tribuo has one unique feature which makes it an interesting option, which is that it can remember exactly what its inputs were, and it is also able to describe the range and type of each input. This makes Tribuo ideal for backtesting results, since you can see the exact inputs which resulted in the output.   \nThis library is quite new, as it has been changed to Open-Source only a month ago (15 September 2020). This also means that the community for Tribuo is still developing and is as of today, quite small. A small community is not necessarily the end of the world, but it would be good to have a community to ask things to when we get stuck. \nTribuo can run on a RPI, but it does not have any known optimizations for it, whereas TensorFlow does have some.  Besides that, Tribuo is a high-level library, this means that you can very concretely communicate with the library and not have to deal with any complicated ML issues. Also, Tribuo does not have (for as far as I could find) any concrete support for Continuous learning, which is necessary.  \nKeras\nThis library is built on top of Tensorflow 2.0 and is written to enable fast experimentation with deep neural networks. This is an interesting library, since it is of such high-level that it can run on top of TensorFlow, but also libraries like CNTK and Theano. It scores high grades in ease to use and the simplicity it brings with it. \nKeras has a big community and is surprisingly enough more popular than the framework that it was written on top of, being TensorFlow. Besides that, it also has the ability to integrate continuous learning and run on a RPI. \nPyTorch\nPyTorch is an open source machine learning library for Python, based on Torch. It was developed by Facebook\u2019s AI research group and is used for applications like natural language processing. This library is however of lower-level and focusses on direct work with array expressions. This library has gained loads of interest over the last few years, especially in the academic sector. \nBesides being a hard to approach library PyTorch has a lot of potential. I do not think however that it is suitable for our project, since this means that we have to learn a lot of theory and apply this before we can actually get started with the project.  \n\n\nHow to test against real-life data?\nTo create a precise prediction model, we need to test our algorithm against real data. This could be done in two ways: by using external services or collecting our own data from a wind turbine that would be purchased. Both have their pros and cons.\nOwn wind turbine\nIn a meeting with the stakeholders of this project, it was suggested to find a small wind turbine that can be easily installed in a suitable location. The turbine must be able to collect statistics about the generated power in small enough periods of time. This requirement is crucial since the data collected will be used in training the prediction model. While this solution would be independent by any other external services it hides a lot of questions that our team could not be able to solve throughout the duration of the project like choosing a suitable location for the turbine, the proper selection and installation of it, as well as maintaining it. \nExternal services\nBy choosing to use external services for our data we ensure that we would have enough reliable historical data to train the prediction model from day 1 of the development process. This would speed up the process of training.\n\nWindstats.nl\nThis service provides live and historical data for wind farms across The Netherlands. It is the only one we found providing such information. We have reached out to windstats.nl to request sample data from one of their windmills. They sent us the format of the data that they provide. The information we need from them is how much power a windmill is generating at given timeframe and at what location. They provide this data. Their subscription costs 995 euro per year but offer a discount of 25% for educational institutions.\n\nOpenWeatherMap API\nopenweathermap.org provides historical weather information for any location. There is an option to get bulk historical data for up to 40 years. From this source we can gather the wind data needed to compare against the power generated data at the same timeframe.\n\n\n\nConclusion\nWe are going to try and create a working project with Keras, as this library comes out on the top in user friendliness and usability. There is a lot of documentation on Keras readily available, which will help the process. \n\nIn order to test it against real life data we will be purchasing our own wind turbine, which will store the energy output every x minutes. This can go hand-in-hand with windstats.nl, where we will gather the other data necessary to predict the wind speed. "
        }
      ]
    },
    "WindPowerSprintdeliveries": {
      "hand-ins": [
        {
          "text": "Reflection\nDelivering a new iteration to OpenRemote has always been a good experience so far. The people at OpenRemote have a really open mindset, and aren\u2019t afraid to give us their own input, which gives a really good vibe during the delivery. \nDuring almost all of the deliveries, I am the one who has the word most of the time. This makes sense for the most part, as I am the \u2018project lead\u2019, so this is expected from me. In the beginning I was really opposed to the idea of continuously presenting to OpenRemote, this however, changed rapidly. Right now we are at the 5th sprint delivery, and I can comfortably say that my presenting skill have increased.  \nAlthough I am the one communicating with OpenRemote, I\u2019ll still try to get the others to give in some input as well, to make things less \u2018one sided\u2019. This usually consists of asking my teammates for their opinion, or asking if there have been any points that I have missed during the presentation. Usually there is nothing left to add, but I still think it\u2019s important to give everyone a chance at presenting if they want to.\nIn terms of the \u2018content\u2019 and \u2018actual product\u2019 that we are delivering to OpenRemote, I am usually not 100% satisfied with the result we are delivering. I think this is because we chose to work in springs of two weeks, and I simply do not have enough time to create a good functioning product in that timespan considering I have more projects to work on. Luckily, we also have Infrastructure, who are working on a different \u2018sub-project\u2019. This means that when the Software project doesn\u2019t have a lot of impressive deliverables, we always have infrastructure, and vice-versa. "
        },
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWind Energy Sprint Delivery \n\n\n\n\nWind Energy\nSprint Delivery \nDaan de Weirdt \nNiek  van Dam\nDaniel Vaswani\nMartin markov\nElmira Drost \n\n\n\nIndex\nGoal of the sprint\nPOC \nNext sprint\n\n\n\nGoal of the sprint\nResearch data sources\nPoC Software\nPoC Infrastructure\n\n\n\nResearch Data Sources\nWindmill Power Generated Data\nWindstats.nl offers historical data for power generated for each windmill in the windfarms in The Netherlands\nWind Historical Weather Data\nOpenWeatherMap API offers bulk historical weather data for any location for up to 40 years back\n\n\n\n\nPoC Software\nLSTM RNN\nDesign \nTurbine dataset\nNaN values\n\n\n\n\n\nPoC Infrastructure\nBalenaCloud\nBalenaOS\nBalenaOS Raspberry Pi\n\n\n\n\nNext Sprint \nSoftware: \u00a0\tRebase/optimize prediction model on the previously created PoC\nInfrastructure :\tTesting on Raspberry PI with OpenRemote Software \nMisc: \nSolving NaN values with specialist from Fontys\nObtain or Convert docker file to a lower version\n\n\n\nQuestions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n.MsftOfcThm_Accent1_Fill {\n fill:#BE977B; \n}\n.MsftOfcThm_Accent1_Stroke {\n stroke:#BE977B; \n}\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/docProps/thumbnail.jpeg\n\n"
        },
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWind Energy Sprint Delivery \n\n\n\n\nWind Energy\nSprint Delivery \nDaan de Weirdt \nNiek  van Dam\nDaniel Vaswani\nMartin markov\nElmira Drost \n\n\n\nIndex\nGoal of the sprint\nResearch data sources\nPOC's\u00a0\nDemo\nNext sprint\n\n\n\nGoal of the sprint\nData sources\nPoC Software\nPoC Infrastructure\n\n\n\nResearch Data Sources\n\nWindstats gives turbine specs\nNo real-time turbine data\u00a0\n\n\n\n\n\n\n\nResearch Data Sources\nOption 1:\nResearch new datasets\n+ More relevant data\u00a0\n+ Larger dataset\n- Potentially\u00a0slow development\n- Possibly costly\nOption 2:\nCreate a wind turbine/weather station\n+ More accurate data\n+ No subscription\n- Retrieving data\n- Time consuming\n\n\n\n\nResearch Data Sources\nOption 3:\nHybrid option (openweather + our own turbine)\n+ Most accurate data\u00a0\n- Time consuming\n\n\n\n\nPoC Software\nLSTM RNN\nTurbine dataset\n\n\n\n\n\n\nWind Turbine for backtesting\nAutoRegressive Integrated Moving Average\nPopular time series forecasting\n\n\n\n\n\nPoC Infrastructure\nOpenBalena vs BalenaCloud\nOpenBalena\nDemo\n\n\n\n\n\nDemo\n\n\n\nSoftware\n\n\nInfrastructure\n\n\n\nNext Sprint \nS: \u00a0Create final prediction model\nI:\u00a0\u00a0 Linking more RPI\u2019s with OpenBalena\nMisc: \n\n\n\nQuestions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n.MsftOfcThm_Accent1_Fill {\n fill:#BE977B; \n}\n.MsftOfcThm_Accent1_Stroke {\n stroke:#BE977B; \n}\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/docProps/thumbnail.jpeg\n\n"
        },
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWind Energy Sprint Delivery \n\n\n\n\nWind Energy\nSprint Delivery \nDaan de Weirdt \nNiek  van Dam\nDaniel Vaswani\nMartin markov\nElmira Drost \n\n\n\n1\n\n\nIndex\nGoal of the sprint\nWind turbine\nCloud formation\nNext sprint\n\n\n\nGoal of the sprint\nWind Turbine setup\nCloudformation script\n\n\n\nWind turbine\nDocument Building Management\nPermit\n\n\n\n\n\nCloud formation\nCloud-formation\u00a0succesfull\nInstall-script for openbalena not fully functional.\n\n\n\n\n\n\nNext Sprint \nS: Backtesting model on the real-time data  & installation of wind turbine\nI: Deployment of OpenBalena using cloudformation and research OpenBalena-api.\nMisc: \n\n\n\nQuestions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n.MsftOfcThm_Accent1_Fill {\n fill:#BE977B; \n}\n.MsftOfcThm_Accent1_Stroke {\n stroke:#BE977B; \n}\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/docProps/thumbnail.jpeg\n\n"
        }
      ]
    },
    "WindPowerTurbineinstallationspecifications": {
      "hand-ins": [
        {
          "text": "\n\nVersion control\n\n\n\n\nIntroduction\nThis document is meant to give insight into why we want to install a wind turbine on the TQ building. \nWe are a group of Fontys ICT Delta students working on a project for , where we have to predict Wind Turbine power output. For this we would like to install a wind turbine on the roof of the TQ building. \nWhy are we installing a turbine?\nWe are a group of Delta students working on a project for OpenRemote, where we have to predict the wind power which turbines give off. So far we created a proof of Concept with Artificial Intelligence, this is currently working an ideal wind dataset with a predefined wind turbine. This means that our algorithm will be able to predict the wind turbine power output in ideal situations with wind turbine x. \nAfter we have researched and found out that the Proof of Concept works as expected, we enter the second stage of the project. We need to create a \u2018dynamic\u2019 prediction model based on the proof of concept. In this case, dynamic means that the model must be able to predict the wind power output of different types of wind turbines, by just giving in a few specifications. In order to create this dynamic model we need to install our own wind turbine and train it on the real life data as a first step. \nWhere could it optimally be?\nOur intentions are to install the wind turbine on top of the roof of Strijp-TQ. Specifically, on top of the elevated part of the roof , located above the staircases. We chose this location because this is close to our base of operations, combined with the fact that this is high up, and eligible for harvesting a lot of wind data. \n \n\n\nPermit\nA permit for the installation of a wind turbine is not necessary, as the windmill is less than 4 meters. A permit is also not needed in case the turbine isn\u2019t connect to the powergrid.\nWhat is the impact\nIt will have an impact on the weight (circa 30kgs) of the building and it adds a little extra height (3,58 meter) on the building.\nWhat has to be taken into account?\nWe have to take the bearing force of the pole into account when installing this turbine. We calculated the maximum force and torque that a pole should withstand in order to stand on the roof while maintaining a safe environment. The highest force that our pole should be able to withstand is 24kg, which we calculated by taking the highest windspeed which Brabant has seen in the last 15 years (100 km/h).\nCalculations\nThe calculations are as follows:\nAverage air density = 1.225 kg/m3\nMass of the air (m) = air density (kg/m3)  * area (m2)= 1.225 kg/m3   *  0.25m2 (assuming the pole is 250cm and 10cm wide) = 0.306 kg\nAcceleration (a) = max wind speed(m/s)2  = 100 km/h =  27.7778 m/s2 = 661.6\nForce to withstand = m * a = 661.6 * 0.306 = 202.4 N = 20.639 KG\nWhat is the added value?\nThe added value for Strijp-TQ is that it can come in the media as a positive point. Strijp-TQ will have a good imago with green energy and a learning environment. \nSafety\nFor the safety we kept the following things in mind:\nVery high windspeeds till 100km/h\nSteady pole for the turbine\n\nWhat will be installed?\nParts installed\nOn the roof of Strijp-TQ there will be the following items installed:\nA turbine\nCharge controller\n2,5 meter pole of steel\nBattery for the energy\nArduino \nPot with soil\nTurbine\nSpecifications of the turbine\n\n\nConclusion\nA turbine on the roof for a period of a few months.\nQuestions? \nIf you have any questions contact Eric Slaats for the needed information."
        }
      ]
    },
    "WindPowerWindPredictionFlowchart": {
      "hand-ins": [
        {
          "text": "Version control\n\n\nTable of Contents\n\n\nIntroduction\nWhen training a neural network, a lot of specialized terminology is usually incorporated into the script, which can make it difficult for outsiders to read and comprehend the code. Therefore this sequence diagram below can be used to explain the code more clearly, without having to know what each method concretely does. \n\n\nDiagram\nThe flowchart below describes the script which is used to train the Neural Network for predicting wind power, a project for OpenRemote. At the moment the script converts a big (100 000+ rows) csv dataset into a pandas dataframe. Pandas has a broad array of tools for handling big datasets, which makes it ideal in this situation. After importing, we clean up all rows with invalid data, as to not feed any invalid data to the neural network when training. Before creating the test and train split dataset, we have to normalize all of the data. We are forced to normalize the data between 0-1 to prevent exploding/vanishing gradients, a phenomena which occurs when data is recursively multiplied by the network and exponentially goes up or down. Best case scenario this causes an OverflowError, worst case you don\u2019t notice it until the network is put to practice. \nOnce normalization is done, we split the data into the test and train datasets, and fit the datasets on the model. Once the fitting/training is done, we plot the historical test results and are able to validate whether or not the model is trained in the best way\n\n"
        }
      ]
    }
  },
  "18233": {
    "AddingimportLenexfilefunctionality": {
      "hand-ins": [
        null
      ]
    },
    "AdviseonMessageBrokerSystem": {
      "hand-ins": [
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\nAdvise Message Brokers \nFor a notification system implementation for the DeX platform \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nMax van Hattum, Martin Markov \n\n5-10-2020 \n\n  \n\n \n\n\n\nTable of Contents \nVersion History 1 \n\nIntroduction 2 \n\nResearch questions 2 \n\nMain question 2 \n\nSub questions 2 \n\nDOT Framework 3 \n\nWhat 3 \n\nHow 3 \n\nWhy 3 \n\nWhat does the DeX platform want to achieve? 3 \n\nWhat is a message broker? 3 \n\nWhy use a message broker? 4 \n\nWhat are important aspects of a message broker service? 4 \n\nWhat are currently available message broker services? 4 \n\nComparing the important aspects of these services 5 \n\nVerifying requirements with a prototype 5 \n\nRabbitMQ 6 \n\nWorker 6 \n\nTask 8 \n\nResult 9 \n\nApache Kafka 11 \n\ndocker-compose.yml 11 \n\nProducer 12 \n\nConsumer 13 \n\nResult 13 \n\nFinal comparison 14 \n\nConclusion 14 \n\nReferences 15 \n\n \n\n \n\n \n\n\n\nVersion History \n\n \n\n \n\n \n\n1 \n \n\nVersion Date Author Changes \n\nV1 5-10-2020 Max van Hattum Setup document, introduction, research \nquestions, what is a message broker, \naspects and start of currently available \nservices. \n\nV2 11-10-2020 Martin Markov Answered to the question why we need a \nmessage broker and added comparison \ntable of the different options plus \nconclusion \n\nV3 13-10-2020 Max van Hattum Added Validating requirements with a \nprototype  \n\nv4 18-11-2020 Max van Hattum Added DOT framework section and sub \nquestion regarding DeX specifically. \n\nv5 20-11-2020 Martin Markov Added PoC with Apache Kafka and \ncompared it to RabbitMQ plus small other \nimprovements \n\n\n\nIntroduction \nFor the DeX platform it will be necessary to send users notifications by means of different message \n\nservices. For this to be released one component is needed to manage notifications intended for \ndifferent subscribers. This component should not only store the data to be sent but should also be \n\nresponsible for load balancing and making sure messages are delivered.  \n\nMany existing services implement a so-called message broker service to handle this. (Google, z.d.) \n\n \n\nResearch questions \nMain question \nTo give direction to this research a main question to be answered will be formulated. There are \n\nseveral aspects that need to be kept in mind when researching what the preferred solution for the \nDeX platform will be. The solution should be secure, scalable, functional, and compatible with the \ncurrent system. Keeping these criteria in mind results in the following research question: \u200b\u201cWhat is a \nsecure, scalable and compatible message broker service best suited for the DeX platform?\u201d \n\nSub questions \nTo get to the answer to this question, the subject will be divided in multiple smaller subjects. The \n\nfollowing questions will be answered: \n\n- What does the DeX platform want to achieve? \n- What is a message broker? \n\n- Why use a message broker service? \n\n- What are important aspects of a message broker service? \n- What are currently available message broker services? \n\n- How do currently available message broker services compare? \n\n  \n\n2 \n \n\n\n\nDOT Framework \nWhat \nThis research is going to take place in both the application domain and the available work domain. \nThe current software system of the DeX Platform needs to be analysed to get the best service fit for \n\nthe context. However, before this can be done research needs to be done about message broker \n(services) in general so that a better understanding can be achieved, allowing us to find the best fit \n\nfor the current context.  \n\n \n\nHow \nTo get an overview of what exactly we are trying to achieve we first applied the \u200bField\u200b method in a \ntalk with one of the stakeholders and project leader Brend Smids and another team member Niray \nMak. Then we focused heavily on the \u200bLibrary \u200bmethod to gather existing information and expertise \nregarding message brokers. Finally, we compared the gathered information, and setup prototypes for \n\nthe most promising options which belongs to the \u200bShowroom\u200b research. \n\n \n\nWhy \nThese methods were chosen because we want to attain a good overview of what is needed for the \nDeX Platform and how existing work might already solve the issue. When this is clear we can focus on \n\nachieving the best fit for the context, leaving room for potential improvements. \n\n \n  \n\n3 \n \n\n\n\nWhat does the DeX platform want to achieve? \nTo get a grip on what the DeX Platform wants, a meeting was scheduled with Brend Smits, Niray Mak, \n\nMartin Markov and Max van Hattum. There Brend explained that currently there is no way to send \nnotifications to users. He wanted a scalable solution for this keeping in mind that maybe in the future \n\nmultiple ways of sending notifications are going to be used. \n\nHe went on to describe how currently the architecture of the REST API is a monolithic structure, but \n\nhow they might want to gradually change to a more microservice oriented architecture. He wants a \nsolution where a service in the API can register notifications to a message broker, and then let the \n\nmessage broker handle things from that point, distributing it to a specified notification service.  \n\nMoreover, he stated that he would prefer it if this was all locally hosted, using Docker and that the \n\nsolution should be simple to use.  \n\n \n\nWhat is a message broker? \nA message broker is sometimes also called Integration Broker or interface engine. It is a service that \nminimally message transformation and routing services, communication program to program. \n\n(Gartner, z.d.) \n\nMost often it is able to store messages, keep track of which messages need to be delivered and \n\nbalance the load of delivering messages. \n\n  \n\nWhy use a message broker? \nMessage brokers make the process of data exchange simple and reliable. They use different \n\nprotocols that show how the message should be transmitted, processed and consumed. They allow \nasynchronous communication which allows both the producer and the consumer to interact directly \nwith the message broker and not between each other. While a producer can enqueue new messages \n\na consumer may read from it simultaneously without blocking it. Message brokers also allow us to \n\nbetter scale the communication between different services on demand. \n\n \n\nIn the context of DeX, we are planning to use a message broker so we can process notifications to our \nusers asynchronously. The producer of the eventual message broker will be our API which will \n\nenqueue the notification message, how it should be sent (ex. via email), to which user and when \n\nshould it be sent. The consumer will be the service responsible for sending the message. \n\n \n\nWhat are important aspects of a message broker service? \nForemost the message broker service should be able to validate, transform and route messages. The \n\nmain goal of the message broker architecture is decoupling programs while facilitating \n\ncommunication between these programs, while keeping them unaware of each other\u200b (Ejsmont, \n2015, pp. 275\u2013276).  \n\nMoreover cost, scalability, compatibility, hosting, ease of use and speed are important subjects to \n\nresearch. \n\n4 \n \n\n\n\n \n\nWhat are currently available message broker services? \nAWS SQS \n\n- 1 million requests free \n- Hosted by Amazon \n- Compatible with .NET \n\n- Good documentation \nhttps://docs.aws.amazon.com/sdk-for-net/v3/developer-guide/sqs-apis-intro.html \n\n- Official Library available \u200bhttps://aws.amazon.com/sqs/?did=ft_card&trk=ft_card \n\nGoogle Cloud Pub/Sub \n\n- 10GB gratis, 40$ per 1TiB after \n\n- Hosted by google \n- Compatible with .NET \n- Great documentation: \u200bhttps://cloud.google.com/pubsub/docs/apis \n- Official Library available \n\nRabbitMQ \n\n- Own hosting, hosting specific pricing \n- Good documentation: \u200bhttps://www.rabbitmq.com/tutorials/tutorial-one-dotnet.html \n- Official Library available for .NET \n\n- Open source \n- Big community \n \n\nPulsar 2.0 \n\n- Own hosting, hosting specific pricing \n\n- Great documentation: \u200bhttps://pulsar.apache.org/docs/en/pulsar-2.0/ \n- Ported library available for ASP.NET \n- Open source \n\n \nApache Kafka \n\n- Own hosting, hosting specific pricing \n\n- Good documentation: \u200bhttps://kafka.apache.org/documentation/ \n- Community library available for .NET \n- Open source \n\n \n\n  \n\n5 \n \n\nhttps://docs.aws.amazon.com/sdk-for-net/v3/developer-guide/sqs-apis-intro.html\nhttps://aws.amazon.com/sqs/?did=ft_card&trk=ft_card\nhttps://cloud.google.com/pubsub/docs/apis\nhttps://www.rabbitmq.com/tutorials/tutorial-one-dotnet.html\nhttps://pulsar.apache.org/docs/en/pulsar-2.0/\nhttps://kafka.apache.org/documentation/\n\n\nComparing the important aspects of these services \nTo compare the services, we take into account several aspects; ease of use, scalability, hosting, cost, \n\nsupport and if the service is open source. The project leader communicated that own hosting and \n\nease of use are the most important aspects. Cost certainly plays an important role too.  \n\n \n\n \n\n  \n\n6 \n \n\n Ease of use Scalability Hosting Cost Open Source Support \n\n \nAWS SQS \n\n \nGood \n\n \n\n \nGreat \n\n \nAmazon \n\n$0.50 per \nmillion \n\nrequests \n\n \nNo \n\n \nAverage \n\nGoogle \nCloud \n\nPub/Sub \n\n \nGreat \n\n \n\n \nGreat \n\n \n\n \nGoogle \n\n \n$40 per TB \n\n \nNo \n\n \nGood \n\n \nRabbitMQ \n\n \nGreat \n\n \n\n \nGreat \n\nOwn / \nhosting \noptions \n\n \nFree \n\n \nYes \n\n \nGreat \n\n \nPulsar 2.0 \n\n \n\n \nGreat \n\n \nGood \n\n \nOwn \n\n \nFree \n\n \nYes \n\n \nGood \n\nApache \nKafka \n\n \nGood \n\n \n\n \nGreat \n\nOwn / \nhosting \noptions \n\n \nFree \n\n \nYes \n\n \nGood \n\n\n\nVerifying requirements with a prototype \nTo verify that the requirements are being met and to analyse the resource usage by the service, we \n\nset up a local demo.  \n\nRabbitMQ \nThe system requirements are not high, a minimum of 256mb of RAM always needs to be free and at \n\nleast 50MB of disk space must always be available to prevent failures. \u200b(RabbitMQ, z.d.) \n\nInstallation for a demo is simple when using docker: \n\ndocker run -it --rm --name rabbitmq -p 5672:5672 -p 15672:15672 rabbitmq:3-management \n\nThis includes a monitoring tool (not fit for production, but other tools are available for this). \n\nRunning this exposes the rabbitmq service on the default port and gives access to the \n\nmonitor tool with credentials: \u200bguest - guest. \n\nTo validate that the service fulfils the needs of the system and to monitor system usage it is \n\nnecessary to set up a publisher and one or more clients. The publisher will register multiple \n\nmessages, the clients will consume them. \n\nSince the clients will be doing a task based on a message, we have incorporated the \n\ncompeting consumer pattern. A consumer will only get a new task when it is finished with \n\nanother. \n\nTwo windows console apps are created, one for publishing one thousand messages and one \n\nfor receiving.  \n\n7 \n \n\n\n\nConsumer \n\n \n\n \n\n  \n\n8 \n \n\n\n\nProducer \n\n \n\n \n\n  \n\n9 \n \n\n\n\nResult \n\nStart up two workers by entering \u200bdotnet run\u200b  \u200binto a console opened in the folder where the worker \napp resides. Then start up one publisher with a message with \u200bdotnet run Message:..  \n\nThe number of dots represent the amount of seconds this task takes to run. \n\nThe broker now adds one thousand messages to the queue and sends them one by one to connected \n\nconsumers. It only sends a new message after it has received acknowledgment from the worker that \n\nthe task is completed.  \n\n \n\nAs you can see the messages are distributed between the workers. When checking out the \n\nmonitoring tool for ram and disk space usages, the values stay low.  \n\n \n\nBefore registering messages \n\n \n\n10 \n \n\n\n\n \n\nAfter registering messages with 2 seconds delay per task \n\n \n\n \n\nAfter subscribing six more workers \n\n  \n\n11 \n \n\n\n\nApache Kafka \nWhile there are not official system requirements on the official documentation, Confluent, the most \n\nfamous platform to use Kafka with, is recommending a minimum of 6GB RAM.  \n\nTo run Kafka locally we have to run Apache ZooKeeper alongside with it for maintaining the \nconfiguration information. To ease the process of local configuration and ensure that the correct \nservices are used we are going to use Docker and docker-compose. We have set-up the following  \n\ndocker-compose.yml \n\u00a0\n\n \n\u00a0\nThen to run it we are going to execute \u200bdocker-compose up\u200b . \n \nFrom this image it can be seen that the configuration of the Kafka is not as straight forward as the \none for running RabbitMQ. Also, there is not a monitoring tool coming out-of-the-box as the one that \nRabbitMQ provides, so the performance is not easily measurable. \n \nTo produce the same example as with RabbitMQ we are going to implement a pub-sub \ncommunication. \n\n12 \n \n\n\n\nProducer \n\n \n\n \n\n13 \n \n\n\n\nConsumer \n\n \n\n \n\nResult \n\n \n \n\n14 \n \n\n\n\nIn the result we can see that we launched one producer with two consumer instances which read \nfrom the same Kafka topic. \n\nFinal comparison \nWhile both tools could be used as a message broker, it appears that RabbitMQ is the better solution \n\nfor DeX as it is only a message broker and it is doing its job really efficient. Kafka is a great platform, \nbut it adds plenty of overhead when used only as a message broker which is the use case of DeX. \n\nPlus, the system requirements are much higher compared to RabbitMQ. \n\n \n\nConclusion \nWe conclude that based on the comparison of the different technologies and the research, the \nselected technology to use for the message broker of the DeX notification system is \u200bRabbitMQ\u200b. We \nadvise this service because it supports its own hosting, has good integration for .NET Core and has an \n\nactive community surrounding the service.  \n\n15 \n \n\n\n\nReferences \n\nEjsmont, A. (2015). \u200bWeb Scalability for Startup Engineers\u200b. McGraw-Hill Education. \n\nGartner. (z.d.). \u200bDefinition of IB (Integration Broker) - Gartner Information Technology \n\nGlossary\u200b. Geraadpleegd 5 oktober 2020, van \n\nhttps://www.gartner.com/en/information-technology/glossary/ib-integration\n\n-broker \n\nGoogle. (z.d.). \u200bCloud Pub/Sub |\u200b. Google Cloud. Geraadpleegd 5 oktober 2020, van \n\nhttps://cloud.google.com/pubsub#customers \n\n \n\n16 \n \n\n\n"
        }
      ]
    },
    "AnalyseandDesignnotificationsystem": {
      "hand-ins": [
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n1 \n\nC4 Notification System \nDeX Project  \n\nMartin Markov \u2013 Max van Hattum \n\n  \n\n\n\n \n2 \n\nInhoudsopgave \nContext Diagram ...................................................................................................................................... 2 \n\nContainer Diagram .................................................................................................................................. 3 \n\nComponent Diagram ............................................................................................................................... 4 \n\nCode Diagram .......................................................................................................................................... 5 \n\n \n\n \n\nContext Diagram \nA message broker is used to decouple the email service from the web application. The email service \n\nthen handles sending the notification to the email distributor which in turn sends the notification as \n\nan email. \n\n \n\n  \n\n\n\n \n3 \n\nContainer Diagram \nThe DeX Backend will be responsible for registering notifications to be send. Then the message \n\nbroker passes this to subscribed email services. \n\n \n\n  \n\n\n\n \n4 \n\nComponent Diagram \nThere is a NotificationService available to register messages to the RabbitMQ message broker. Within \n\nthe Notification Service there is a SubscribeService responsible for making a connection to the \n\nRabbitMQ message broker, and a ListenerService for listening to new messages and triggering the \n\nEmailService. \n\n \n\n \n\n\n\n \n5 \n\nCode Diagram \nIn the notification service the following classes are most important for the functionality. \n\n \n\n\n\n \n6 \n\nFor registering a notification the following class can be imported and used. \n\n \n\n\n"
        }
      ]
    },
    "AnalyseenAdviesInfrastructuur": {
      "hand-ins": [
        {
          "text": "\nDeX\nArchitecture \nAnalysis and Advice\n\nDelta - Max van Hattum\n06.04.2021\n\n\n\nVersion History\n\nDocument Dependency\n\n\n\n\nTable of Content\n\n\n\n1. Introduction\nDeX is a platform that wants to enable employees and students of learning institutions to share their products, project ideas or even research papers and thesis\u2019s. \nCurrently there are several groups working on realizing and adding features to this platform. There are two project leaders guiding the project, Niray Mak and Ruben Fricke, they are responsible for maintaining contact with stakeholders, management, and deliveries. They also lead the software and UX teams within Delta. Moreover, an associate degree team and a cybersecurity team work on DeX. \nAt the time of writing this document, DeX is more than a year old. At the beginning multiple decisions have been made about the infrastructure and software structures/patterns. Now DeX has grown immensely and new features have been created, moreover stakeholders and criteria may have changed. \nTherefor an analysis of these structures and patterns is necessary to identify if these are still the best options for DeX. Based on this analysis advice should be given about steps to undertake that could improve DeX based on defined criteria.\n\n\n\n2. Research questions\nTo give guidance to this research, a main research question will be formulated. This document should ultimately give a concise and accurate answer to this question. However, since there are multiple facets that need to be looked at, sub-questions will also be defined. These must first be answered before an accurate answer to the main question can be given.\n2.1 Main research question\nThe main goal of this research will be to give advice on where the DeX project can improve its infrastructure and software, focusing on the architectonical aspects. This can be concisely formulated in the following way:\nWhat are steps that can be taken to improve the infrastructure and software architecture?\n2.2 Sub questions\nThere are multiple facets that need to be identified and researched before an answer to the main question can be given. \nFirst and foremost, all the stakeholders involving the infrastructure and software architecture should be identified, defining their wants and needs. Based on these stakeholders\u2019 criteria can be discovered and given a score of priority per stakeholder. This results in the following sub-questions:\nWho are the stakeholders involved with the DeX infrastructure and software architecture?\nWhat are criteria that the previously identified stakeholders find important?\nBased on the discovered criteria an analysis of the current patterns and architecture can be done, where the criteria give weight to scoring the architecture that is currently in place. Before this scoring can be done, a general analysis should  be done, identifying the architecture, and supplying relevant available information about this architecture.\nWhat is the current structure and what are the dis- and advantages of this structure?\nHow does the current structure score, based on the criteria defined by the stakeholders?\n\n\nNext up research must be done on defining alternatives, describing what these alternatives are and how they could fit in the current context. They should also be scored with the same criteria as the current structure. \nWhat are alternative patterns or structures and how would these fit in the DeX context?\nHow do the previously identified structures score, based on the criteria defined by the stakeholders?\nLastly the highest scoring option should be explored more in-depth defining what must and will happen within DeX if the choice to switch to this option would be made. Not only the steps to achieve the switch, but also the impacts, positive and negative should be defined.\nWhat needs to be done to make a switch to the best scoring structure?\nWhat impact on DeX would this switch to the new structure have?\nThen, ultimately the main research question can be answered giving advice on what steps should be taken for improving the infrastructure and software architecture.\n\n3. Research Methods\nMultiple research methods are going to have to be used to get a complete and correct research. Therefor a breakdown of research methods necessary for answering the sub questions will be listed below, organized per sub question.\n3.1 Research methods sub questions one and two\nFor identifying the stakeholders and discovering what their priorities and criteria for the infrastructure and software architecture are, a mix of qualitative and quantitative data should be collected. Identifying all the stakeholders will result in primary qualitative data collected using interviews.\nWhen dealing with a larger volume of the same type of stakeholder, such as the software engineers, a questionnaire will be used to decide the common important criteria quickly and accurately. This will then result in primary quantitative data.\n\n\n3.2 Research methods sub questions three and four\nFor the sub question three a qualitative approach should be taken, since the primary objective of this question is to identify the current architecture and what common dis- and advantages are. It is not yet relevant to approach these dis- and advantages in a numerical way since it will not yet be a comparison of data.\nThere will however be a need of primary and secondary information. The primary facet would be defining the current structure in a descriptive way. The secondary facet will be the definition of both the dis- and advantages, a literature study should be conducted to accurately define these.\nThen, after the current architecture with its dis- and advantages has been identified, a more quantitative approach should be taken to score the architecture based on the earlier defined criteria. \n\n3.3 Research methods sub questions five and six\nYet again a qualitative approach will be taken for sub question five, identifying alternative structures and patterns, however now this will be secondary data obtained by doing literature research. \nNow for the sixth sub question quantitative data needs to be used, using the primary data collected while answering the first two sub questions. This must then be used for scoring the alternative patterns and architectures.\n\n3.4 Research methods sub questions seven and eight\nFor the last two questions experiments must be conducted, creating prototypes of the new pattern or architectures, and measuring what the effects are. The data resulting from these experiments could be both quantitative and qualitative.  It is possible to quantitatively measure performance by tracking for example CPU usage and response times. However, aspects such as ease of use and understandability is data that will be presented in a more descriptive, qualitative way. \nThese results need to be compared to the current system in place. This comparison will make it possible to identify what steps need to be taken to transform the current situation to the new preferred situation.\nConclusion\n\nDiscussion\n\nGlossary\n\n\n"
        }
      ]
    },
    "C3ModelRESTAPI": {
      "hand-ins": [
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPSV Swimming App C3 Model \nFontys Open Learning Semester 3 \n\nMax van Hattum\n\n\n\n \n2 \n\nInhoud \nIntroduction ............................................................................................................................................. 3 \n\nContext Diagram ...................................................................................................................................... 3 \n\nCurrent situation ................................................................................................................................. 3 \n\nPreferred ultimate situation ................................................................................................................ 4 \n\nContainer diagram ................................................................................................................................... 5 \n\nComponent Diagram ............................................................................................................................... 6 \n\nCode diagram .......................................................................................................................................... 8 \n\n \n\n  \n\n\n\n \n3 \n\nIntroduction \nFor Open Learning we get the opportunity to work for an external stakeholder, giving context to our \n\nlearning process. I\u2019ve chosen to work on an application for PSV Swimming. The main features this app \n\nneeds to offer are enabling swimmers to manage their event registrations, coaches to review this \n\nregistrations and administrators to see an overview of these registrations so they can process them. \n\nPart of this application is a REST API, which is responsible for handling and processing data. To \n\nprovide more insight in this software system, a C3 model is chosen to document the architecture. \n\n  \n\nContext Diagram \n\nCurrent situation \nThe existing application has three actors, with limited interaction. It also utilizes the SendGrid service \n\nfor sending emails.  \n\n \n\n \n\n  \n\n\n\n \n4 \n\nPreferred ultimate situation \nThe preferred ultimate situation would be that the administrator will only have to upload meets to \n\nthe PSV Swimming application, and that the application then also handles the synchronization of this \n\ndata with the European Swimming Federation. Moreover, they should be able to make notifications. \n\nThe coach should also be able to create and manage trainings, plus only being able to approve meet \n\nregistration of their own athletes. \n\nAthletes should now also be able to see notifications and manages registrations to trainings. \n\n \n\n \n\n  \n\n\n\n \n5 \n\nContainer diagram \nThe software system consists of an PWA made with Vue.js, a Web Server serving the PWA, a Rest API \n\nwhich handles data requests, and a MySQL database for persisting data. Moreover, SendGrid is uses \n\nto send emails. \n\n \n\n\n\n \n6 \n\nComponent Diagram \nThis diagram contains all endpoints of the API showing the data flow and the components used. \n\nIt is quite an extensive application, therefor the diagram is extensive as well. Services are utilized by \n\nendpoints; these services contain the business logic of the application. Repositories are used for \n\npersisting data to the database.  \n\n(See next page for the diagram, with landscape orientation to allow the diagram to utilize as much \n\nscreen space as is available, zoom in where necessary.)\n\n\n\n \n7 \n\n\n\n \n8 \n\nCode diagram \nIn my opinion a code diagram does not provide extra value or understanding into the project. If \n\nnecessary, this can be (and has been) generated by the IDE. However, this diagram is enormous and \n\nhard to comprehend. Consequently, it does not fit in this document. \n\n\n"
        }
      ]
    },
    "CICD": {
      "hand-ins": [
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReport CI/CD Strijp-T Rest APi \nMax van Hattum \u2013 27-11-2020 \n\nIntroduction \nFor Strijp-T we are making an application that is going to make use of a REST API that is going to \n\nmanage the data and business logic.  \n\nTo set up this project correctly and ensuring a good workflow with CI/CD (continuous integration and \n\ncontinuous delivery), I\u2019ve set up a azure devops account and project. There were also other options \n\navailable like FHICT\u2019s own gitlab, or GitHub\u2019s actions. However since we get credit from school to use \n\nAzure, and it seemed really interesting to me soI\u2019ve decided to go with them. \n\nMy goals were to learn how to automate testing on commits or merges, automate the building, \n\nautomate the publishing and protect branches with enforced pull requests.  \n\n \n\nConfiguring pipeline with a yaml file \nAzure-pipeline.yml \n\n \n\nThis file is responsible for setting up the pipeline. First I defined the triggers, these are the branches \n\non which the pipeline should be run when changes are detected. Now it will trigger on master, dev \n\n\n\nand every branch starting with feature/. This could be improved on by making a separate pipeline for \n\nthe feature branches, which only does testing, since now it also builds. \n\nNext up is the pool, this is where the commands will be run on. I\u2019ve chosen for Microsoft hosted \n\nagent, not a private one. Since we have no specific or complex requirements this will be sufficient.  \n\nThen we define the steps that are to be taken for the pipeline to complete. First we use the \n\nDotNetCore Command Line Interface to run the tests available. We define the paths to the test \n\nprojects and define the build configuration. \n\nThen we use Docker to build an image from the defined Dockerfile and push these to a container \n\nregistry for artifacts I\u2019ve setup, these are then ready for deployment.  \n\n \n\nResult coverage \nAfter the pipeline has completed a report is ready about the tests. \n\n \n\n \n\n \n\n\n\nPublished to registry container after pipeline \nAs you can see below after the pipeline triggers it stores the images with version control in a artifact registry.  \n\n \n\n\n\n \n\n \n\n \n\n \n\n \n\n  \n\n\n\nBranch policies  \n\n \n\nTo ensure code quality in release, I\u2019ve set up branch policies to enforce code reviews so that releases will only go through after a review and a pipeline that \n\nneed to have the tests to be run successful.  \n\n \n\n\n"
        }
      ]
    },
    "CompetenceDocument": {
      "hand-ins": [
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompetence document\n\n\n \n\n  \n\nCOMPETENCE DOCUMENT \nDit document gaat inzicht geven in hoe mijn competenties gegroeid zijn in het \n\nkomende semester. \n\nHattum,Max M.B. van \n3928780 \n\n31-08-2020 \n\nOpen Learning semester 3 \n\n\\*Docent naam TBD*\\ \n\nVersie 6 \n\nAbstract \nIn dit document beschrijf ik mijn startpunt en beschrijf en lever ik bewijs van mijn groei in \n\ncompetenties. Daarnaast zijn de projecten waar ik aan gewerkt heb hier in terug te vinden, \nbenoem ik wat ik hier voor heb gedaan en reflecteer ik hier op.  \n\n\n\n  \n\nInhoud \nPersona .................................................................................................................................................... 1 \n\nContext .................................................................................................................................................... 2 \n\nChallenge description Open Learning ..................................................................................................... 2 \n\nFull stack development PSV Swimming (project 1) ......................................................................... 2 \n\nChallenge descriptions Delta ................................................................................................................... 2 \n\nDeX Platform (Project 2) ...................................................................................................................... 2 \n\nStrijp-T (Project 3) ............................................................................................................................... 2 \n\nMagicLeap (Project 4) .......................................................................................................................... 2 \n\nCompetence profile ................................................................................................................................. 3 \n\nHuidig niveau ....................................................................................................................................... 3 \n\nGeambieerd niveau ............................................................................................................................. 3 \n\nKPI-table with proof ................................................................................................................................ 5 \n\nOpen Learning ..................................................................................................................................... 5 \n\nDelta .................................................................................................................................................... 5 \n\nRetrospective Delta ................................................................................................................................. 6 \n\nSprint retrospectives Open Learning ....................................................................................................... 8 \n\nSprint 1 ................................................................................................................................................ 8 \n\nSprint 2 ................................................................................................................................................ 8 \n\nStarr reflectie 1 ................................................................................................................................ 8 \n\nStarr reflectie 2 ................................................................................................................................ 9 \n\nStarr reflectie 3 .............................................................................................................................. 10 \n\nConclusie ....................................................................................................................................... 10 \n\nSprint 3 .............................................................................................................................................. 11 \n\nSprint 4 .............................................................................................................................................. 11 \n\nSprint 5 .............................................................................................................................................. 11 \n\nEvaluation and Reflection ...................................................................................................................... 12 \n\nFeedpulse .............................................................................................................................................. 12 \n\n \n\n \n\n  \n\n\n\n  \n\nVersiebeheer \n\nVersienummer Datum Auteur Veranderingen \n\n1 31-08-2020 Max van Hattum Voorblad, layout, eerste vier \nonderwerpen ingevuld. \n\n2 1-09-2020 Max van Hattum Pagina nummers toegevoegd (wel zo \nhandig met een inhoudsopgave) \n\n3 2-09-2020 Max van Hattum Extra challenge toegevoegd die \ninteressant lijkt. \n\n4 30-09-2020 Max van Hattum Persona uitgebreid met voorgaande \nrichting en huidige constructie. \nGekozen challenge uitgelicht. \nCompetence profile huidig niveau \ntoegelicht en beschreven hoe ik denk \nhet geambieerde niveau te bereiken. \n \n\n5 2-10-2020 Max van Hattum Retrospective toegevoegd \n\n6 4-11-2020 Max van Hattum Competence document layout beter \nopgesplitst in OL en Delta, bewijs \nDelta en OL uitgebreid, specifieke \nDelta retrospective toegevoegd en \nsprint 2 retrospectives toegevoegd met \nstarr reflecties. \n\n \n\n  \n\n\n\n  \n\n \n1 \n\nPersona \n \n\nIk ben Max van Hattum, vier\u00ebntwintig jaar oud en studerende aan \n\nhet Fontys ICT, geboren en getogen in Tilburg. Hiervoor heb ik al \n\ndrie jaar Biomedische Technologie gestudeerd, door ziekte moest \n\nik helaas uitvallen en heb ik besloten om ICT op HBO niveau te \n\ngaan doen. Hier heb ik gekozen in semester 2 voor de richting \n\nsoftware, en mijn propedeuse met een outstanding behaald. \n\nICT was altijd al mijn hobby, op mijn twaalfde heb ik een boek \n\nover PHP 5 voor mijn verjaardag gevraagd, dit zegt vast al genoeg \n\nover mijn interesse in software development. \n\nVerder hou ik van lezen, fitness, zwemmen en zo af en toe \n\ngamen. Graag onderneem ik ook dingen met vrienden, een \n\nterrasje in de zon of iets anders is altijd leuk. Daarnaast kijk ook \n\ngraag naar sport, denk hierbij aan voetbal, hockey, basketbal, \n\nFormule 1 of League of Legends. \n\nVoor semester drie combineer ik het Delta excellentie \n\nprogramma en open learning. Martijn heeft aangegeven dat ik \n\nzelf mijn tijd kan en moet verdelen. Ik heb er voor gekozen om \n\ntwee dagen aan het open learning, PSV Swimming app, te \n\nbesteden en mijn individuele project alleen te doen wanneer ik tijd over heb. \n\n \n\n  \n\n\n\n  \n\n \n2 \n\nContext \nMijn interesse ligt op het moment bij het ontwikkelen van software applicaties. Op het moment heb \n\nik al wat ervaring in het ontwikkelen van webapplicaties en ik wil dit graag doorzetten naar mobile \n\napplicaties. Daarnaast vind ik een stukje user interaction ook interessant, dus hier wil ik ook wat van \n\nmeepakken. \n\n \n\nChallenge description Open Learning \nFull stack development PSV Swimming (project 1) \nEr is een huidige app waarmee zwemmers zich kunnen aanmelden voor verschillende evenementen. \n\nDeze is echter verouderd en moet daarom verbeterd worden. Er moet aan een back-end gewerkt \n\nworden, en aan een front-end in de vorm van een mobiele applicatie. Er is een huidige userbase \n\nwaarmee getest kan worden. \n\nHet is uiteindelijk dit project geworden, dit was niet mijn eerste keuze maar hier ben ik in ingedeeld. \n\nAl met al ben ik wel tevreden hier mee, het is een uitdagende challenge. Hier in kan ik veel van de \n\nsoftware doelstellingen aantonen. Daarnaast is het systeem wat nu gebruikt wordt interessant en \n\ningewikkeld, dus is het leuk puzzelen om een goede plan van aanpak te construeren waarmee dit \n\ngeleidelijk vervangen kan worden door een effici\u00ebnter en gebruiksvriendelijker systeem. \n\nNatuurlijk is het ook leuk om voor een \u2018grote\u2019 naam zoals PSV een opdracht te doen en is het een \n\nnieuwe situatie voor mij om een project dat al half staat te continueren en dus gebruik te moeten \n\nmaken van vooraf onbekende programmeer talen, tools en frameworks. \n\n \n\nChallenge descriptions Delta \n\nDeX Platform (Project 2) \nHet DeX Platform is een webapplicatie die een centrale plaats moet worden voor het weergeven van \n\nprojecten. Het doel hiervan is om het vinden van projecten makkelijker te maken, waardoor \n\nsamenwerken makkelijker wordt.  \n\nEr is hiervoor een front-end, back-end en full-stack team, ik ben toegewezen aan het back-end team. \n\n \n\nStrijp-T (Project 3) \nStrijp-T als organisatie heeft bepaalde kernwaarden en wilt deze graag naar voren laten komen op \n\nhet terrein. De opdracht is erg breed, er is ons gevraagd om een concept uit te werken dat dit op een \n\nof andere manier voor elkaar krijgt.  \n\n \n\nMagicLeap (Project 4) \nDe MagicLeap is een AR-bril, het doel van dit project is exploratie. Wij mogen zelf met een idee \n\nkomen zolang het maar nieuw en uitdagend is. \n\n \n\n\n\n  \n\n \n3 \n\nCompetence profile \n\nHuidig niveau \nIk heb het eerste jaar gekozen voor de software , hiervoor moesten wij de onderstaande criteria \n\naantonen.  \n\nEr waren twee dagen beschikbaar voor een proftaak die voor een bedrijf gedaan moest worden \n\nwaarin de professionele vaardigheden aangetoond moesten worden en drie dagen voor een \n\nindividuele challenge waarin de software specifieke doelstellingen bewezen moesten worden. \n\nBeide waren mijn coaches erg enthousiast over, zowel mijn software kennis en professionele \n\nvaardigheden zaten zeer boven het verwachte niveau. waardoor ik dus ook de hoogste score behaald \n\nheb. De stakeholder voor de proftaak was zelfs zo enthousiast dat zij mij samen met twee anderen \n\nuit het groepje een contract wilden aanbieden om het project intern voor te zetten.  \n\nHiernaast heb ik ook al eerder drie jaar biomedische technologie gestudeerd aan de TU/e. Hier kwam \n\nook enkele overlap met ICT voor. Hier heb ik ook al tijd gehad om mijn professionele vaardigheden te \n\noefen, al hoewel hier minder de focus op lag dan nu. \n\n \n\nCurrent profile  \n\n Managing Analysing Advising \nDesignin\n\ng \nRealising  \n\nCommuni-\n\ncation \nLearning \n\nJudge- \n\nment \n\nUser interaction       x x x \n\nBusiness \n\nprocesses \n\n      \n\nSoftware x x x x x  \n\nHardware       \n\nInfrastructure       \n\n \n\nGeambieerd niveau \nIk wil sowieso mijn software kennis naar het volgende niveau tillen, hiernaast wil ik ook voor user \n\ninteraction kennis aantonen. Verder wil ik zoveel mogelijk van de professionele vaardigheden verder \n\nontwikkelen. \n\nDoordat ik meer dan een project heb door Delta heb ik veel ruimte om diverse doelstellingen aan te \n\ntonen, er was mij ook verteld dat ik werk gedaan binnen Delta kon gebruiken als bewijs voor KPI\u2019s. Ik \n\nheb op het moment twee projecten lopen waarin we van de grond af aan moeten beginnen en ik ook \n\nsamenwerk met van origine media studenten. Ik doe hier op het moment mee met de concepting, \n\nonderzoek en adviesgeving. Ik verwacht dus dit te kunnen gebruiken als bewijs voor UX. \n\nVerder lopen er ook twee projecten, Dex en PSV Swimming, waarin ik veel van de software \n\nvaardigheden kan aantonen. Beide zijn projecten die al lopen en al een basis, of meer, hebben staan. \n\nDit is een mooie gelegenheid om mijn software vaardigheden te toetsen binnen al een gedeeltelijk of \n\ngeheel bestaand systeem.  \n\n\n\n  \n\n \n4 \n\nBij Dex wordt gewerkt met een CI/CD, issues en pull requests met code reviews. Daarnaast wordt \n\nhierbinnen gebruikt gemaakt van een ORM, integratie tests doormiddel van postman en unit tests \n\ndie gebruik maken van een mocking framework. Na elke sprint wordt de master branch bijgewerkt, \n\nwaarna automatisch het project wordt gebuild, getest en gedeployed, gebruikmakend van docker. \n\nHiernaast heb ik hier ook de mogelijkheid in om nieuwe componenten te onderzoeken, designen en \n\nte implementeren, met in ooghoudend het bestaande systeem en de ambitie om over te stappen van \n\neen monolitische architectuur naar microservices.  \n\nBinnen deze verscheidene projecten, met contact met meerdere stakeholders verwacht ik dat ik veel \n\nof zelfs alle professionele vaardigheden naar het volgende niveau kan brengen.  \n\n \n\nIntended development v1 (After every adjustment to your competence profile (you may change this over time \n\nbecause it\u2019s dynamic) you\u2019ll add the new version below the other ones. In this way we can see the development \n\nof your profile.) \n\n Managing Analysing Advising \nDesignin\n\ng \n\nRealisin\n\ng \n \n\nCommuni-\n\ncation \nLearning \n\nJudge- \n\nment \n\nUser interaction x x x x x  x x x \n\nBusiness \n\nprocesses \n\n      \n\nSoftware x x x x x  \n\nHardware       \n\nInfrastructure       \n\n \n\nFinal development \n\n Managing Analysing Advising \nDesignin\n\ng \n\nRealisin\n\ng \n \n\nCommuni-\n\ncation \nLearning \n\nJudge- \n\nment \n\nUser interaction          \n\nBusiness \n\nprocesses \n\n      \n\nSoftware       \n\nHardware       \n\nInfrastructure       \n\n \n\nLevel  1 Level  2 Level 3 \n\n \n\n\n\n  \n\n \n5 \n\nKPI-table with proof \n\nOpen Learning \nBij Open Learning werk ik aan het project PSV Zwemmen App. Ik verwacht dat ik hier binnen bewijs \n\nkan aanleveren dat verscheidene software KPI\u2019s aantoont.  \n\nIk heb veel geleerd over Java, Spring Boot, Autowired, JPA en Hibernate. Deze waren nodig om te \n\nsnappen hoe de demo werkte, hoe ik deze kon verbeteren en uiteindelijk kon omtoveren in een \n\nfunctionele app. Dit is uiteindelijk toch niet de plan van aanpak geworden, waarna ik een nieuw \n\nproject heb gestart. \n\nBinnen dit nieuwe Spring Boot project heb ik geleerd hoe hier authenticatie en autorisatie te \n\nconfigureren doormiddel van JWT, hierbij kwam ook endpoint configuratie aanbod. Dit ging een stuk \n\nanders dan hoe ik het gewend ben in ASP.NET Core, het is ingewikkelder maar staat wel veel meer \n\naangepaste configuratie toe. \n\n \n\nDelta \nBij Delta werk ik aan drie projecten, waarvan twee het meest relevant zijn voor het aantonen van \n\nKPI\u2019s. Binnen het DeX project verwacht ik veel software KPI\u2019s aan te kunnen tonen. Voor het project \n\nStrijp-T denk ik dat ik UX KPI\u2019s kan halen en nog ondersteuning voor software KPI\u2019s.  \n\nBinnen DeX heb ik al werk gedaan wat gebruikt kan worden voor software KPI\u2019s, namelijk het \n\nreviewen van code, het opstellen van een advies document of een notificatie systeem en het \n\nopstellen van de C4 modellen voor ditzelfde notificatie systeem. \n\nBij Strijp-T hebben wij concepten uitgedacht en deze gepresenteerd aan de stakeholder, hier is een \n\nkeuze in gemaakt. Nu kan ik hier beginnen met de opzet van een PWA. \n\n \n\nKPI Proof Rating \n\nPL-2.1, PL-2.2, PL-2.3, TI-2.1, TI-2.2, TI-\n\n2.3, TI-2.4, TI-2.5. \n\nProject 1 \n\nSprint opleveringen aan docenten en \n\nstakeholder. \n\nhttps://fhict.instructure.com/courses/10657/a\n\nssignments/164899?module_item_id=586631 \n\n \n\nAnalysis-S2.1, Analysis-S2.2, FOO-2.1, \n\nFOO-2.2, FOO-2.5 \n\n. \n\nProject 1 \n\nEen onderzoek naar hoe een bestaande demo \n\nvan een vorige project groep omgezet kan \n\nworden naar een functionele app. \n\nhttps://fhict.instructure.com/courses/10657/a\n\nssignments/169490?module_item_id=596662 \n\n \n\nhttps://fhict.instructure.com/courses/10657/assignments/164899?module_item_id=586631\nhttps://fhict.instructure.com/courses/10657/assignments/164899?module_item_id=586631\nhttps://fhict.instructure.com/courses/10657/assignments/169490?module_item_id=596662\nhttps://fhict.instructure.com/courses/10657/assignments/169490?module_item_id=596662\n\n\n  \n\n \n6 \n\nManage&Control-S2.1, \n\nManage&Control-S2.2 \n\nProject 3 \n\nVoor een goede workflow heb ik een CI/CD \n\nopgezet met automatisch testen en deployen.  \n\nhttps://fhict.instructure.com/courses/10657/a\n\nssignments/168667?module_item_id=592308 \n\n \n\nAdvise-2.1, Advise-2.2, Advise-2.3, IPS-\n\n2.1, IPS-2.2, IPS-2.3, IPS-2.4 \n\nProject 2 \n\nVoor het DeX Platform moest een notificatie \n\nsysteem opgezet worden, hiervoor heb ik een \n\nadvies document gemaakt. \n\nhttps://fhict.instructure.com/courses/10657/a\n\nssignments/167651?module_item_id=588845 \n\n \n\nDesign-S2.1, Design-S2.2, Design-S2.3 Project 2 \n\nVoor het DeX Platform moest een notificatie \n\nsysteem opgezet worden, hiervoor heb ik een \n\ndesign document gemaakt die gebruikt maakt \n\nvan het C4 model. \n\nhttps://fhict.instructure.com/courses/10657/a\n\nssignments/167655?module_item_id=588858 \n\n \n\n \n\n \n\n \n\nRetrospective Delta \nBij Delta ligt de verantwoordelijkheid erg bij jezelf, wat ik wel fijn vind. Als ik terugkijk op de eerste \n\nhelft van dit semester vind ik dat de manier van werken goed bij mij past. Ik heb veel initiatief \n\ngetoond, waardoor ik lekker bezig kon blijven. Veel projecten hadden wel een lastige opstart, dit had \n\nook wel erg te maken met de huidige Corona situatie. Het duurde wel even voordat ik echt lekker \n\nbezig was met de Delta projecten. Bij DeX kon ik het snelste aan de slag, hierna Strijp-T en daarna \n\nMagic Leap.  \n\nBij DeX staat al een heel project en een gestructureerde manier van werken, na een eerste sprint \n\nhiermee kennis te maken, verliep het soepel. Ik kan binnen dit project veel leren van de manier van \n\nwerken, en heb hier al veel van opgestoken. Doordat er wel met een extensief review systeem \n\ngewerkt wordt, kan het soms wel even duren voor je weer verder kan met je taak. Dit was echter niet \n\nerg omdat ik toch andere projecten had om ook aan te werken. \n\nBinnen DeX kan ik echt mij ei kwijt qua software engineering, ik heb het gevoel dat ik kennis breng, \n\nmaar dat er ook zeker veel op te steken valt voor mij. Het samenwerken met de andere studenten \n\ngaat ook lekker, ik werk vooral in paren wat zorgt voor beter kwaliteit werk. \n\nStrijp-T was leuk samenwerken, plus dat ik hier wat anders tegenkom dan ik normaal gewend ben \n\nvanuit software. Mijn groepsgenoten komen allemaal vanuit de Media kant, en we moesten hier ook \n\nhttps://fhict.instructure.com/courses/10657/assignments/168667?module_item_id=592308\nhttps://fhict.instructure.com/courses/10657/assignments/168667?module_item_id=592308\nhttps://fhict.instructure.com/courses/10657/assignments/167651?module_item_id=588845\nhttps://fhict.instructure.com/courses/10657/assignments/167651?module_item_id=588845\nhttps://fhict.instructure.com/courses/10657/assignments/167655?module_item_id=588858\nhttps://fhict.instructure.com/courses/10657/assignments/167655?module_item_id=588858\n\n\n  \n\n \n7 \n\nbeginnen met het verzinnen en uitwerken van concepten. Ik vind dit uitdagend en interessant om te \n\ndoen, ook verfrissend omdat het weer eens wat anders is dan het standaard software systemen \n\ndesignen. \n\nMagic Leap ben ik tot nu toe het minst tevreden mee, ik behandel dit op het moment ook als meer \n\neen hobby project. We hebben hier tot nu toe veel brainstorming gedaan. Het is verder vind ik lastig \n\nom voor de Magic Leap concreet software te maken, we zijn al wel zo ver dat we een app kunnen \n\ndeployen en hier eigen geanimeerde objecten in kunnen zetten. De documentatie is echter erg slecht \n\nnaar mijn mening en dit maakt het frustrerend om mee te werken. \n\nAl met al heb ik ook de projecten samen goed gemanaged om een persoonlijk niveau. Ik moet erg \n\nletten op mijn energie niveau en wat mijn grenzen zijn. Dit is mij goed gelukt en ik heb het idee dat ik \n\nveel heb kunnen bijdragen, terwijl ik tegelijkertijd goed heb kunnen letten op mijn mentale \n\ngezondheid en energie niveau.  \n\nDe organisatie binnen Delta is wat mijn idee net iets t\u00e9 losjes, van mij mag er meer structuur in \n\nzitten. Maar ik heb dit goed los kunnen laten en mijn eigen structuur op kunnen zetten. \n\n \n\n  \n\n\n\n  \n\n \n8 \n\nSprint retrospectives Open Learning \nAfter every sprint demo, you will have a retrospective. You will reflect on your process, work method \n\nand the communication within your group. Include a summary of each retrospective in this \n\ndocument. \n\nSprint 1 \nVoor mij persoonlijk ging de eerste sprint vlot. Ik heb meteen in het begin veel initiatief genomen en \n\ncontact gezocht met stakeholders en projectleden. Helaas waren enkele projectleden minder \n\ngemotiveerd en deden niet mee zoals ik verwacht van medestudenten. De coaches hebben hier een \n\ngoede oplossing voor gevonden door het groepje in twee\u00ebn te splitsen. \n\nVerder is de samenwerking tussen mij en Stephanie wel goed gegaan, we hebben een klik en weten \n\nvan elkaar wat we binnen dit project willen bereiken. Van mij mag de communicatie over de taken en \n\nhuidige werkzaamheden wel beter, hier hebben we het over gehad en we gaan dit meteen de eerste \n\ndag van de nieuwe sprint oppakken. \n\nDe externe stakeholder was nog niet bereikbaar wat ik wel jammer vond, dit nam toch een stukje \n\nmotivatie bij mij weg. Ik had een beetje het idee dat ik niet wist wat ik daadwerkelijk kon doen, \n\nvooral omdat het ook veel inlezen was over wat de vorige groep had staan. \n\nMaar Wilrik heeft het goed opgepakt als acting stakeholder en hierdoor werd ik weer enthousiaster. \n\nMaar voor mij is het dus wel een belangrijk punt om op te letten, dat ik zelfs met tegenslagen \n\nprobeer de motivatie er in te houden.  \n\nVoor de rest wil ik toekomstige retrospectives op een meer methodische manier gaan benaderen en \n\nwil ik de Starr-reflectie methode gaan toepassen. \n\nAl met al ben ik tevreden met hoe het proces is verlopen, maar ik verwacht wel dat ik in de volgende \n\nsprint meer concreets kan opleveren. \n\n \n\nSprint 2 \n\nStarr reflectie 1 \nSituatie \n\nDit speelde zich af binnen een teams meet, met Stephanie, Cees, Martin en ik. Het was ons eerste \n\ngesprek met de externe stakeholder. \n\nTaak \n\nMijn taak was om samen met Stephanie kennis te maken met de stakeholder, te laten zien wat wij \n\ntot nu gedaan hebben en vragen stellen zodat wij erachter konden komen waar de prioriteiten van \n\nde stakeholder exact lagen. \n\nActie \n\nIk sprong te snel in het gedeelte over wat we al gedaan hadden en verwaarloosde het kennismaking \n\ngedeelte. Cees wees mij terplekke terecht er op dat het misschien een goed idee was om wat \n\nuitgebreidere kennis te maken alvorens we naar de zakelijke inhoud gingen. Ik bood mijn excuses aan \n\nen gaf toen een wat uitgebreidere uitleg van wie ik ben en wat ik wil leren. Ook Martin gaf wat meer \n\ninzicht in wie hij is en waarom hij naar Fontys is gekomen met deze challenge. \n\n\n\n  \n\n \n9 \n\nResultaat \n\nWe hadden een korte onderbreking in het gesprek omdat we dus door mij moesten schakelen van \n\nhet ene naar het andere onderwerp, echter ging dit snel en soepel. Uiteindelijk hadden we een goed \n\nen fijn gesprek. Waarin we elkaar en de opdracht beter leerde kennen. \n\nReflectie \n\nIk vond het slecht van mijzelf dat ik veel te snel doorging naar het zakelijke gedeelte en het \n\n\u2018menselijke\u2019 gedeelte verwaarloosde. Ik schaamde mij ook wel een beetje toen ik hier (terecht) op \n\ngewezen werd. Verder ben ik wel tevreden met hoe ik het heb aangepakt, ik heb het goed opgepakt \n\nen het resultaat was erna een fijn gesprek. De essentie is wel dat ik beter moet letten op wat mijn \n\ndoelgroep is en goed moet opletten dat ik niet door mijn enthousiasme voor het een, andere \n\nbelangrijke zaken verwaarloos. Een volgende keer ga ik beter van te voren voor mijzelf doornemen \n\nwat nou de bedoeling van de situatie is, wie de doelgroep is en hoe ik het het beste kan aanpakken. \n\n \n\nStarr reflectie 2 \nSituatie \n\nIn de week na de herfstvakantie moesten wij opleveren aan de coaches en de stakeholder. De week \n\nvoor de herfstvakantie was Stephanie al niet aanwezig door ziekte, de maandag na de herfstvakantie \n\nliet ze weten dat ze nog steeds (erg) ziek was. Zij was verantwoordelijk voor de oplevering deze keer.   \n\nTaak \n\nMijn originele taak was om materiaal aan te leveren voor de oplevering, echter nu Stephanie ziek \n\nwas moest er gecommuniceerd worden met de coaches en stakeholder voor een oplossing. \n\nAangezien ik het enige verdere lid ben in de groep lag deze (onverwachte) verantwoordelijkheid dus \n\nbij mij. Ik moest dus een oplossing vinden. \n\nActie \n\nIk kon het snelste contact hebben met Britt, na de Open Learning general stand-up. Toen heb ik \n\ndoorgegeven wat de situatie was en om input gevraagd. \n\nResultaat \n\nUiteindelijk was de conclusie dat het dan maar meer een update ging worden en dat zulke \n\ntegenslagen nou eenmaal bij de realiteit horen. \n\nReflectie \n\nIk vind dat ik het goed heb gedaan, ik heb echt zo snel mogelijk er werk van gemaakt. Hier ben ik wel \n\ntrots op omdat, ook al was het gewoon een tegenslag, ik mij dit niet liet weerhouden van juist \n\nhandelen. Dit zorgde er voor dat sneller bij mij de onrust weggepakt werd omdat ik meteen een \n\ngeruststellend antwoord kreeg van de coach. Ik ben dus tevreden met hoe het gelopen was. \n\nIn essentie was dit dus gewoon de juiste aanpak en moet ik het direct aanpakken van problemen \n\ndoorzetten in mijn toekomstige werkwijze.  \n\n \n\n\n\n  \n\n \n10 \n\nStarr reflectie 3 \nSituatie technische tegenslag -> zelf eerst gedaan wat ik kon doen -> niet te lang doorgegaan maar \n\nbesproken met cees -> goede feedback -> andere richting in gegaan -> goed gehandeld omdat elke \n\nverandering weer meer werk met zich meebracht, maar vervelend gevoel want voelt als opnieuw \n\nbeginnen. \n\nSituatie \n\nIk had een face to face gesprek met Cees met als onderwerp dat het te (onnodig) moeilijk was om de \n\nhuidige demo om proberen te zetten naar een functionele app. \n\nTaak \n\nMijn taak was om de huidige demo zo om te bouwen dat de data in een database werd opgeslagen \n\ndoormiddel van een ORM. Voor ik hier aan begon verwachte ik dat dit goed te doen zou moeten zijn, \n\nook al had ik nog geen eerdere ervaring met Spring Boot, het leek heel erg op ASP.NET waar ik wel \n\nveel ervaring in heb. Van mijzelf verwachte ik dus ook dat ik dit met minimale moeite kon \n\nbewerkstelligen. \n\nActie \n\nHet bleek uiteindelijk veel lastiger dan ik van te voren dacht. Door verscheidene tegenslagen deed \n\nelke stap die ik zette weer extra werk opleveren. Mijn conclusie was hierdoor dat het makkelijker zou \n\nzijn om het Spring Boot project opnieuw te configureren en vanaf begin af aan te beginnen, natuurlijk \n\nwel met de huidige demo als leidende draad. Voor ik dit wilde doen vond ik dit wel een beslissing die \n\nbesproken moest worden met Cees. Vandaar dat ik dit onderwerp met hem aansneed en hem mijn \n\nproblemen uitlegde met de voor- en nadelen van het opnieuw opzetten van een project. \n\nResultaat \n\nHet resultaat was dat Cees goed met mij meedacht en kritische vragen stelde. Uiteindelijk merkte hij \n\nop dat hij vond dat ik er goed over nagedacht had en hij het dus prima vond als ik een nieuw project \n\nwilde opzetten.  \n\nReflectie \n\nIk vind dat ik het goed aangepakt heb, ik ben niet te lang blijven doorploeteren, maar ik heb ook niet \n\nmeteen opgegeven. Daarnaast heb ik ook wel degelijk geleerd van het werken in de demo en \n\nhierdoor nieuw inzichten gekregen die ik anders niet zou hebben. Ook vind ik dat ik een goede \n\nbeslissing heb genomen in het betrekken van Cees er bij, vooral omdat ik geen mede Software \n\nstudenten heb om het mee te bespreken. Ik heb geleerd om goed op te blijven letten naar de \n\nvooruitgang en niet nodeloos door te werken puur door koppig- of stugheid. Een volgende keer zou \n\nik misschien zelfs al eerder aan de bel moeten trekken, al vond ik dat in deze situatie ik niet zonder \n\nwaarde doorgewerkt heb.  \n\nConclusie \nIn zijn geheel heb ik een voor mij leerzame sprint gehad. Ik heb veel bijgeleerd in Java, Spring Boot en \n\nbijbehorende libraries. Daarnaast heb ik correcte conclusies kunnen trekken over wat een goede plan \n\nvan aanpak is. Ik vind het wel jammer dat door ziekte, technische tegenslag en internet uitval we ging \n\nechte oplevering hebben kunnen doen. Het opleveren is wel altijd iets speciaals aan een sprint waar \n\nik persoonlijk echt naar toe werk, dat deze dan uitvalt vind ik teleurstellend. \n\n\n\n  \n\n \n11 \n\nSprint 3 \nDeze sprint was voor mij echt bikkelen, er was enorm veel werk te doen aan de back-end. Ik heb een \n\ntotaal nieuw project opgezet en de functionaliteiten die in de demo beschikbaar waren werkend, \n\nmet database en oog op veiligheid, gekregen. Hier heb ik ook een research document voor opgezet.  \n\nPersoonlijk had ik er een goed gevoel bij, ik heb hard gewerkt en ook enorm veel voor elkaar \n\ngekregen. De oplevering met Martin ging wat mij betreft goed, maar Martin zelf zag helaas niet veel \n\nvooruitgang. Dit had te maken met wat vorige projectgroepen al gedaan hadden en hij het gevoel \n\nhad dat al dit werk opnieuw gedaan werd. Vanuit zijn oogpunt is dit zeker begrijpelijk, maar dit \n\nneemt vind ik ons gedane werk niet weg. Veel werk dat ik gedaan heb ook aan de back end is ook \n\npraktisch \u2018onzichtbaar\u2019 voor een niet-expert stakeholder. \n\nGelukkig zien onze docenten wel het werk dat wij verzetten en nam Cees het ook voor ons op.  \n\nDe docenten zijn ook gewoon tevreden met het werk dat wij doen, maar hopelijk hebben we in \n\nsprint 4 een meer interactieve demo waar de stakeholder ook  meer aan heeft. \n\nHet samenwerken met Stephanie gaat wat mij betreft ook prima, tot nu toe hebben we veel \n\ngescheiden kunnen werken, maar we houden wel contact. Deze komende sprint moeten wij meer \n\ngaan samenwerken, om de koppeling tussen back-end en front-end te verwezenlijken. Ik verwacht \n\ndat dit gewoon goed gaat verlopen, afgaande op het fijne samenwerken al tot nu toe. \n\n \n\nSprint 4 \nSprint 4 ging persoonlijk goed, ik heb een grote nieuwe feature kunnen implementeren, namelijk het \n\nimporten van Meets vanuit Lenex files. Ook heb ik veel aan de automatische testen gedaan. \n\nHelaas heeft Stephanie het niet gehaald om tot implementatie van de design te komen aan de front-\n\nend kant, dit vind ik wel jammer omdat ik wel een groot systeem heb opgezet voor data verwerking. \n\nMaar dit niet gebruikt kan worden zonder een user interface.  \n\nWe hebben het er wel goed over gehad, ook over hoe we dit aan de stakeholder brengen, dus de \n\ncommunicatie is wel fijn gebleven. \n\nVerder heb ik ook een groot gedeelte van de presentatie gedaan en hier kreeg ik goede feedback op \n\nvan Britt. Het advies was om de volgende keer (als we dan weer gescheiden demo\u2019s back-end front-\n\nend hebben), om dan eerst het front-end te laten zien. Dit maakt het makkelijker en concreter om te \n\nbegrijpen. \n\nAl met al met ik tevreden met wat ik bereikt heb en gerealiseerd heb, ik heb ook een duidelijk idee \n\nvan wat ik in de laatste sprint wil gaan doen. Ik hoop dat we dan tot een echte realisatie komen waar \n\nalles samen komen. \n\n \n\nSprint 5 \n \n\n  \n\n\n\n  \n\n \n12 \n\nEvaluation and Reflection \nAdd an evaluation and a reflection of your whole Open Innovation semester. Your evaluation \n\ndescribes what went good and bad during your process and how you dealt with that. Your reflection \n\ndescribes how you have grown as a person, and what you will take with you in your further \n\nprofessional career. In the reflection you should also shine a light on the following aspects: \n\nResponsible \n\n- Acts consciously with concern for the greater good with contemplation of relevant approaches. \n\n- Makes an assessment of different interests. \n\nInnovativeness \n\n- Focuses on renewal, improvement and making new connections. \n\n- Spots or creates opportunities and seizes them. \n\nResilience  \n\n- Challenges own ideas. \n\n- Perseveres in finding a result or solution. \n\n \n\n \n\nFeedpulse \nAdd a screenshot of your feedpulse overview.  \n\n \n\n \n\n\n"
        }
      ]
    },
    "Concepting": {
      "hand-ins": [
        null
      ]
    },
    "ConceptsResearchandAdvise": {
      "hand-ins": [
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantified Student  \nAdvise Regarding Concepts \nRuben Fricke \u2013 Niek van Dam \u2013 Max van Hattum \n\n1-4-2021 \n\n  \n\n\n\nDependency Matrix \nDocument Name \n\nTopic Analysis \n\nReal-Time Sentiment Analysis \n\nResearch Proficiency Analysis \n\n \n\nTable of Contents \nDependency Matrix  ............................................................................................................................ 2 \n\nIntroduction ............................................................................................................................................. 2 \n\nAnalyzing the language proficiency of the students hand-ins. ............................................................... 3 \n\nNudging students based on the sentiment of sentences they write  in FeedPulse. ............................... 3 \n\nConnecting students with each other based on similar assignments they\u2019ve created. .......................... 4 \n\nCreating a word web based on the student's course which shows their areas of expertise. ................. 4 \n\n \n\n \n\nIntroduction \nQuantified Student wants to enable students to get more insight in their learning process. We want \n\nto achieve this by gathering and processing all kinds of data available through the digital learning \n\nenvironment.  This should allow students to understand their learning process and adjust where \n\ndeemed necessary.   \n\nThe four concepts that are up for the discussion are as follows: \n\n- Analyzing the language proficiency of the student\u2019s hand-ins. \n\n- Nudging students based on the sentiment of sentences they write  in Feedpulse. \n\n- Connecting students with each other based on similar assignments they\u2019ve created. \n\n- Creating a word web based on the student\u2019s course which shows their areas of expertise. \n\n \n\n  \n\n\n\nAnalyzing the language proficiency of the student\u2019s hand-ins. \nThe grading of student\u2019s language proficiency can be done with readability indexes, which grade the \n\nentirety of the text based on the occurrence of sentences, syllables, and words. In my research  I \n\nlooked at one NLP solution and 5 widely applied readability indexes: \u2018ARI\u2019, \u2018Gunning Fog Index\u2019 \u2018Flesh-\n\nKincaid Grade\u2019, and \u2018Flesh readability Index\u2019.  \n\nThe NLP solution matches input texts to textbooks of that level and language, and afterwards took \n\nthe level of proficiency needed to read the textbook. Although this is a very innovative solution, the \n\nsetup and implementation are too big for our scope and would take longer than wished for to realize.  \n\nCompared to the NLP solution, the readability indexes are compact and powerful, usually appearing \n\nas one formula which can be calculated based on metadata from the text. This formula outputs the \n\n\u2018proficiency level\u2019 required to read the given text, which therefore also requires a person to write at \n\nthis level. I would still advise against using the readability indexes, as these are language-specific \n\n(mostly English only). If you attempt to use a language for which the readability index is not made, \n\nyou will get warped results which not properly represent the text. To overcome this, we will have to \n\nresearch and develop our own set of tools for the Dutch which require a great level of linguistic \n\nexpertise.   \n\n \n\nNudging students based on the sentiment of sentences they write  in \n\nFeedpulse. \nIt would indeed be possible to achieve this, when using a rule-based approach of analyzing the \n\nsentiment of sentences. This results in speedy and accurate results, allowing it to be used in real-\n\ntime.  \n\nThe sentence score could then be used to nudge the student, for example if they\u2019ve typed multiple \n\nnegative sentences, they could be nudged with a pop-up: \u201cWas your day really that bad?\u201d. This could \n\ngive the student more insight in their state of mind. \n\n \n\n  \n\n\n\nConnecting students with each other based on similar assignments \n\nthey\u2019ve created. \nThe research conducted leans to the conclusion that it would indeed be possible to achieve this feat \n\nwith Topic Modeling. Machine learning could be applied to create a model that assigns documents, in \n\nthis case the assignments, to categories. Then when a student creates a new assignment, the same \n\nmodel can be used to categorize the new one.  \n\nBased on which category the new assignment belongs too, the student can be shown other students, \n\nwith extra data such as the grading/expertise,  that also have assignments in this category. Allowing \n\nthem to get in to contact with each other. \n\nHowever, there are some flaws in the research, mainly that the data used to create the POC are \n\nprofessionally written articles, while the data that is going to be used are written on a lower level by \n\nstudents.  \n\n \n\nCreating a word web based on the student's course which shows their \n\nareas of expertise. \nThe last concept that is wanted is creating a word web for students, so they are giving insight into \n\ntheir own skills. An important aspect for this concept is TF-IDF. TF-IDF is a natural language \n\nprocessing and information retrieval method. With this algorithm, terms will get weighted on their \n\nimportance which gives a more insightful image of the analyzed text.  \n\nFor creating a word web, TF-IDF is recommended. It can create a selection of important terms that \n\ncan be used for filling the word web that represent the student in a meaningful and insightful way. \n\nThe research also conducted that TF-IDF is a perfect candidate for preprocessing that can also be \n\nused in other concepts to improve the outcome. \n\n \n\n\n"
        },
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantified Student \u2013 Real-time Sentiment \nAnalyse \nMax van Hattum\u2013 Delta \u2013 18-3-2021 \n\n\n\n \n1 \n\nTable of Contents \nIntroduction ............................................................................................................................................. 2 \n\nResearch question ................................................................................................................................... 2 \n\nResearch methods ................................................................................................................................... 2 \n\nWhat is Sentiment Analysis? ................................................................................................................... 3 \n\nTypes of sentiment analysis ................................................................................................................ 3 \n\nFine-grained sentiment analysis ...................................................................................................... 3 \n\nEmotion detection ........................................................................................................................... 3 \n\nAspect-based sentiment analysis .................................................................................................... 3 \n\nReal-time sentiment analysis. ......................................................................................................... 3 \n\nHow does Sentiment Analysis work? ...................................................................................................... 4 \n\nDrawbacks ........................................................................................................................................... 4 \n\nImplementations ................................................................................................................................. 4 \n\nIs Sentiment Analysis de best solution for the context? ......................................................................... 5 \n\nProof of Concept ................................................................................................................................. 5 \n\nConclusion and Advise ............................................................................................................................. 6 \n\nGlossary ................................................................................................................................................... 6 \n\n \n\n  \n\n\n\n \n2 \n\nIntroduction \nQuantified Student wants to enable students to get more insight in their learning process. We want \n\nto achieve this by gathering and processing all kinds of data available through the digital learning \n\nenvironment. \n\nThis should allow students to understand their learning process and adjust where deemed necessary.  \n\nOne of the concepts is to nudge a student if the text they are typing is predominantly negative or \n\npositive. Mainly while they are utilizing the Feedpulse module, which allows students to reflect on \n\ntheir day. \n\nAn example of such an implementation would be that when the student has typed multiple negative \n\nsentences in a short time, that they will get a pop-up saying: \u201cDid you really have such a bad day?\u201d. \n\nTo achieve this automatic Sentiment Analysis in real-time should be researched. \n\n \n\nResearch question \nThe main question is as follows: \n\nHow can Sentiment Analysis be used to instantly make a student aware of the \n\ntone of their text? \n\nTo be able to answer this question accurately, the question needs to be divided in several sub-\n\nquestions. It\u2019s important to first determine wat sentiment analysis is, how it works and if it fits the \n\ncontext. \n\n- What is Sentiment Analysis? \n\n- How does Sentiment Analysis work? \n\n- Is Sentiment Analysis the best fit for the context? \n\n \n\nResearch methods \nWe will be using literature-based research and the creation of a proof of concept. This will also be \n\nvalidated with the stakeholder to guarantee a right fit for the problem. Combining the three methods \n\nwill result in certainty and fit. \n\n  \n\n\n\n \n3 \n\nWhat is Sentiment Analysis? \nSentiment analysis is used for  discovering the polarity of a text, which can be defined as negative, \n\nneutral, or positive.  This could be done with a rule-based or machine learning approach (or a hybrid). \n\nThis could for example be used for quickly gaining insight into the sentiment of people towards a \n\ntweet or scanning product reviews in bulk.  \n\n \n\nTypes of sentiment analysis  \n\nFine-grained sentiment analysis  \nThis type of analysis focusses on a better, more defined assessment of the sentiment of the text in \n\nquestion. It will divide it in more categories e.g., very negative, negative, neutral, positive, and very \n\npositive. This is perfect for generating a star-based evaluation of a review.   \n\n \n\nEmotion detection  \nIt\u2019s also possible to focus on detecting emotion within a text. It might be more suitable for a problem \n\nto be quantified in types of emotion. While \u2018thrilling\u2019 and \u2018interesting\u2019 could both be construed as \n\npositive, they convene a different sentiment.   \n\nThis type of sentiment can be tricky though, since people express their emotions differently, e.g. \u2018I \n\nwould kill for that\u2019.   \n\n \n\nAspect-based sentiment analysis  \nA piece of text might be a reaction to an issue with multiple facets. You might be interested in \n\ndetermining which aspects are received negatively, and which are received positively. For this you \n\ncan use aspect-based sentiment analysis, which tries to determine multiple facets in the text and \n\nassign the sentiment accordingly  \n\n \n\nReal-time sentiment analysis.  \nIt could also be possible to monitor for example tweets or posts on a forum in real-time. You could \n\nset a threshold for when you should be notified and discover issues or trends quickly. (Monkeylearn, \n\nz.d.)  \n\nIn general rule-based solutions are faster in this than implementations based on Machine Learning: \n\nHowever, when compared to sophisticated machine learning techniques, the \n\nsimplicity of VADER carries several advantages. First, it is both quick and \n\ncomputationally economical without sacrificing accuracy. (Hutto & Gilbert, 2014, \n\np. 10) \n\n  \n\n\n\n \n4 \n\nHow does Sentiment Analysis work? \nRule-based programs generally have a lexicon with words that have a score and then use an \n\nalgorithm to determine the sentiment of a piece of text. Most of the time they don\u2019t take position or \n\ndouble negatives into account. Which can result in less accurate determinations. \n\nMost solution use machine-learning or a combination of rules and machine-learning. Most \n\nimplementations use a bag of words or bag-of-ngrams to vectorize the text.   \n\nA bag of words uses a list of predetermined words to search and quantify them from the text. A bag \n\nof n-grams search for a combination of words. Both methods spit out a vector representation of the \n\npiece of text, the vector represents which words or n-grams are present in the text. Then these \n\nvectors can be scored. (Brownlee, 2019)  \n\nTo score the vectors the vocabulary must be classified e.g., positive, neutral, negative.  \n\n \n\nDrawbacks  \nIt\u2019s hard to detect irony or sarcasm, furthermore sentiment must be placed in context. Because \n\nsentiment is subjective, it\u2019s also difficult to score words. This leads to needing to define the terms \n\nwith which the text is going to be scored.   \n\n \n\nImplementations   \nThere are multiple resources available for scoring text based on sentiment. There is Microsoft Azure \n\nText analytics, IBM Watson natural language detection, Amazon Comprehend and VaderSentiment.  \n\nInterestingly VaderSentiment is the only rule-based implementation between these noteworthy \n\nservices. However, it scores well in comparison to the above services: \n\nVADER performed as well as (and in most cases, better than) eleven other highly \n\nregarded sentiment analysis tools. (Hutto & Gilbert, 2014, p. 11) \n\nMost are extensive and complicated. There are also explanations available online on how to train \n\nyour own model (Jain, 2020)  \n\n \n\n  \n\n\n\n \n5 \n\nIs Sentiment Analysis de best solution for the context? \nFor the context, real-time sentiment analysis can be used. A rule-based implementation would be \n\npreferable since it is very fast and light weight. Feedpulse can be in different languages, however \n\nVaderSentiment also works with multiple languages, it states that it is accurate in determining \n\nsentiment even when a sentence is first translated to English and then analyzed. (C, z.d.). \n\nTaking these points in account VaderSentiment is a good fit for the context. It must be noted \n\nhowever that it is made for analyzing small pieces of text, namely focused on social media, and is \n\nprobably less accurate on larger texts.  \n\n \n\nProof of Concept \nAs a proof of concept, I\u2019ve created a simple JavaScript application that reads out a textbox. To extract \n\nsentences, it monitors usages of points (\u2018.\u2019). After every \u2018.\u2019 entry, it takes the text from the previous \n\npoint, up until the latest point. It uses Google\u2019s Compact Language detector to then determine the \n\nlanguage, if it\u2019s not English it first translates the sentence to English. \n\nThen it analyses this piece of text with the VaderSentiment JavaScript library. (V, z.d.-b). This results \n\nin four scores, the positivity, naturality and negativity.  \n\nThe final score is the compound score, it uses rules to give more weight to certain situations. For \n\nexample, in the following sentence: \u201cThe idea was great, however the execution was horrible.\u201d It \n\nregards the part after however as more important. \n\nhttps://github.com/Quantified-Student/POCs/tree/main/poc-sentiment-textbox  \n\n \n\n \n\nhttps://github.com/Quantified-Student/POCs/tree/main/poc-sentiment-textbox\n\n\n \n6 \n\nConclusion and Advise \nTo conclude this research, I can state with certainty that Sentiment Analysis could indeed be used to \n\nmake students aware of the tone of their text in real-time. Sentiment analysis is accurate and can be \n\ndone sufficiently quick, so that real-time analysis is possible. \n\nMy advice is to use a rule-based lexicon implementation, since it is light-weight giving fast results. \n\nThe most used and citated implementation is VaderSentiment, and it proves to fit for the context. \n\nUsing one of the VaderSentiment libraries, passing in a sentence, and then using the result to nudge \n\nthe student would be the desired course of action. \n\n \n\n \n\nGlossary \nBrownlee, J. (2019, 7 August). A Gentle Introduction to the Bag-of-Words Model. Machine Learning \n\nMastery. https://machinelearningmastery.com/gentle-introduction-bag-words-model/  \n\nMonkeylearn. (z.d.). Everything There Is to Know about Sentiment Analysis., van \n\nhttps://monkeylearn.com/sentiment-analysis/  \n\nJain, S. (2020, 5 June). Ultimate guide to deal with Text Data (using Python) \u2013 for Data Scientists and \n\nEngineers. Analytics Vidhya. https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-\n\ndeal-text-data-predictive-python/ \n\nHutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of \n\nSocial Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann \n\nArbor, MI, June 2014. \n\nC. (z.d.). cjhutto/vaderSentiment. GitHub. https://github.com/cjhutto/vaderSentiment#demo-\n\nincluding-example-of-non-english-text-translations \n\nV. (z.d.-b). vaderSentiment/vaderSentiment-js. GitHub. Geraadpleegd op 18 maart 2021, van \n\nhttps://github.com/vaderSentiment/vaderSentiment-js \n\n \n\n \n\nhttps://github.com/cjhutto/vaderSentiment#demo-including-example-of-non-english-text-translations\nhttps://github.com/cjhutto/vaderSentiment#demo-including-example-of-non-english-text-translations\nhttps://github.com/vaderSentiment/vaderSentiment-js\n\n"
        },
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearch proficiency analysis\n\n\n   \n \n\n   \n \n\n \n\n  \n2021 \n\nResearch proficiency \nanalysis \n\nRESEARCH THE POSSIBILITY OF ANALYSING TEXT PROFICIENCY \n\nNIEK VAN DAM \n\n\n\nResearch proficiency analysis  Niek van Dam \n\n \n1 \n\nVERSION CONTROL \n\n \n\nVersion Author Date Adjustments \nV0.1 Niek van Dam   Initial document setup \nV0.2 Niek van Dam 03/03/2021 Added research \nV1.0 Niek Van Dam 29/03/2021 Implementing new \n\nfeedback \n \n\n \n\nTABLE OF CONTENTS  \n\nVersion Control ........................................................................................................................................................ 1 \n\nIntroduction ............................................................................................................................................................. 2 \n\nResearch questions .............................................................................................................................................. 2 \n\nResearch ................................................................................................................................................................... 3 \n\nPaper 1: Predicting Proficiency levels .................................................................................................................. 3 \n\nAlternatives .......................................................................................................................................................... 3 \n\nAnalysing proficiency based on grammar mistakes ......................................................................................... 3 \n\nCalculating the ARI (Automated readability index) ......................................................................................... 4 \n\nGunning fog index ............................................................................................................................................ 4 \n\nFlesch-Kincaid grade......................................................................................................................................... 4 \n\nPoC: Using translated texts ...................................................................................................................................... 5 \n\nDiscusison ................................................................................................................................................................. 2 \n\nWhy is there no PoC regarding research paper ................................................................................................... 2 \n\nFinal thoughts ........................................................................................................................................................... 3 \n\nPaper 1 ................................................................................................................................................................. 3 \n\nARI ........................................................................................................................................................................ 2 \n\nGunning Fog Index ............................................................................................................................................... 2 \n\nFlesch-Kincaid ...................................................................................................................................................... 2 \n\nConclusion ................................................................................................................................................................ 1 \n\nSources ..................................................................................................................................................................... 2 \n\n \n\n  \n\n\n\nResearch proficiency analysis  Niek van Dam \n\n \n2 \n\nINTRODUCTION \n\nWithin the Quantified Student project, we thought of a new feature where we would analyse the written text \n\nfrom the students and detect whether their proficiency level in that language is up to standards. Before we can \n\ngo into creating PoCs about this new feature, we first must research whether this is even feasible.  \n\nThe research will be conducted by incorporating Literature study as main research method.  \n\nRESEARCH QUESTIONS \n\nThe main question I want to get answered in this research is the following: \n\n- Is it possible to analyse someone\u2019s language level based using NLP? \n\n- Is the approach that is proposed feasible? \n\n- Is the approach able to be implemented for the target languages (Dutch and English)? \n\n  \n\n\n\nResearch proficiency analysis  Niek van Dam \n\n \n3 \n\nRESEARCH \n\n \n\nPAPER 1:  PREDICTING PROFICIENCY LEVELS \n\nThis paper covers the measuring of proficiency levels by comparing the linguistic complexity to that of a \n\ntextbook at the given level. The language used in the textbooks is recorded per proficiency level, measured by \n\nthe CEFR (Common European Framework of Reference for languages). This level goes from A to C, respectively \n\nbeing ranked from basic to proficient user. These grades can further be subdivided according to the needs of \n\nthe local context. The idea behind this research method is that the model can read the input data from the \n\nstudent, normalize it to its basal form and compare the formatted text to the proficiency level. \n\nThe focus within this research paper was put on the correcting and normalizing of grammar mistakes, which \n\nled to greater performance within the network. This gave it a better accuracy and comprehension level of the \n\ntext it is analysing. The reason why it focusses so much around the cleaning of data is because the method \n\napplied in the paper has a really narrow error margin, making it underperform significantly on poorly prepared \n\ndata.   \n\nThere is a possibility of implementing this into FeedPulse, as the comparison of languages against a textbook \n\ncan be done in every language. There is an argument to be made regarding the workload this would bring with \n\nit, but the possibility remains.   \n\n \n\nALTERNATIVES \n\nDuring my research I saw a lot of instances where the NLP model used was way over the top for our \n\nimplementation, which, although interesting, did not meet the requirements needed (mostly since the \n\napproach was not feasible and too avant-garde). There were some more basic approaches which were covered \n\nthroughout my research, which seemed more in-line with what we are trying to achieve. The most promising \n\nmethods have been noted down below. \n\n \n\nANALYSING PROFICIENCY BASED ON GRAMMAR MISTAKES  \n\nThe analysing of proficiency based on grammar mistakes is a more feasible goal than the methods which are \n\nproposed in the research papers. Mostly because the analysing of grammar mistakes is a lot easier to do and \n\ndoes not require the same amount of preparation to be done beforehand. One of the easiest ways to \n\nimplement this metric is by calculating the average rate of grammar mistakes per sentence. We can measure \n\nthese against a predefined scale, which can later be given back to the student as feedback.  \n\n \n\nThe reason this is easier to implement than one of these research papers is because we can incorporate pre-\n\nexisting software for this, and do not require us to build our own solution from scratch. \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\nResearch proficiency analysis  Niek van Dam \n\n \n4 \n\n \n\nCALCULATING THE ARI (AUTOMATED READABILITY INDEX) \n\nThe readability index is a simple but powerful formula which rates the readability of the given text. The \n\nformula only requires basic metrics, which can easily be extracted from the data. The formula is as follows: \n\n4.71 (4,71 (\n\ud835\udc36\n\n\ud835\udc64\n) + 0,5 (\n\n\ud835\udf14\n\n\ud835\udc60\n) \u2212 2 \n\nWhere c is the number of letters and numbers, w is the number of \n\nspaces, and s is the number of sentences (not necessarily the number \n\nof periods present in a piece of text).  \n\nThis formula outputs a number between 1-14, which can be used to \n\ncalculate the grade level at which the text has been written. The \n\nscores can be a decimal number, in which case they will be rounded \n\nup; causing a 10.1 to be interpreted as an 11. The meaning of the \n\nscores is in the table to the left, taken from the Wikipedia page \n\nregarding ARI. \n\n \n\n \n\n \n\n \n\nGUNNING FOG INDEX \n\nIn linguistics, the Gunning fog Index is a readability formula for English writing which estimates the years of \n\nformal education needed to understand the text on first reading. This method has been developed in 1952 by \n\nRobert Gunning and has been used since. The fog index is commonly used to confirm that text can be read \n\neasily by the intended audience and can also be used to indicate the level (proficiency) of your writing skills.  \n\nThe Gunning fog index is calculated with the following formula:  \n\n0.4[(\n\ud835\udc64\ud835\udc5c\ud835\udc5f\ud835\udc51\ud835\udc60\n\n\ud835\udc60\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc52\ud835\udc60\n) + 100 (\n\n\ud835\udc50\ud835\udc5c\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52\ud835\udc65 \ud835\udc64\ud835\udc5c\ud835\udc5f\ud835\udc51\ud835\udc60\n\n\ud835\udc64\ud835\udc5c\ud835\udc5f\ud835\udc51\ud835\udc60\n)] \n\nIn comparison to the ARI, we do not round this number up or down \n\nwhen checking the end score, if the number is a decimal, it should be \n\ntreated as \u2018in between\u2019 the levels. The results of this index can be seen \n\nin the table to the left.  \n\n \n\n \n\n \n\n \n\n \n\nFLESCH-KINCAID GRADE \n\nThe Flesch-Kincaid grading system is a widely used readability formula which assesses the approximate reading \n\nlevel of a given text. It has been developed by the US Navy who worked with the Reading Ease formula first but \n\nScore Age Grade level \n\n1 5-6 Kindergarten \n\n2 6-7 1st/2nd grade \n3 7-9 3rd grade \n\n4 9-10 4th grade \n\n5 10-11 5th grade \n\n6 11-12 6th grade \n\n7 12-13 7th grade \n\n8 13-14 8th grade \n\n9 14-15 9th grade \n\n10 15-16 10th grade \n\n11 16-17 11th grade \n\n12 17-18 12th grade \n\n13 18-24 College student \n14 24+ Professor \n\nFog Index Reader level by grade \n\n17 College graduate \n\n16 College senior \n\n15 College junior \n14 College sophomore \n\n13 College first-year \nstudent \n\n12 High school senior \n\n11 High school junior \n\n10 High school sophomore \n\n9 High school first-year \nstudent \n\n8 8th grade \n\n7 7th grade \n\n6 6th grade \n\nhttps://en.wikipedia.org/wiki/Automated_readability_index\n\n\nResearch proficiency analysis  Niek van Dam \n\n \n5 \n\nhas been converted to the analysis of reading levels instead of reading ease. The obsolete version still exists; \n\nhowever, it is not as clear in its grading as the current formula. \n\nThe formula uses the same principle as the Gunning fog index but uses different variables and therefore also \n\noutputs a different result. The result which this grade gives is more generalised, whereas the fog index is \n\nspecific enough to classify writer grades.  \n\nThe formula for this grade is as follows:  \n\n0.39 (\n\ud835\udc64\ud835\udc5c\ud835\udc5f\ud835\udc51\ud835\udc60\n\n\ud835\udc60\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc52\ud835\udc60\n) +  11.8 (\n\n\ud835\udc60\ud835\udc66\ud835\udc59\ud835\udc59\ud835\udc4e\ud835\udc4f\ud835\udc59\ud835\udc52\ud835\udc60\n\n\ud835\udc64\ud835\udc5c\ud835\udc5f\ud835\udc51\ud835\udc60\n) \u2013  15.59  \n\nWhere all variables describe the count of a property within the \n\ngiven text (words -> total words, syllables -> total syllables). \n\nThis formula results in a number between 0-18 and displays the \n\ndata in 6 different sections, in value pairs of 3.  \n\n \n\n \n\nPOC: USING TRANSLATED TEXTS  \n\nThe alternative solution: \u2018readability indexes\u2019 discussed above, all give a clear and direct grading of the text to \n\nbe analysed, but have one prominent requirement which could be tricky. This is regarding the input language. \n\nThe readability indexes are all made with the mindset of grading English texts. In theory these readability \n\nindexes can be modified to a desired language, as the majority of these readability indexes solely require the \n\namount of: syllables, words and sentences. Before we can reliably apply this to every language, I decided to \n\nvalidate it myself with the following pieces of text, found on the \u2018Dutch dummy text generator\u2019: \n\n\u201cIk ben makelaar in koffi, en \n\nwoon op de Lauriergracht. Het is \n\nmijn gewoonte niet, romans te \n\nschrijven, of zulke dingen, en het \n\nheeft dan ook lang geduurd, voor \n\nik er toe overging een paar riem  \n\npapier extra te bestellen, en het \n\nwerk aan te vangen, dat gij, lieve \n\nlezer, in de hand hebt genomen, \n\nen dat ge lezen moet als ge \n\nmakelaar in koffie zijt, of als ge \n\nwat anders zijt. Niet alleen dat ik \n\nnooit iets schreef wat naar een \n\nroman geleek, maar ik houd er \n\nzelfs niet van, iets dergelijks te \n\nlezen.\u201d (Max Havelaar - Multatuli)  \n\n \n\n \n\n \n\n \n\n\u201cDe volle maan, tragisch dien \n\navond, was reeds vroeg, nog in \n\nden laatsten dagschemer \n\nopgerezen als een immense, \n\nbloedroze bol, vlamde als een \n\nzonsondergang laag achter de \n\ntamarindeboomen der Lange \n\nLaan en steeg, langzaam zich \n\nlouterende van hare tragische \n\ntint, in een vagen hemel op. Een \n\ndoodsche stilte spande alom als \n\neen sluier van zwijgen, of, na de \n\nlange middagsi\u00ebsta, de avondrust \n\nzonder overgang van leven \n\nbegon.\u201d  (De Stille Kracht \u2013 Louis \n\nCouperus)  \n\n \n\n \n\n \n\n \n\n \n\n\u201cOnbegrijpelijk veel mensen \n\nhebben familiebetrekkingen, \n\nvrienden of kennissen te \n\nAmsterdam. Het is een \n\nverschijnsel dat ik eenvoudig \n\ntoeschrijf aan de veelheid der \n\ninwoners van die hoofdstad. Ik \n\nhad er voor een paar jaren nog \n\neen verre neef. Waar hij nu is, \n\nweet ik niet. Ik geloof dat hij naar \n\nde West gegaan is. Misschien \n\nheeft de een of ander van mijn \n\nlezers hem wel brieven \n\nmeegegeven. In dat geval hebben \n\nzij een nauwgezette, maar \n\nonvriendelijke bezorger gehad, \n\nals uit de inhoud van deze \n\nweinige bladzijden waarschijnlijk \n\nduidelijk worden zal.\u201d (Camera \n\nObscura \u2013 Hildebrand) \n\nThe method I used for testing these is by programming the readability indexes myself and putting these texts \n\nthrough it. I used google translate for the translation of the texts from Dutch to English, as their translation \n\nservice is \u2013 in most situations \u2013 of high quality. Once translated, I process both versions of the text through the \n\nreading indexes and return the grades. The grades have been noted in the table below, where the highest \n\nscoring language is indicated with green and the lower one with red.  \n\n \n\nhttps://projects.haykranen.nl/dummytekst/?sourcechoice=cameraobscura&nr=3&choice=paragraphs&Submit=Genereer+een+tekst\n\n\nResearch proficiency analysis  Niek van Dam \n\n \n1 \n\n Text sample 1 Text sample 2 Text sample 3 \n\n EN NL EN NL EN NL \nARI 11,46 13,54 17,04 19,29 6,62 8,86 \n\nFRE 66,40 42,89 40,62 13,73 61,27 44,69 \n\nFKG 15,14 11,45 19,66 16,16 7,65 10,18 \n\nGFI 52,06 52,81 53,93 53,68 44,97 45,35 \n\n \n\nThis data shows us that the reading samples do not return consistent data, as I expected would be the case. In \n\nour results we can see that the highest grading language and index varies from language to language, which \n\nshows inconsistency in the indexes. Following this logic we can hypothesise the following:  \n\nIf student a were to write a piece of Dutch text and student b  were to copy the same text but in English, \n\nstudent b would be graded harsher and have more nudges regarding the quality/readability while the two \n\ntexts are theoretically the same. \n\nThis situation is unwished for, therefore we cannot use the barebones reading indexes and use them on Dutch. \n\nThere are still a lot of variations on the reading indexes, mostly manipulated for different languages, implying \n\nthat there might still be reading indexes which could be applied to the Dutch language.   \n\n\n\nResearch proficiency analysis  Niek van Dam \n\n \n2 \n\nDISCUSISON \n\nAny further questions/inquiries can be discussed here. \n\n \n\nWHY IS THERE NO POC REGARDING RESEARCH PAPER  \n\nThe reasoning for not having a PoC regarding the research paper is because of the fact that the research is too \n\nbroad and comprehensive to actually realise in this short amount of time. We set out a time schedule for \n\nourselves to which we have to stick. When writing our own NLP solution and prepare this much data would \n\ntake far longer than the time available for this research. For this reason I have decided to not write a PoC, the \n\nconclusion of the document itself already gave us most of the important information already. \n\n  \n\n\n\nResearch proficiency analysis  Niek van Dam \n\n \n3 \n\nFINAL THOUGHTS \n\nAfter researching and reading every possible method, what are the final thoughts regarding this research and \n\neach individual method? \n\nPAPER 1 \n\n \n\nThis feature is not something which I think we should be focussing on right now, as this is supposed to be a \n\nsecondary feature, not something which we should direct all our time and resources towards. However, it is \n\ngood to keep in mind that these things can still improve performance within NLP.  \n\nWhen asking the question whether this is feasible to implement into our own systems, I would have to say no. \n\nThere are multiple reasons for this, the main one being that this method takes up a lot of resources and time \n\nto properly implement, which seems over-the-top for a small feature. Besides, this application has only been \n\ntested on one language (Swedish), which has an easier grammar system than the languages we would have to \n\nimplement this software into. In our implementation, we would have to teach the network two languages \n\n(assuming people fill in their FeedPulse in either English or Dutch), this adds another dimension of difficulty as \n\nthese languages have a more complex grammar system than the language used in the paper.  \n\nIn conclusion, I do not think this is a viable approach to use, however, it did explain a lot about the data \n\ncleaning process going on behind the scenes in NLP.  \n\nALTERNATIVES \n\n \n\nARI: \n\nThe ARI looks like a good metric to use to determine proficiency of the writer, without having to train a model \n\nto predict it for us. Since this is only a basic formula, it can more easily be implemented into the system, \n\nrequiring basic text analysis at best.  \n\nGunning Fog Index: \n\nIn conclusion, this index can be used within our application, besides one critical point. This index is only \n\napplicable to the English language, which goes against our target languages (Dutch and English). Therefore, we \n\nwould only be able to apply this to the English side of canvas.  \n\nFlesch-Kincaid: \n\nThis grade level has a lot more potential than the Gunning fog index, as it can be widely applied to every \n\nlanguage for as far as I can tell. This gives it significant pros over the other linguistical grading formulas. \n\n  \n\n\n\nResearch proficiency analysis  Niek van Dam \n\n \n1 \n\nCONCLUSION \n\nAfter reading a handful of academic papers and researching a wide array of language grading systems I can \n\nsafely say that the academic papers are above our reach. The methods which are being used are advanced \n\nenough to be branched into their own little project, which is over the top for the small feature we are trying to \n\nimplement. \n\nThe grading systems and readability indexes, however, show a more promising result. These grading systems \n\nare more generic, which respectively, leads to an easier implementation. Besides that, the data which is \n\nrequired to work for these formulas is significantly lower than what would be needed for the NLP approaches. \n\nWhere the NLP approaches would require several batches of textbook examples, combined with data cleaning; \n\nthe formulas only require primitive parameters which can be extracted from the text with relative ease. \n\nTherefore, I think that we should incorporate the grading systems into this feature, instead of using an NLP \n\nsolution. \n\nWhen choosing between the indexes, there is one which significantly stands out above the rest in terms of \n\nusefulness in my opinion: the Flesch-Kincaid grade. Where the other tools output a grade, which is specifically \n\nfocussed on the American schooling system, the Flesch grade can be represented in more \u2018global\u2019 terms of \n\ndifficulty. The downfall of these methods is in the fact that it is limited to one language, meaning that you will \n\nhave to develop unique reading indexes per language. After trying to fix this issue by using translated text, we \n\nstill got sub-par results which cannot be used in a professional environment. \n\nIn conclusion, I do not think it is feasible to use these proficiency analysis tools as is. Further research can be \n\ndone regarding language-specific reading indexes, but using one reading index for multiple languages is not \n\nfeasible and yields untrustworthy results. If we do get the possibility of realising these indexes, my preference \n\nwould go out to the Flesch-Kincaid grading system, combined with the ARI.  \n\n  \n\n\n\nResearch proficiency analysis  Niek van Dam \n\n \n2 \n\nSOURCES \n\nFlesch, R. (2016, July 12). How to write plain english. Www.Mang.Canterbury.Ac.Nz. \n\nhttps://web.archive.org/web/20160712094308/http://www.mang.canterbury.ac.nz/writing_guide/writin\n\ng/flesch.shtml \n\nKincaid, J., Fishburne, R., Rogers, R., & Chissom, B. (1975, January). Derivation of new readability formulas \n\n(automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel \n\nUniversity of Central Florida. \n\nhttps://stars.library.ucf.edu/cgi/viewcontent.cgi?article=1055&context=istlibrary \n\nThe gunning fog readability formula. (2021). Readabilityformulas.Com. \n\nhttps://www.readabilityformulas.com/gunning-fog-readability-formula.php \n\nUniversity of Gothenburg Sweden. (2016, November). Predicting proficiency levels in learner writings by \n\ntransferring a linguistic complexity model from expert-written coursebooks (No. 1). \n\nhttps://www.aclweb.org/anthology/C16-1198.pdf \n\n \n\n\n\tVersion Control\n\tIntroduction\n\tResearch questions\n\n\tResearch\n\tPaper 1: Predicting Proficiency levels\n\tAlternatives\n\tAnalysing proficiency based on grammar mistakes\n\tCalculating the ARI (Automated readability index)\n\tGunning fog index\n\tFlesch-Kincaid grade\n\n\n\tPoC: Using translated texts\n\tDiscusison\n\tWhy is there no PoC regarding research paper\n\n\tFinal thoughts\n\tPaper 1\n\tAlternatives\n\n\tConclusion\n\tSources\n\n"
        },
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm Frequency - Inverse Document Frequency Quantified Student\n\n\nRuben Fricke\n\nTerm Frequency - Inverse Document Frequency\nQuantified Student\n\n\n\nTable of Contents\n\nTable of Contents 2\n\nVersion control 3\n\nIntroduction 5\n\nResearch questions 6\nMain question 6\nSub-question 6\n\nResearch method 6\n\nWhat is TF-IDF 7\nDefinition 7\n\nHow does TF-IDF work? 8\nProof of Concept 10\n\nWhat are the differences with common alternatives? 12\nKeyness 12\nTopic models 12\nAutomatic text summarization 12\n\nWhat are common usages for tf-idf 13\nVisualization & Exploratory 13\nSimilarities 13\nPre-processing 13\n\nIs TF-IDF the best solution in the requested context? 14\n\nConclusion 15\n\nSources 16\n\n2\n\n\n\nVersion control\n\nVersion Author Date Changes\n\n1.0 Ruben Fricke 11-03-2021 Initial document setup\n\n1.1 Ruben Fricke 12-03-2021 How is it calculated\n\n1.2 Ruben Fricke 25-03-2021 Added proof of concept\n\n3\n\n\n\nDependency matrix\n\nVersion Date Dependency\n\n1.2 25-03-2021 Research topic analysis\n\n4\n\n\n\nIntroduction\n\nFor the Quantified Student project, we had a meeting with Eric Slaats. Together we came up\n\nwith a feature that would create some sort of word web giving insight into your skills. It\n\nwould use NLP to analyze your assignments, feed pulse, KPI-matrixes, and other data on\n\nyour individual course to find out what your skills are and at what level these skills are.\n\nFirst application of this feature would be to give more insight into your own progression\n\nand skills. It would be nice to know what topics you have experience in. Another application\n\nfor this feature could be to ask questions to the right person. For example, you\u2019re stuck on\n\na python hosting problem and you already asked a few people and they could not help you\n\nany further. With this feature, the correct student that has knowledge could be found. But\n\nnot only students, but this is also applicable for teachers. Teachers that have more\n\nknowledge about certain topics could give better feedback on a certain assignment on this\n\ntopic. This way, you bring knowledge to the correct place.\n\nAs part of this, research should be done about Term Frequency - Inverse Document\n\nFrequency to see what this means, if we can apply this in this context and to see if it\u2019s\n\napplicable.\n\n5\n\n\n\nResearch questions\n\nMain question\n\nThe main question for this research is as followed:\n\n\u201cHow could TF-IDF contribute to giving more insight into the student\u2019s skill level at their topics\u201d\n\nSub-question\n\nTo answer this question, we have to answer the following sub-questions.\n\n1. What is TF-IDF?\n\n2. How does TF-IDF work?\n\n3. What are the differences with common alternatives?\n\n4. What are common usages for tf-idf\n\n5. Is TF-IDF the best solution in the requested context?\n\nResearch method\n\nThe following research has been performed with the help of the DOT-framework. This\n\nprovides convincing and thorough solutions, research will be performed in several\n\nresearch-areas depending on the question.\n\nTo answer the subquestions, a lot of new knowledge needs to be acquired. Therefore, a big\n\npart of these subquestions will exist in the literature research-area.\n\nAlso, the stakeholder (Eric Slaats) should give fiat to my proposal and a proof of concept will\n\nbe built. This combines different research areas.\n\n6\n\n\n\nWhat is TF-IDF\n\nDefinition\n\nTerm Frequency - Inverse Document Frequency (tf-idf) is a natural language processing and\n\ninformation retrieval method.\n\nIt has many uses, most importantly in automated text analysis, and is very useful for\n\nscoring words in machine learning algorithms for Natural Language Processing (NLP).\n\nTerm Frequency - Inverse Document Frequency was introduced in a 1972 paper by Karen\n\nSp\u00e4rck Jones under the name \u201cterm specificity. It was invented for document search and\n\ninformation retrieval. It works by increasing proportionally to the number of times a word\n\nappears in a document but is offset by the number of documents that contain the word\n\ninstead of by its raw frequency (number of occurrences) or its relative frequency (term\n\ncount divided by document length). So, words that are common in every document, such as\n\nthis, what, and if, rank low even though they may appear many times since they don\u2019t mean\n\nmuch to that document in particular.\n\nSo, the overall effect of this weighting scheme is to avoid a common problem when\n\nconducting text analysis: the most frequently used words in a document are often the most\n\nfrequently used words in all of the documents. (What is TF-IDF?, 2019, Scott, 2019)\n\n7\n\n\n\nHow does TF-IDF work?\n\nA term might be:\n\n1. Frequently used in a language like English, and especially frequent or infrequent in\n\none document\n\n2. Frequently used in a language like English, but used to a typical degree in one\n\ndocument\n\n3. Infrequently used in a language like English, but distinctly frequent or infrequent in\n\none document\n\n4. Infrequently used in a language like English, and used to a typical degree in one\n\ndocument\n\nTo understand how words can be frequent but not distinctive, or distinctive but not\n\nfrequent, let\u2019s look at a text-based example.\n\nRank Term Count\n\n1 The 32\n\n2 she 23\n\n3 python 20\n\n4 TF-IDF 4\n\n5 also 3\n\nWhen you use tf-idf term weighting, this new list will get created\n\nRank Term Count\n\n1 TF-IDF 35\n\n2 python 15\n\n3 she 6\n\n4 The 3\n\n5 also 3\n\n8\n\n\n\nTf-idf can be implemented in many different ways. In this report I will focus on a calculation\n\nthat parallels Scikit-Learn\u2019s tf-idf implementation. This is a popular way and later I will use\n\nthat implementation to create my proof of concept.\n\n\ud835\udc56\ud835\udc51\ud835\udc53\n\ud835\udc56\n\n= \ud835\udc59\ud835\udc5b[(\ud835\udc41 + 1)/\ud835\udc51\ud835\udc53\n\ud835\udc56\n] + 1\n\nThe above formula is the most direct formula to calculate the inverse document frequency\n\nfor each term. N represents the total number of documents that are available. Many\n\nimplementations normalize the results. Scikit-Learn\u2019s implementation represents N as N+1,\n\ncalculates the logarithm of (N+1)/dfi and finally adds 1.\n\nOne's idfi is calculated, tf-idfi is tfi multiplied by idfi. This results in the following formula.\n\n\ud835\udc61\ud835\udc53 \u2212 \ud835\udc56\ud835\udc51\ud835\udc53\n\ud835\udc56\n\n= \ud835\udc61\ud835\udc53\n\ud835\udc56\n \ud835\udc65 \ud835\udc56\ud835\udc51\ud835\udc53\n\n\ud835\udc56\n\n(Lavin, 2019; What is TF-IDF?, 2019)\n\n9\n\n\n\nProof of Concept\n\nI created a proof of concept to validate the functionality of this algorithm. I downloaded a\n\nset of articles for testing the tf-idf functionality. I store all the text files in a variable (line 4).\n\nIn line 18 I create a vectorizer. I use this to convert documents from a list of strings to tf-idf\n\nscores. I had to specify a few parameters. The first parameters are the max_df and min_df\n\nparameter. These parameters control the minimum number of documents a term must be\n\nfound in to be included and the maximum number of documents a term can be found in\n\norder to be included. Setting max_df below 0.9 will most of the time remove all the\n\nstopwords.\n\n10\n\n\n\nThe next parameter is the stop_words parameter. In my code snippet I used\n\nstopwords=None but stopwords=\u2019english\u2019 is also available. This will filter out words using a\n\npreselected list of high frequency function words as \u2018to\u2019 and \u2018of\u2019. In my example I did not use\n\nthis because I tested this on multiple languages. Therefore, to keep this proof of concept\n\nsimple to comprehend I did not specify this (in our topic modeling proof of concept we\n\nused another way to remove stopwords).\n\nAnd the last parameter I specified is the norm parameter. This is used to affect the range of\n\nnumerical scores that the tf-idf algorithm outputs.\n\nAnd finally, from line 30 I will output all the results in its own file. This results in a list of\n\nwords and their according score.\n\n(Scott, 2019, Scikit Learn TfidfVectorizer, n.d.)\n\n11\n\n\n\nWhat are the differences with common alternatives?\n\nTf-idf can be compared with several other ways to rank important term features in a\n\ndocument or collection of documents.\n\nKeyness\n\nAlright, the first alternative we will look into is keyness. Keyness is a broad term for a set of\n\nstatistical measurements that attempt to indicate the numerical significance of a term to a\n\ndocument or a set of documents in comparison with a larger set.\n\nTopic models\n\nTopic modeling and tf-idf are radically different, however online you can read that\n\nnewcomers often just want to run topic modeling on a corpus as a first step and in some\n\ncases running tf-idf would be preferable. Tf-idf is appropriate if you are looking for a way to\n\nget a bird\u2019s eye view of your corpus. Topic models can help explore corpora and they have\n\nseveral advantages. They suggest broad categories or communities of text, this is a general\n\nadvantage of unsupervised clustering methods. Topic models are good because documents\n\nare assigned scores for how well they fit each topic, and because topics are represented as\n\nlists of terms, this provides a good sense of how terms relate to the groupings.\n\nAutomatic text summarization\n\nThe final alternative I will look into is text summarization, for example TextRank. TextRank\n\nand tf-idf are different in their approach to retrieve information. Although, the goal of both\n\nalgorithms has overlap.\n\n(Lavin, 2019)\n\n12\n\n\n\nWhat are common usages for tf-idf\n\nTf-idf is mainly used for information retrieval and weighting term frequencies against\n\nnorms in a larger corpus. Tf-idf is suited for a particular set of tasks, these tasks usually fall\n\ninto one of the following groups.\n\nVisualization & Exploratory\n\nA common usage for tf-idf is as an exploratory tool or visualization technique. Terms lists\n\nwith tf-idf scores for each document that is available can be a strong interpretive aid on its\n\nown, they can help generate hypotheses or research questions. Word lists can also be used\n\nfor browsing strategies and visualizations. (Stray, 2010, Duplan, 2019)\n\nSimilarities\n\nOther use cases for tf-idf are tasks involving textual similarity, because tf-idf will produce\n\nhigh scores for terms related to the topic of the text. A search index can be performed\n\nusing tf-idf and return results to users searched by looking for documents with the highest\n\nsimilarity to the user\u2019s search string. (Scott, 2019)\n\nPre-processing\n\nFinally, another good application for tf-idf is pre-processing. Features that have been\n\ntransformed using tf-idf tend to have more predictive values compared to raw term\n\nfrequencies. This is especially the case when you use a supervised machine learning model\n\nbecause it tends to increase the weight of topic words and reduce the weight of high\n\nfrequency function words. (Lavin, 2019)\n\n13\n\n\n\nIs TF-IDF the best solution in the requested context?\n\nThe three use cases could all be used in our project, however I recommend to primarily use\n\nit for pre-processing and for visualizations.\n\nI do not recommend tf-idf for similarities. A use case for similarities for us could be to ask\n\nother students help on a topic they have more knowledge about. Tf-idf works at the word\n\nlevel, so a document that is about helmets might be far from a document about bikes when\n\nrepresented using tf-id. On the other hand, in topic analysis we\u2019ll be able to find groups of\n\nrelated words, which you can't do in tf-idf. This would work better for the specified use\n\ncase.\n\nPre-processing would be one of the best use cases for our project. As said in the last\n\nsection,  if you use a supervised machine learning model it tends to increase the weight of\n\ntopic words and reduce the weight of high frequency function words. This is a great\n\ncombination of two techniques we might use.\n\nAnd finally another use case is visualisation. Tf-idf is great for this. Tf-idf can get a selection\n\nof important words and is able to create a visualisation for this. In our project, this would\n\nbe a perfect option to create a wordweb. An easy implementation could be to use the\n\nWordCloud library to create a word cloud for a selection of the words.\n\n14\n\n\n\nConclusion\n\nFor creating a word web I can recommend using tf-idf. Tf-idf can be a perfect candidate for\n\ndoing this. In my research I can also conclude that tf-idf can also be used for\n\npre-processing, especially for supervised machine learning.\n\n15\n\n\n\nSources\n\nWhat is TF-IDF? (2019, May 10). MonkeyLearn Blog.\n\nhttps://monkeylearn.com/blog/what-is-tf-idf/\n\nScott, W. (2019, May 21). TF-IDF from scratch in python on real world dataset.\n\nMedium.\n\nhttps://towardsdatascience.com/tf-idf-for-document-ranking-from-scra\n\ntch-in-python-on-real-world-dataset-796d339a4089\n\nDuplan, S. (2019, November 6). Visualizing Topic Models with Scatterpies and\n\nt-SNE. Medium.\n\nhttps://towardsdatascience.com/visualizing-topic-models-with-scatterp\n\nies-and-t-sne-f21f228f7b02\n\nLavin, M. J. (2019, May 13). Analyzing Documents with TF-IDF. Programming\n\nHistorian.\n\nhttps://programminghistorian.org/en/lessons/analyzing-documents-wi\n\nth-tfidf\n\n16\n\n\n\nStray, A. (2010, December 10). A full-text visualization of the Iraq War Logs.\n\nJonathan Stray.\n\nhttp://jonathanstray.com/a-full-text-visualization-of-the-iraq-war-logs\n\nScikit learn TfidfVectorizer. (n.d.). Https://Scikit-Learn.Org/.\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.feature_extr\n\naction.text.TfidfVectorizer.html\n\n17\n\n\n"
        },
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantified Student \u2013 Topic Analysis \nMax van Hattum & Ruben Fricke \u2013  Delta \u2013  22-3-2021 \n\n\n\n \n1 \n\nTable of Contents \nDependency matrix .................................................................................................................................. 2 \n\nIntroduction.............................................................................................................................................. 3 \n\nResearch question .................................................................................................................................... 3 \n\nResearch-methods ................................................................................................................................... 3 \n\nWhat is Topic Analysis? ............................................................................................................................ 4 \n\nApplications for Topic Analysis............................................................................................................. 4 \n\nAdvantages and disadvantages ............................................................................................................ 4 \n\nTopic Modeling vs Topic Classification ................................................................................................. 4 \n\nHow does Topic Analysis work? ............................................................................................................... 5 \n\nTopic modeling ..................................................................................................................................... 5 \n\nLatent Semantic Analysis (LSA) ........................................................................................................ 5 \n\nLatent Dirichlet Allocation (LDA) ...................................................................................................... 6 \n\nTopic classification ............................................................................................................................... 7 \n\nRule-based system ............................................................................................................................ 7 \n\nMachine learning system ................................................................................................................. 7 \n\nIs Topic Analysis the right fit for the context? ......................................................................................... 8 \n\nDiscussion ............................................................................................................................................... 11 \n\nConclusion .............................................................................................................................................. 11 \n\nSources ................................................................................................................................................... 12 \n\n \n\n\n\n \n2 \n\nDependency matrix \nDependency Dependency version \n\nQuantified Student - TF-IDF research 1.2 \n\n  \n\n\n\n \n3 \n\nIntroduction \nQuantified Student wants to enable students to get more insight in their learning process. We want \n\nto achieve this by gathering and processing all kinds of data available through the digital learning \n\nenvironment. \n\nThis should allow students to understand their learning process and adjust where deemed necessary.  \n\nOne of these concepts is to be able to connect students with each other based on the assignments \n\nthey create. This will allow them to more easily get in contact with peers who already worked on a \n\nsimilar assignment. \n\nThis means that research needs to be done about Topic analysis, what it is, if it\u2019s applicable for our \n\ncontext and how it should be implemented. \n\nAls onderdeel hiervan moet er onderzoek gedaan worden naar Topic Analysis, wat dit inhoudt, of het \n\ntoepasselijk is en hoe het toegepast kan worden in de context. \n\n \n\nResearch question \nThe main questions is as follows: \n\nHow could Topic Analysis contribute to connecting students with each other based on \n\nsimilar assignments? \n\nTo be able to answer this question accurately, sub questions will be defined. It\u2019s important to get a \n\ngood understanding of what Topic analysis is, how it works and how it should be applied in the \n\ncontext.  \n\n- What is Topic Analysis? \n\n- How does Topic Analysis work? \n\n- Is Topic Analysis the right fit for the context? \n\n \n\nResearch-methods \n De onderzoeksmethoden wordt beschreven met de terminologie van het Dot framework. Om de \n\neerste twee deelvragen te beantwoorden moet er veel nieuwe kennis opgedaan worden. Voor een \n\ngroot gedeelte gaat dit onderzoek dus gebruik maken van literatuuronderzoek, wat valt binnen het \n\nbieb domein.  \n\nOm de derde deelvraag te beantwoorden moet er gebruik gemaakt worden van prototyping en \n\nvergelijkingen met andere oplossing. Ook moet er geverifieerd worden met de stakeholder (Eric \n\nSlaats) dat de oplossing past in de context. Hiermee raken we dus de werkplaats en veld domeinen. \n\nDoor deze combinatie van strategie\u00ebn wordt gegarandeerd dat er een goed overzicht van de kennis \n\nbereikt wordt en dat er met zekerheid een correct antwoord gegeven kan worden op de hoofdvraag. \n\n  \n\n\n\n \n4 \n\nWhat is Topic Analysis? \nTopic analysis is a machine learning technique that is used when great quantities of documents need \n\nto be organized based on their subjects. This is done by assigning these documents in \u2018buckets\u2019 \n\nwhere they are grouped with similar documents. \n\nTopic analysis used Natural Language Processing to understand the documents, then it recognizes \n\ncertain patterns and structures. It\u2019s not only used on a document level, but also for sentences or \n\neven parts of sentences. \n\n \n\nApplications for Topic Analysis \nIt can be applied immensely broadly, and even more so when combined with other techniques like \n\nsentiment analysis. First you could split up a document based upon topics, then you could apply \n\nsentiment analysis on singular topics. \n\nExamples of use cases like this are: analyzing product reviews, customer service complaints, SEO or \n\nsocial media. Basically, if a great quantity of documents need to be grouped together based on \n\nsubjects, but the set is to huge to be sorted through manually, then Topic Analysis is a fitting \n\nsolution.  \n\n \n\nAdvantages and disadvantages  \nAfter a model is trained, it will assign documents to categories consistently in the same way, you will \n\nalways get the same results. In contrary to assigning documents with humans, who will always judge \n\ndocuments in a different way. Each person has a slightly different way of classifying documents. \n\nMoreover Topic analysis is easily scaled horizontally, and can be applied in real-time. \n\nHowever there are also several disadvantages, the training of a model can be very hard, and Topic \n\nAnalysis is inherently prone to overfitting. This means that it performs very well on the training data, \n\nbut not so much on real, unknown for the model, data. On top of that is that the model does not jibe \n\nwell with documents that do not fit in the previously (during training) established categories. \n\n \n\nTopic Modeling vs Topic Classification \nThere are two most used ways to approach Topic Analysis, these are Topic Modeling and Topic \n\nClassification. Topic modeling uses a unsupervised way of machine learning, this means that the \n\ndataset does not have to be labeled beforehand, the model will define categories on its own. \n\nTopic Analysis is trained with prelabeled data, where all the documents that are used for training are \n\nalready assigned to a category. \n\nWhen comparing these two methods with each other it becomes clear that Topic Modeling is more \n\nflexible and can be set up quickly, while Topic Analysis is harder to set up. This is because there are \n\nmore requirements for the data to meet and the categories need to be defined from the beginning. \n\nHowever, when Topic Analysis is set up, it is more accurate than Topic Modeling \n\n \n\n \n\n\n\n \n5 \n\nHow does Topic Analysis work? \nThe two most common approaches for topic analysis with machine learning are NLP topic modeling \n\nand NLP topic classification. \n\nThe topic modeling variant is an unsupervised machine learning technique while topic classification is \n\nsupervised.  \n\nTopic modeling \nTopic analysis models can detect topics in a text with advanced machine learning algorithms that \n\ncount words and find and group similar word patterns. \n\nTo better understand the ideas behind topic modeling, we must understand the two most popular \n\nalgorithms: LSA and LDA. \n\nLatent Semantic Analysis (LSA) \nLatent Semantic Analysis is seen as the \u2018standard\u2019 method for topic modeling. This method is based \n\non the distributional hypothesis principle. Basically, this means that words and expressions that \n\noccur in similar pieces of text will have similar meanings. \n\nIt is based on the word frequencies of the dataset. The general idea is that for every word in each \n\ndocument, you can count the frequency of that word and group together the documents that have \n\nhigh frequencies of the same word. \n\nAlthough, this approach is limited, so many times tf-idf is used here. In our TF-IDF research we look \n\ninto the application of tf-idf to see if it's applicable for our use cases. After doing the word frequency \n\ncalculation we are left with a matrix that has a row for every word and a column for every document. \n\nEach cell is the calculated frequency for that word in that document. This is the document-term \n\nmatrix.  \n\nIn this matrix we must extract a document-topic matrix and a term-topic matrix which relate to the \n\ntopics and term topics. This can be done using singular value decomposition (SVD).  \n\n(Wikipedia contributors, 2021) \n\n \n\n  \n\n\n\n \n6 \n\nLatent Dirichlet Allocation (LDA) \nImagine a fixed set of topics. Each topic is represented by an unknown set of words. These are the \n\ntopics that our documents cover, but we do not know these topics are.  \n\nLDA tries to map all the documents to the topics in such a way that the words in each document are \n\nmostly covered by these topics. \n\nLDA and LSA both use the same fundamental assumption: \u2018Documents with the same topic will use \n\nsimilar words.  \n\nLDA assumes documents are generated in a mixture of topics and then picks words that belong to \n\nthose topics. These words are picked randomly according to how likely they\u2019re to appear in a \n\ndocument. \n\nSo, LDA sees a document and then works backwards from the words that make up the document and \n\ntries to guess the mixture of topics that resulted in that order of words. \n\nThe implementation has two parameters for training (alpha and beta). Alpha is used to control the \n\nsimilarity of documents. So, this means that when alpha is low, this represents documents as a \n\nmixture of few topics and a high alpha more topics. Beta is the same but used for topic similarity. A \n\nlow value will represent topics as distinct. A high value results in topics containing more words. \n\nThese values must be specified before the training starts.  \n\n(Wikipedia contributors, 2021b, Dwivedi, 2019) \n\n  \n\n\n\n \n7 \n\nTopic classification \nIn contrast to top modeling, with topic classification you already know what the topics are. Unlike the \n\nalgorithms for topic modeling, the machine learning algorithms are supervised. This means you need \n\nto feed them documents already labeled by topic to learn the algorithm how to label new unseen \n\ndocuments with these topics.  \n\nBut how would label topics for documents with a different issue? Well, if you\u2019re looking to automate \n\nsome already existing tasks, then you probably have a good idea about the topics of the text. \n\nOtherwise, you could use the topic modeling methods to better understand the content beforehand. \n\nSince automated classification always involves a first step of manually analyzing and tagging texts, \n\nyou usually end up updating your topics.  \n\nRule-based system \nIt\u2019s important to note that it\u2019s possible to build a topic classifier entirely without machine learning. \n\nYou do this by programming a set of rules based on the content of the documents that a human \n\nexpert read. The idea is that the rules represent the knowledge of the expert. Each one of these rules \n\nconsists of a pattern and a prediction.  \n\nRule-based systems are comprehensible for humans. Humans can read these rules, update existing \n\nrules and add new ones.  \n\nThis sounds great right? But there are some disadvantages. Remember that I used the word \u2018human \n\nexpert'? This is necessary because the system requires deep knowledge of the topic. This can take a \n\nlot of time and can be expensive. Besides that, although I said human can update it, it\u2019s hard to do so. \n\nRule based systems are hard to maintain and don\u2019t scale easily. Adding new rules will affect the \n\nperformance that were already created.  \n\nMachine learning system \n\nSo, let\u2019s continue to machine learning classification. With machine learning classification examples of \n\ntext and expected categories are used to train the model. This model learns to recognize patterns an \n\ncategorizes the text in categories. To train a model, you must create data that the model can \n\ncomprehend, in other words, you must convert the data to vectors. At this moment they are fed to \n\nthe algorithm which uses these data to create a model that can classify text.  \n\nTo categorize new pieces of text, the model will convert this new text to vectors, extracts important \n\nfeatures and makes the prediction.  \n\nThe model can be improved by keep training it with more and more data. Another thing that can be \n\nused to improve the outcome of the model is changing the training parameters.  \n\n(Topic Analysis: a comprehensive guide to detecting topics from text, z.d.) \n\n\n\n \n8 \n\nIs Topic Analysis the right fit for the context? \nTo ascertain that Topic analysis is the right fit for the context, a POC will be set up. Since for the use \n\ncase the categories will not be known before hand, Topic Modeling is the method that is going to be \n\nused. The POC needs to prove that it is indeed feasible to train a model that can accurately \n\ndetermine topics based on a collection of documents. \n\nThe data that is used to train the model is a collection of news articles. First text preproccesing will \n\nbe done, namely removing whitespace, punctuation and stop words. Then TF-IDF is used to \n\ndetermine which words are important in a document, these words will be given extra weight. \n\nAlso, to check if the results are indeed correct, two word clouds will be made, one based on the titles \n\nand one based on the content of the articles. \n\nThere are several libraries that will be used, the most important ones are Pandas, Gensim, nltk, dfITF \n\nand pyLDAvis. Pandas is used for easily organize our data in dataframes. Gensim is used for creating \n\nthe Topic Modeling model, in combination with pyLDAvis which visualizes this model for us. Nltk and \n\ndfITF are both used for text preprocessing purposes. \n\n \n\nFrist we define some methods mostly for text preprocessing purposes, and also one for creating the \n\nword cloud. \n\n\n\n \n9 \n\n \n\nIn the main program we  read out the articles from a CSV file and store them in a dataframe. Then we \n\nuse the aforementioned text preprocessing methods to clean up  text.  Before we use TF-IDF to \n\ntransform the text, we first make word clouds of both the titles and contents of the articles, this \n\nallows us to get a feel of the subjects of the articles. \n\nAfterwards we remove stop words from the text, and give more weight to important words in a \n\ndocument with TF-IDF. Then we create a kind of library where we give each unique word an ID, which \n\nis used to create corpuses for the documents.  \n\nThen to create the model we pass in the corpus, the library of words with ID\u2019s, and the amount of \n\ntopics we want to extract.  \n\nLastly we use pyLDAvis  to visualize our model,  this is a interactive html file where the topic are \n\nclickable and are described by their most defining terms.\n\n\n\n \n10 \n\n \n\n\n\n \n11 \n\nDiscussion \nThis research could be improved by using a more fitting dataset for the POC. Now a dataset of \n\nprofessionally written news articles was used, however for the use case the documents will be \n\nwritten by students with less experience in writing. This could impact the performance of the model. \n\n \n\nConclusion \nTo conclude this research it can be stated with reasonable certainty that Topic Analysis would indeed \n\nbe a good fit for connecting students based on their assignments. Namely Topic Modeling, since it \n\nwould not be known beforehand to what category the assignment would belong to.  \n\nThen with Topic Modeling a model could be trained with a collection of assignments, then when a \n\nstudent creates a new assignment the model can be used to assign this new assignment to a topic. \n\nThe last step would be to then extract some of these similar other assignments with more details, \n\ne.g. student name and score.  \n\nWith this information the student with their freshly made assignment can now contact other \n\nstudents that already have made a similar assignment. \n\n\n\n \n12 \n\nSources \n \n\nTopic Analysis: a comprehensive guide to detecting topics from text. (z.d.). MonkeyLearn. \n\nhttps://monkeylearn.com/topic-\n\nanalysis/#:%7E:text=Topic%20analysis%20is%20a%20Natural,of%20unstructured%20\n\ntext%20every%20day. \n\nWikipedia contributors. (2021, 31 januari). Latent semantic analysis. Wikipedia. \n\nhttps://en.wikipedia.org/wiki/Latent_semantic_analysis \n\nWikipedia contributors. (2021b, maart 13). Latent Dirichlet allocation. Wikipedia. \n\nhttps://en.wikipedia.org/wiki/Latent_Dirichlet_allocation \n\nDwivedi, P. (2019, 27 maart). NLP: Extracting the main topics from your dataset using LDA in \n\nminutes. Medium. https://towardsdatascience.com/nlp-extracting-the-main-topics-\n\nfrom-your-dataset-using-lda-in-minutes-21486f5aa925 \n\n \n\nhttps://monkeylearn.com/topic-analysis/#:%7E:text=Topic%20analysis%20is%20a%20Natural,of%20unstructured%20text%20every%20day\nhttps://monkeylearn.com/topic-analysis/#:%7E:text=Topic%20analysis%20is%20a%20Natural,of%20unstructured%20text%20every%20day\nhttps://monkeylearn.com/topic-analysis/#:%7E:text=Topic%20analysis%20is%20a%20Natural,of%20unstructured%20text%20every%20day\nhttps://en.wikipedia.org/wiki/Latent_semantic_analysis\nhttps://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\nhttps://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925\nhttps://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925\n\n\tDependency matrix\n\tIntroduction\n\tResearch question\n\tResearch-methods\n\tWhat is Topic Analysis?\n\tApplications for Topic Analysis\n\tAdvantages and disadvantages\n\tTopic Modeling vs Topic Classification\n\n\tHow does Topic Analysis work?\n\tTopic modeling\n\tLatent Semantic Analysis (LSA)\n\tLatent Dirichlet Allocation (LDA)\n\n\tTopic classification\n\tRule-based system\n\tMachine learning system\n\n\n\tIs Topic Analysis the right fit for the context?\n\tDiscussion\n\tConclusion\n\tSources\n\n"
        }
      ]
    },
    "CVStageMaxvanHattum": {
      "hand-ins": [
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCV - Max van Hattum \n \n\nPersonalia \nIk ben Max van Hattum, vierentwintig jaar oud en studeer HBO ICT te \n\nFontys. Ik ben geboren en getogen in Tilburg, waar ik ook nog steeds \n\nwoon in een studiootje. Voordat ik aan het Fontys ging studeren, heb ik \n\nhet VWO afgerond en heb ik eerst drie jaar biomedische technologie \n\ngestudeerd aan het TU/e. \n\nNaast mijn studies ben ik een sociale kerel die graag met zijn vrienden \n\nafspreekt of uitjes onderneemt. Verder hou ik enorm van lezen, hier en \n\ndaar wat krachttraining en gamen (met vrienden).  \n\nIn professioneel verband zou ik mij willen beschrijven als iemand die \n\neffici\u00ebnt en doelgericht naar oplossingen zoekt voor een gegeven \n\nprobleem. Ik ben niet vies van een uitdaging, dit blijkt ook wel uit het feit \n\ndat ik deelneem aan het FHICT excellentietraject; Delta. Bij meerdere \n\nprojecten in dit programma ben ik ook projectleider.  \n\n \n\nWerk- en projectervaring \nHier en daar heb ik al ervaring op verscheidene vlakken opgedaan. In \n\nmijn jeugd heb ik als trainer gewerkt bij HC Tilburg, verder heb ik toen ook stage gelopen bij De \n\nBibliotheek Midden-Brabant. \n\nTijdens mij studie Biomedische technologie heb ik op vrijwilligers basis de website van \n\nVrouwenkamerkoor Cantilare opgezet en beheerd.  \n\nNu ben ik ICT & Open Learning aan het studeren met de focus op Software engineering. Ik heb de \n\nmogelijkheid gekregen om aan veel interessante projecten te werken, met name door mijn deelname \n\naan het excellentietraject Delta. \n\n \n\nHotspot App Strijp-t \nVoor het Delta traject ben ik projectleider bij het Hotspot app project voor Strijp-T. Deze applicatie \n\nmoet op een kaart verscheidene Hotspot weergeven, die informatie bevat over bijvoorbeeld \n\naanwezige bedrijven, interviews die hier gedaan zijn en andere gelegenheden. Bij dit project ben ik \n\nvanaf het begin betrokken geweest, de concepting fase tot nu de realisatie waar we iteratief mee \n\nbezig zijn.  \n\nBinnen dit project ben ik bezig geweest met een REST API in ASP.NET Core die migrations gebruikt \n\nom de database up to date te houden, het opzetten van de Vue.js front-end, het realiseren van het \n\ngoogle maps component, het opzetten van de CI/CD en het deployen van de applicatie met behulp \n\nvan Docker.  \n\n  \n\n\n\nQuantified Student \nQuantified student is een project wat door het verzamelen en analyseren van data studenten in staat \n\nwil stellen inzicht te krijgen in hun leerproces.  \n\nOp het moment ben ik projectleider van een groep die binnen dit mandaat onderzoek doet naar \n\nNatural Language Processing en Machine Learning en hoe dit toegepast kan worden voor Quantified \n\nstudent. Onderwerpen zoals Topic Analyse, Sentiment Analyse en het analyseren van \n\ntaalvaardigheidsniveaus komen hier bij kijken. \n\n \n\nDigital Excellence \nBinnen het Delta traject werk ik ook aan een project genaamd Digital Excellence. Dit is een platform \n\ndat mensen en projecten met elkaar moet verbinden. Dit project is opgezet in C# met ASP.NET Core.  \n\nBinnen dit project heb ik een message broker (RabbitMQ) opgezet om communicatie tussen \n\nmicroservices te faciliteren, verder heb ik een ElasticSearch stack opgezet die gebruikt wordt voor \n\neen project-recommendation system en voor het verbeteren van de zoekfunctionaliteiten.  \n\nOok heb ik binnen dit project te maken met de software architectuur en het deployen door gebruik \n\nvan Docker.  \n\n \n\nPSV Zwemclub Applicatie \nDe PSV Zwemclub had het probleem dat het een erg ingewikkeld proces was voor zwemmers om zich \n\nte kunnen registreren voor wedstrijden. Dit werd allemaal met de hand gedaan en bijgehouden door \n\nde administratie. De opdracht was om dit te vergemakkelijken, maar wel binnen vooraf \n\ngedefinieerde beperkingen. Er is namelijk al een internationaal data model genaamd Lenex wat \n\ngebruikt moest worden. \n\nVoor dit project heb ik een REST API met Java geschreven met behulp van Spring boot. Gerealiseerde \n\nfunctionaliteiten waren het inschrijven op wedstrijden, het beheren van deze inschrijvingen door \n\ncoaches en management en het importeren van wedstrijden vanuit Lenex files. \n\n \n\nMagazijn applicatie DualInventive \nVoor het bedrijf DualInventive heb ik in groepsverband een applicatie gerealiseerd die het reserveren \n\nvan, uitgifte van en teruggave van producten deed ondersteunen. Functionaliteiten zoals het \n\nbijhouden van logs van deze handelingen, exporteren van logs naar pdf formaat en het \n\nterplekke/online zetten van handtekeningen waren gerealiseerd. \n\n\n"
        }
      ]
    },
    "Deployingtheapplication": {
      "hand-ins": [
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStrijp-T  \u2013  Deployment \nMax van Hattum \u2013 Delta \u2013 23-3-2021 \n\nV0.2 \n\n \n\n  \n\n\n\nVersion control \nVersion Author Date Adjustments \nV0.1 Max van Hattum  Initial document setup \nV0.2 Max van Hattum  Introductie \n\ntoegevoegd het \nbedoelde publiek van \nhet document en doel \nvan het document. \nVerduidelijking bij \nCloud Service Provider \nwat de eisen precies \nzijn. \nUitbreiding \nDomeinnaam. \nOnder SSL het proces \nvan een certificaat \nbemachtigen \nbeschreven. \nInfrastructuur design \nnaar voren gehaald \nvoor beter beeld lezer. \nDocker CI/CD \nopmerking toegelicht. \nOnder Docker \ncredentials toegelicht. \n \n\n \n\n  \n\n\n\nInhoud \nVersion control ........................................................................................................................................ 2 \n\nIntroductie ............................................................................................................................................... 4 \n\nCloud Service Provider ............................................................................................................................ 4 \n\nAWS ..................................................................................................................................................... 4 \n\nHeroku ................................................................................................................................................. 4 \n\nGoogle Cloud ....................................................................................................................................... 5 \n\nDigitalocean ......................................................................................................................................... 5 \n\nKeuzes productie en test server .............................................................................................................. 5 \n\nDomeinnaam ........................................................................................................................................... 5 \n\nSSL............................................................................................................................................................ 6 \n\nInfrastructuur .......................................................................................................................................... 7 \n\nDocker ..................................................................................................................................................... 8 \n\n.Env ...................................................................................................................................................... 8 \n\nDocker-compose.yml ........................................................................................................................... 9 \n\nApache ................................................................................................................................................... 11 \n\nGoogle Maps Pricing .............................................................................................................................. 12 \n\nConclusie ............................................................................................................................................... 13 \n\n \n\n \n\n  \n\n\n\nIntroductie \nVoor Strijp-T was de opdracht om met een concept te komen die de kernwaarden van Strijp-T zou \n\ntentoonstellen. Na verscheidene concepten gepitcht te hebben werd het Hotspot App idee \n\nuitgekozen om te gaan realiseren. \n\nDeze app laat verscheidene interessante bezienswaardigheden, foto\u2019s van vroeger, interviews en \n\nbedrijven weergeven op een kaart, samengebundeld in Hotspots.  \n\nOm de app te realiseren is ervoor gekozen om de PWA richtlijnen te volgen en de user interface te \n\nbouwen met behulp van het Vue.js framework. Deze is dan gekoppeld aan een REST API, gebouwd \n\nmet ASP.NET Core en een MySql database. \n\nDit document is bedoelt voor het weergeven van mijn werkzaamheden, als bewijs voor mijn \n\ncompetenties. Daarnaast kan het gebruikt worden door andere studenten om meer inzicht te krijgen \n\nin het proces achter het deployen van de Strijp-T applicatie.  \n\nIn dit document worden de stappen beschreven die gezet moeten worden om de app te deployen \n\nnaar een test omgeving en naar een productie omgeving. Verder wordt er onderbouwt waarom \n\nverscheidene keuzes gemaakt zijn.  \n\nHet release proces wordt nog apart gepubliceerd in een los document, om op te leveren aan de \n\nstakeholder. \n\n \n\nCloud Service Provider \nVoor de test omgeving moet er mogelijkheid tot schaling zijn, de belangrijkste factor is dan de \n\nbandbreedte. Dit omdat er relatief weinig CPU en RAM intensieve processen plaatsvinden, maar er \n\nwel een mogelijkheid bestaat dat er een influx van gebruikers kan komen.  \n\nDe test server moet daarnaast gratis zijn, of gratis door studenten te gebruiken zijn. Tot slot moet er \n\nsprake zijn van gebruiksvriendelijkheid, zoals goede documentatie en eventueel handleidingen voor \n\nuse cases.  Voor de release server heeft performance een hogere prioriteit, verder is schaalbaarheid \n\nook van belang, ook met name weer bandbreedte. \n\nEr zijn verscheidene Cloud serviceproviders die gebruikt kunnen worden om apps te deployen. AWS, \n\nGoogle Cloud, Heroku, Azure en Digitalocean worden het meest genoemd.  \n\nAWS \nAWS wordt genoemd als de premium oplossing voor hosting van een enorm aanbod aan \n\nverschillende services, performance en schaalbaarheid. Dit is wel terug te zien in de prijs, ook \n\nworden als nadelen genoemd de User Interface en slechte gebruiksvriendelijkheid. Een kanttekening \n\nis wel dat het eerste jaar van AWS gratis is, plus dat er gebruik gemaakt wordt van een Pay-as-you-go \n\nmodel, waarbij je alleen betaald voor wat je gebruikt en er geen vast abonnementen zijn. \n\nHeroku \nHeroku is een service die gehost is op AWS, wat betekent dat je dezelfde stabiliteit en veiligheid kan \n\nverwachten. Het verschil is dat alles al voor je geconfigureerd is. Voor kleinschaligere apps is Heroku \n\neen goede keuze, de kosten van Heroku beginnen op 0 euro voor hobby projecten, waar een \n\nabonnement geschikt voor productie op 25 dollar per maand staat. \n\n\n\nGoogle Cloud \nGoogle Cloud is op dit moment minder uitgebreid qua aanbod van services dan AWS, maar is goed \n\naan het opkomen. Je begint met 300 dollar aan gratis credit, en veel services zijn gratis binnen een \n\nbepaald limiet op maand basis. Je gebruikt dezelfde infrastructure als Google zelf voor populaire \n\ndiensten gebruikt. \n\nDigitalocean \nDigitalocean is een provider die veel al geconfigureerde services aanbiedt. Ze focussen erg op \n\ngebruiksvriendelijkheid, zijn goedkoop en schaling is makkelijk toe te passen.  \n\nHier tegenover staat wel dat de configuratie minder flexibel is, je kan bijvoorbeeld alleen Linux based \n\nsystemen opzetten. Daarnaast profileren zij zich als een Infrastructure as a Service en niet een \n\nPlatform as a Service. Dit houdt in principe in dat zij verwachten dat de gebruiker alles zelf beheert. \n\nEen Platform as a Service zorgt ervoor dat de gebruiker alleen zich zorgen hoeft te maken over de \n\napplicatie en de data. \n\n \n\nKeuzes productie en test server \nAangezien de Strijp-t Hotspot app kleinschalig is, is de prioriteit voor zowel de test- als \n\nproductieserver dat het goedkoop is. Verder is de mogelijkheid om de brandbreedte omhoog te \n\nschalen een must voor de release server.  \n\nAangezien ik persoonlijk Infrastructuur KPI\u2019s wil aantonen, is het ook een vereiste dat de service een \n\nzogenaamde: \u201cInfrastructure as a Service\u201d is.  \n\nMet deze factors is de keuze voor Digitalocean een duidelijke voor in ieder geval de testserver, hier \n\nkomt boven op dat er een studenten aanbod is voor 100 euro aan gratis credit.  \n\nAangezien Delta niet aan het beheer van applicatie doet wordt er geen keuze gemaakt voor een \n\nproductieserver die langere tijd in de lucht zal zijn. Het advies is om de code van applicatie over te \n\ndragen aan het bedrijf dat ook de website van Strijp-T beheert. \n\n \n\nDomeinnaam \nVoor de testserver wordt een persoonlijk domeinnaam gebruikt, om kosten en registratielasten te \n\nbesparen.  \n\nOns advies voor de productieserver is om deze te koppelen aan een sub domein van het huidige \n\nStrijp-T.nl domeinnaam; e.g. app.Strijp-T.nl. Zo valt er mee te liften op de naamsbekendheid die Strijp \n\nal heeft, en kan het al bestaande SSL certificaat worden, mits deze voor sub-domeinen werkt.  \n\nAls alternatief zou het ook mogelijk zijn om een nieuw domein aan te schaffen, het nadeel hiervan is \n\ndat je dan dus niet mee kan liften op de naamsbekendheid van Strijp-T en er altijd een nieuw SSL \n\ncertificaat aangeschaft moet worden.. \n\n \n\n  \n\n\n\nSSL \nEen SSL-certificaat kan ook niet meer weg in de huidige tijd, daarom is het advies om de webserver \n\ndie de applicatie ter beschikking gaat stellen ook te configureren met een SSL-certificaat.  \n\nOm een SSL certificaat te bemachtigen is er bij Namecheap, met een studenten account, een gratis \n\naangeschaft. Om een certificaat request met private key en certificaat te genereren was de volgende \n\ntool gebruikt: https://decoder.link/csr_generator , na invullen van de domeinnaam en locatie \n\ngenereert deze alles voor je. \n\n \n\nLet op, noteer de private key en sla deze op in onderstaand formaat met de \u201c.key\u201d extensie , deze is \n\nnodig om het certificaat op de server te installeren. \n\n \n\n  --- Rest van de key --- \n\n \n\nVervolgens kon dit request gebruikt worden om het certificaat te activeren bij de leverancier waar \n\nhet SSL certificaat is aangeschaft. Na activeren leverde de leverancier de volgende drie files. \n\nhttps://decoder.link/csr_generator\n\n\n \n\nLees verder op in dit document onder kopje: Apache voor een voorbeeld van hoe dit certificaat \n\nvervolgens te installeren. \n\nInfrastructuur \nOnderstaand het design van de infrastructuur zoals hij opgezet gaat worden. Met Docker zijn twee \n\nnetwerken ge\u00efnstantieerd om de front-end en back-end gescheiden te houden. Verder is het front-\n\nend naar de localhost blootgesteld op poort 8080, en de REST API op poort 5000, de database wordt \n\nniet blootgesteld aan de localhost, deze mag alleen via de API benaderd worden. \n\nVervolgens route de reverse proxy requests op port 443 naar de desbetreffende services, als de URL \n\nde prefix \u201c/backend\u201d bevat naar de API, anders naar de front-end app. Bij deze reverse proxy is het \n\nSSL certificaat ge\u00efnstalleerd, die gebruikt wordt bij port 443 (https) \n\n \n\n \n\n\n\nDocker \nVoor development werd er al een docker-compose.yml bestand gebruikt om de back-end op te \n\nstarten, de api en database.  \n\nVoor productie wordt weer opnieuw gekozen om Docker te gebruiken. De voordelen hiervan zijn dat \n\nna eenmalige configuratie, de applicatie gemakkelijk en snel gedeployed kan worden als Docker \n\nge\u00efnstalleerd is. Verder is het ook makkelijker om later de applicatie op te schalen, bijvoorbeeld door \n\ngebruik van Docker Swarm.  \n\nDaarnaast wordt bij de back-end repository Docker al gebruikt bij de CI/CD pipeline, maar nu zou het \n\nook simpel zijn om dit op te zetten voor de front-end. Daarnaast kan in een later stadia de pipeline \n\naangepast worden om de gebouwde images te publiceren naar een private image repository en om \n\ndan direct de server met deze nieuwe versie up te daten. \n\nNadelen zijn dat er een stuk overhead is, Docker moet ge\u00efnstalleerd zijn en de containers moeten \n\nopgezet worden. \n\nDe configuratie van de applicatie wordt in een docker-compose.yml file gedaan, met behulp van \n\nenvironment variables die in een .env bestand staan. De code van de front- en back-end is zo \n\naangepast dat op basis van deze waardes de applicaties goed geconfigureerd worden. \n\n.Env \n\n \n\n\n\nDocker-compose.yml \n\n  \n\nZoals te zien schakelen we SSL uit in de REST API, dit omdat we SSL gaan configureren vanuit de \n\nwebserver die deze dockerapplicatie blootstelt naar de buitenwereld. Ook wordt er aangegeven dat \n\nCORS geconfigureerd moet worden, zodat het alleen werkt vanuit de front-end url. We stellen de API \n\nbeschikbaar lokaal op port 5000.  \n\nDe database is geconfigureerd om bij het voor het eerst op starten een nieuwe gebruiker aan te \n\nmaken speciaal voor de REST API. De username en password voor deze user worden gehaald uit \n\ngeheime environment variabelen. Deze environment variabelen worden ook gebruikt bij de API om \n\nde connectie met de database te maken.  \n\nOok binden we database data aan een lokaal volume, zodat deze behouden wordt ook als de docker \n\ncontainer offline gehaald wordt. Deze stellen we niet open om toegankelijk te zijn buiten het docker \n\nmysql-network dat geconfigureerd is. Zo kan de data alleen opgehaald worden door de REST API.  \n\nTot slot wordt de front-end opgestart en exposen we deze service op port 8080. \n\nOm dit draaiende te krijgen op een VPS moeten de code en Docker files als volgt op de server staan: \n\n\n\n \n\nWaarbij respectievelijk in de back-end en front-end de code van de master branches staat. \n\nHierna moeten  de commands \u201cdocker-compose build . \u201d en vervolgens \u201cdocker-compose up\u201d \n\nuitgevoerd worden. \n\nDit proces valt te verbeteren door het bouwen op een dedicated service te doen, waarna de images \n\ngepubliceerd worden naar een Container Registry. Deze zouden dan gespecificeerd kunnen worden \n\nin de docker-compose file, in plaats van ook het bouwen te doen in de docker-compose file. \n\nNu zijn de services lokaal draaiende op de VPS, de vervolg stap is om dit nu beschikbaar te stellen via \n\nhet gekozen domeinnaam. \n\n \n\n \n\n\n\nApache \nOm de services beschikbaar te stellen moet er \n\neen webserver gebruikt worden om een \n\nreverse proxy op te stellen.  \n\nApache is een veelgebruikt webserver en hier \n\nis veel documentatie van beschikbaar, vandaar \n\ndat hier de keuze op valt. \n\nVoor de huidige context gaat Apache gebruikt \n\nworden om de verbinding te encrypten met \n\nSSL en om requests op port 80 vanuit het \n\ndomeinnaam \u201cwww.maxvanhattum.me\u201d, \n\n\u201cmaxvanhattum.me\u201d en op port 443 vanuit \n\n\u201cmaxvanhattum.me\u201d te redirecten naar \n\n\u201chttps://www.maxvanhattum.me\u201d.  \n\nVervolgens worden requests op port 443 naar \n\n\u201cwww.maxvanhattum.me\u201d op basis van de \n\nURL geroute naar de bijbehorende service. Op \n\nde root wordt de front-end aangeboden, \n\nverder is de swagger documentatie te vinden \n\nop \n\n\u201cwww.maxvanhattum.me/backend/swagger/\u201d \n\nen de api aanspreekbaar op \n\n\u201cwww.maxvanhattum.me/api/\u201d . \n\n \n\n \n\n  \n\n\n\nGoogle Maps Pricing \nDe Google Maps gerelateerde services vallen onder een regeling waarbij een account begint met \n\n300$ aan credit uit te geven over de eerste 90 dagen en daarna maandelijks 200$ credit \n\n(https://developers.google.com/maps/billing-credits). \n\nDe services die gebruikt worden (of gaan gebruikt worden) zijn de Google Maps API, Geolocation, \n\nGeocoding en de Directions API. \n\nGoogle Maps API \n\n \n\n \n\nGeocoding \n\n \n\n \n\nDirections \n\n \n\n \n\n  \n\nhttps://developers.google.com/maps/billing-credits\n\n\nConclusie \nEr is gedemonstreerd dat de applicatie prima naar een test server gedeployed kan worden. Voor \n\nproductie kunnen dezelfde stappen genomen worden, alleen wel meer met het oog op prestatie en \n\nschaling. Het probleem is wel dat Delta niet aan beheer van applicaties doet, dus deployen naar \n\nproductie is lastig. Een oplossing zou zijn om het over te dragen aan het bedrijf dat ook de site van \n\nStrijp-T beheert. Een andere optie is om het te deployen met een account op naam van Strijp-T en de \n\nkosten terug te vragen bij Strijp-T, het nadeel hiervan is, is dat er geen langdurige ondersteuning zal \n\nzijn. \n\nVerder is er nog ruimte voor verbetering van dit proces, met name door CI/CD te implementeren en \n\ndoor een technologie te gebruiken die downtime vermijdt (Docker Swarm/Kubernetes).  \n\nVerder zouden de front-end en back-end ook op aparte server de deployed kunnen worden, waarbij \n\ner meer wordt gelet op wat de specifieke applicatie nodig heeft. De front-end zou bijvoorbeeld meer \n\nbandwidth nodig hebben, terwijl de api eerder betere processorkracht kan gebruiken. \n\nDe applicaties zijn al wel gemaakt met oog op deze uitbreidingen, doordat de applicaties gescheiden \n\nzijn en geconfigureerd kunnen worden doormiddel van environment variabelen. \n\n\n"
        }
      ]
    },
    "Deployment": {
      "hand-ins": [
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeploying with Docker \nMax van Hattum \n\n \n\nIntroduction \nFor Open Learning we get the opportunity to work for an external stakeholder, giving context to our \n\nlearning process. I have chosen to work on an application for PSV Swimming. The main features this \n\napp needs to offer are enabling swimmers to manage their event registrations, coaches to review \n\nthis registrations and administrators to see an overview of these registrations so they can process \n\nthem. \n\nNow the REST API must be deployed, research has been done for this, the advice is to use Docker. \n\n \n\nConfiguring Docker \nSince we need a database for the application to function, we need to bundle the API with a database. \n\nFirst, we create a Dockerfile to build a Docker image for the REST API. \n\n \n\nBasically, we build further upon the open source jdk11 image, copy the jar we built with maven of \n\nour API and then configure the starting point of the image. \n\nNow to  bundle it with the database, we create a docker-compose.yml file. Detailing how to \n\ncontainers should be bundled and how they should connect. \n\n \n\n\n\nNow this file might be a bit more complicated. \n\nWe start with determining which version we \n\nuse, 3 is the most recent. Then we define which \n\nservices need to run. \n\nWe define a mysql-database service, detailing \n\nwhich image it needs to use. \n\nThen, to configure the database we pass in \n\nenvironment variables. \n\nThen we configure a volume, where it should \n\nsave its data persistent (outside of the docker \n\ncontainer), so that we can keep the data \n\nthrough restarts.  \n\nLastly, we configure a network in the container \n\nit should use. \n\nNow we can go on to define the rest-api service.  \n\nWe let it restart on failure, since it depends on \n\nthe database service, but if the API start before \n\nthe database it will crash. \n\nThen we define the build conditions, setting the \n\ncontext to the current folder and pointing to \n\nthe Dockerfile we create earlier.  \n\nHere also we have defined environment \n\nvariables, so that it sets these values correctly \n\nfor use within Docker. These are used in the \n\napplication-docker.properties file in the API. \n\nThen we connect this service to the same network as the database, so they can communicate.  \n\nLastly, we expose the port so that the service can ben accessed from outside of the Docker container. \n\nNow that the services are defined, we are left with configuring the network and volume we use. This \n\nis quite simple as you can see. \n\n \n\nDynamic configuration \nSpring conveniently reads out an environment variable called: SPRING_PROFILES_ACTIVE. We can set \n\nthis and then it will use the set profile. See the naming of the files: \n\nNow in the regular development properties we hardcode \n\nthe values, but in the docker properties we retrieve the \n\nvalues from the environment variables.  \n\n  \n\n\n\nStarting the application \nNow starting the application is quite easy. Docker needs to be installed but then it is just a matter of \n\nrunning the following command in the root folder of the repository: \n\nDocker-compose up \n\nNow it will build the services and get them up and running, as seen below, the service is exposed on \n\nthe 5000 port. \n\n \n\n\n"
        }
      ]
    },
    "DeploymentResearchandAdvice": {
      "hand-ins": [
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\nPSV Swimming App Deployment \nFontys Open Learning Semester 3 \n\nMax van Hattum\n\n\n\n \n2 \n\nInhoud \nIntroduction ............................................................................................................................................. 3 \n\nDOT Framework approach ...................................................................................................................... 3 \n\nWhat .................................................................................................................................................... 3 \n\nHow ..................................................................................................................................................... 3 \n\nWhy ..................................................................................................................................................... 3 \n\nResearch Question .................................................................................................................................. 3 \n\nMain Question ..................................................................................................................................... 3 \n\nSub questions ...................................................................................................................................... 3 \n\nWhat are the details of the stakeholder\u2019s server? .................................................................................. 4 \n\nWhat services are already running on the server? ................................................................................. 4 \n\nHow are these services configured? ....................................................................................................... 4 \n\nWhat are ways of deploying the application? ......................................................................................... 4 \n\nConclusion ............................................................................................................................................... 5 \n\n \n\n  \n\n\n\n \n3 \n\nIntroduction \nFor Open Learning we get the opportunity to work for an external stakeholder, giving context to our \n\nlearning process. I have chosen to work on an application for PSV Swimming. The main features this \n\napp needs to offer are enabling swimmers to manage their event registrations, coaches to review \n\nthis registrations and administrators to see an overview of these registrations so they can process \n\nthem. \n\nAt the PSV Swimming club, they manage their own server and domain name. The task is to research \n\nhow the application could be best deployed. This contains the REST API, MySQL Database and the \n\nPWA. \n\n \n\nDOT Framework approach \n\nWhat \nThis research will mostly take place in the application domain. The current system needs to be \n\nresearched and understood. Afterwards techniques and methods on how to deploy an application \n\ncan be researched, touching on the Available Work domain. \n\nHow \nField research will be done to investigate the current situation. Key factors are where the server is \n\nhosted, what OS is running on it, what else is already running on it and how these services are \n\nconfigured. Afterwards some Library research will be done to investigate what methods on deploying \n\nare available and which are applicable for the current context. \n\nWhy \nWe focus heavily on the Field approach because we want to achieve the best fit possible for the \n\nsituation. However, since research will be done in the available work domain, it will also touch on \n\nExpertise. These two will provide the balance that is needed for a good and complete result.  \n\n \n\nResearch Question \n\nMain Question \nTo give guidance to this research we formulate a main question: \n\n\u201cHow should the application be deployed on the stakeholder their server?\u201d \n\nFor this question to be answered, multiple facets need to be researched. First, we need to investigate \n\nthe stakeholder\u2019s server, what services are already running on this server and how these are \n\nconfigured. Afterwards we need to research ways of deploying the application. Then we can answer \n\nthe main question. \n\nSub questions \n- What are the details of the stakeholder\u2019s server? \n\n- What services are already running on the server? \n\n- How are these services configured? \n\n- What are ways of deploying the application? \n\n\n\n \n4 \n\n \n\nWhat are the details of the stakeholder\u2019s server? \nThe server is a Virtual Private Server, hosted externally. This server is running a Linux distro: CentOS. \n\nThe address of the server is: 185.107.213.193 and there is an account available for Fontys Students. \n\nNow for security purposes the details will not be shared in this document.  \n\nThe server can be approach by SSH, the stakeholder was using PuTTY.  \n\n \n\nWhat services are already running on the server? \nCurrently the server hosts their website and a filesharing/cloud service. There is an Apache \n\nWebserver present, and a MariaDB service which contains a database for the filesharing/cloud \n\nservice. There are also remnants available of ghost projects, which are not exposed to the public. \n\n \n\nHow are these services configured? \nThe stakeholder has a domain name, and this is configured to point to the IP with certain \n\nports/prefixes/suffixes. Then it seemed to me that the Apache Webserver is configured with \n\nVirtualHosts to point different requests to the linked services. \n\n \n\nWhat are ways of deploying the application? \nThere are several ways on how to deploy the whole application. Lately Docker has risen in popularity \n\nsince it requires almost no configuration after the initial configuration has been done. \n\nTwo Docker files could be written, one for the PWA and one for the API. The API could then be \n\nbundled with a pre-configured MySQL database, with the help of a docker-compose.yml file.  \n\nThen these could be deployed together, again with a docker-compose.yml file or apart from each \n\nother.  \n\nThe other option is to build both separately, the API can be built with Maven, producing a JAR file. \n\nThen to setup a local database and make sure the configurations are the same as in the API. \n\nThe PWA can be built with NPM and then served with a Webserver, note that the webserver needs \n\nto have a fallback URL configured. This because of the way the routing works in Vue.js, if not you will \n\njust get 404 errors. \n\nBoth options have their merits, using Docker allows for very easy deployment and reduces the \n\ndifficulty of deploying plus the time that needs to be invested. \n\nNot using Docker and setting up everything separately means less bloat, but more time is needed for \n\ndeploying. Moreover, this is more difficult to achieve. \n\n  \n\n\n\n \n5 \n\nConclusion \nMultiple facets regarding the deployment of the software system have been touched upon now. The \n\nserver has been investigated, turning out to be an externally hosted VPS, running CentOS.  \n\nOn this server their website and a cloud service are running, these are facilitated by Apache \n\nWebserver and MariaDB.  \n\nThe domain name is configured to point to the server IP, where the Apache Webserver utilizes \n\nVirtualHosts to direct the request to the connected service. \n\nThe application could be deployed with Docker or separately. Separately would mean that everything \n\nmust be configured on its own, every time that the system needs to be deployed. \n\nUsing Docker would mean that the configuration is done once, by the software developer. \n\nAfterwards it is easier to deploy since it will just need one command to get everything up and \n\nrunning. \n\nTaking all this in mind, the advice will be to deploy the system with Docker. This will simplify the \n\ndeployment and save a lot of time. There will be a small bit of bloat, since Docker needs to be \n\ninstalled, but this is worth the save in time and the lesser difficulty of deployment. \n\n\n"
        }
      ]
    },
    "ElasticSearchPerformanceResearch": {
      "hand-ins": [
        {
          "text": "ElasticSearch Performance research\n\nBy: Max van Hattum & Niray Mak\n\n\nVersion\n\n\nDistribution\n\n\n\n\n\nIntroduction\n\nAfter implementing ElasticSearch into our system and creating a real time automatic migration system we found out that the performance dropped drastically. To find out what exactly caused this performance drop we started a research.\n\nArchitecture\n\nTo get an idea of the made implementation we added a C3 model of the created changes.\n\nEvery time a create, update, or delete is executed we use a so-called TaskPublisher to publish a task which is the synchronization to the ElasticSearch dataset. After the task is published the API can continue without waiting for the task. The actual synchronizing is done by a different application which is called ElasticSynchronizer.\n\n\n\n\n\nPerformance measurements\n\nPostman integration tests time\n\nTo find the issue we tracked the difference between the integration tests on the develop branch and our branch.\n\nResults Postman tests:\nResults of 3 runs on the develop branch\nTwo runs on branch 63-recommendationsystem\n\n\nIn 63-recommendationsystem we manually checked the time in ms for each request. We often see 700-900ms.\nIn the develop branch we also manually checked the time in ms for each request. We often see 10-100ms.\n\nThis concluded that the performance definitely dropped drastically since our changes.\n\nApproach\n\nTo find what causes this enormous difference we started looking into several things:\nCPU load\nAmount of queries to database made with their performance\nRAM usage\n\nVisual Studio Performance Profiler\n\nWe used Visual Studio\u2019s Performance Profiler to track what the issue was.\nThe Performance Profiler can track CPU load, Queries that the DBContext makes and RAM usage. Beneath are the results.\n\nCPU\nFirst, we checked for CPU load. A performance analysis session from the build-in Visual studio tool was started, after start-up three iterations of the integration tests were run. From the results it can be concluded that the problem lies not with a lack of CPU power, no huge spikes were found.\n\n \nQueries\nThe performance profiler also has an option to record all queries done by the DBCONTEXT, it shows the timestamp of when the query was executed, plus the execution time.\n Some showed [Unknown] in the Duration column, however quite some did show the execution time in ms. Many of them were showing 1-7ms, only a very few were showing more than 50ms. From this it can be concluded that this is not the origin for the response delay.\n\n \n\nRAM usage\nThen the RAM usage was checked. The performance profiler shows per timestamp how much in megabytes is stored.\nIn the results below (branch 63-recommendationsystem) you can see that it starts stable on +/- 135MB. Then after one iteration of Postman integration tests it climbs up to +/- 485MB. It stays stable till the next iteration was started, then it climbs up to 730MB.\n\n\nTo make sure this is the problem the same RAM usage test was run for the development branch.\nThree iterations of integration tests we\u2019re done, as shown, here the memory does stay stable. \nIn the beginning it started stable again around 135MB. Then after the first run it increases to 245MB. During the next runs the garbage collection reduces the amount MB\u2019s used and after the 3rd run, which returns the value back to 179MB. Below are the results of three iterations of integrations tests in the develop branch.\n\n\n\nWhen comparing these values, it can be concluded that the problem lies somewhere with the RAM usage.\nThe performance profiler for the RAM usage gives additional insight in the objects that are in the memory. The biggest size is used in BufferedStreams, which are used by RabbitMQ. \nTherefore it can be concluded that RabbitMQ does something that results in the creation of, or usage of, big objects.\n\n\nTroubleshooting\nKeeping in mind that the problem originates from RabbitMQ using a large amount of RAM, changes were made in the code. \nFirst the TaskPublisher related methods were commented out, then another iteration of integration tests was done. Total time was: 295077ms. It can be concluded that thus the problem does not lie with the TaskPublisher methods..\nThen the properties TaskPublisher and ElasticSearchRestClient were removed, also the dependency injection in the constructor was removed.\nThis resulted in a better performance: Total time was: 60299ms. Average time per request went up to 100-200ms\nTo then determine if the ElasticSearchRestClient or the TaskPublisher was at fault, the ElasticSearchRestClient was added again. Results after one iteration of integration tests: 64949ms.\nFrom this it can be concluded that the dependency injection for the Taskpublisher is at fault for the slow performance.\n\n\nThe dependency injection for the TaskPublisher was checked, it was created as a scoped service. The Taskpublisher also used a RabbitMQConnectionFactory to create the connection with the RabbitMQ service. This RabbitMQConnectionFactory also was a scoped service. \nBoth were changed into a singleton. This resulted in the performance improving. One iteration of integration tests had a total time of 16055ms.\nWe checked memory again for 3 iterations of integration tests. The results are shown in the image below.\n\nConclusion\n\nWe found out that for every request a new instance of both the RabbitMQConnectionFactory and TaskPublishers were created which caused the used memory to go up. Specifically the RabbitMQConnectionFactory slowed everything down substantially.  When thinking about this afterwards this makes sense, since every request, a new connection to the RabbitMQ service was created. This was not the desired result, since the connection only has to be established once, and then can be reused. This caused the memory usage to spike, since it saved these connections and it caused the response time to go up, because it reestablished a connection every request.\n\nWe refactored both to dependencies to be a singleton. Now these are created once on start up and then reused for every request and dependency injection.See images below for the lines that were changed.\n\n\n\n\n \n"
        }
      ]
    },
    "ImplementatieRecommendationSystem": {
      "hand-ins": [
        null
      ]
    },
    "ImplementatieRESTAPI": {
      "hand-ins": [
        null
      ]
    },
    "ImplementationNotificationSystem": {
      "hand-ins": [
        {
          "text": [
            "implementatie\n",
            "https://github.com/DigitalExcellence/dex-backend/pull/322\n",
            "\n",
            "testing:\n",
            "https://github.com/DigitalExcellence/dex-backend/pull/344"
          ]
        }
      ]
    },
    "Messagebrokerguide": {
      "hand-ins": [
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n1 \n\nDeX  - Message Broker Guides \nMax van Hattum \u2013 11-3-2020  \n\n\n\n \n2 \n\nTable of Contents \nMessage broker RabbitMQ ..................................................................................................................... 3 \n\nRegistering a \u201cSend email\u201d task .............................................................................................................. 4 \n\nCreating new channels and services ....................................................................................................... 5 \n\nUse Case .............................................................................................................................................. 5 \n\nPublishing the task to a new channel .................................................................................................. 5 \n\nConsuming packages from a channel .................................................................................................. 7 \n\nCreating the Worker Service ............................................................................................................... 7 \n\nConfig data .......................................................................................................................................... 8 \n\nDocker ............................................................................................................................................... 12 \n\nCode to execute on receiving a package ........................................................................................... 13 \n\n \n\n  \n\n\n\n \n3 \n\nMessage broker RabbitMQ \nThe message brokers use is extremely versatile, we use it to connect our different services with each \n\nother. Mainly the API registers tasks that needs to be done, for example sending an email because a \n\nuser needs to convert their account, or for syncing our databases.  \n\nThe message broker allows us to scale services horizontally, since it can distribute tasks to multiple \n\ninstances of a service at the same time. \n\nFor new use cases it\u2019s easy to implement extra services listening to the message broker.  \n\nThe workflow is typical like so: \n\n1. An endpoint in the API is called \n\n2. One or multiple tasks which are blocking need to be executed \n\n3. The API registers tasks to a specific channel (e.g., EMAIL) \n\n4. One or more services are listening to channels  \n\na. For example, three emailSender applications \n\n5. When a channel has packages, the message broker handles distribution to connected \n\nservices \n\na. The message broker is in charge of confirming tasks are executed, and evenly \n\ndistributing these to connected services. \n\n6. A service (e.g., a ASP.NET Core Worker Service) is subscribed and listening to a channel and \n\nreceives a package \n\n7. Code is executed based on the package \n\n8. Depending on if this code succeeds or fails, it acknowledges the processing of the package \n\n \n\n  \n\n\n\n \n4 \n\nRegistering a \u201cSend email\u201d task \nWithin DeX we use a message broker, RabbitMQ, to register tasks for other services to execute. This \n\nenables the API to focus on facilitating endpoint requests, while the tasks get executed in a non-\n\nblocking way.  \n\nTo send an email the RegisterTask method from the TaskPublisher.cs class can be used. This is class is \n\npresent in the IOC, facilitating dependency injection like so: \n\n \n\nIn the constructor of the class in which you want to publish a task, set the parameter ITaskPublisher \n\ntaskPublisher and assign this to a private property, so you can use it. \n\n \n\nNow you need to make an instance of the EmailNotificationRegister.cs, passing in the recipientEmail, \n\nthe textContent. \n\nBefore you can publish this data to the message broker, you need to serialize it into JSON. Then you \n\nspecify to which channel of the message broker the task should be published to, in this case it is \n\nSubject.EMAIL. \n\n \n\nWithin the DeX architecture, another service is listening to this channel, it now gets this data and is \n\ngoing to send an email with it. \n\n \n\n  \n\n\n\n \n5 \n\nCreating new channels and services \nSay you want to execute a certain action, but either this will be blocking the API, just takes a lot of \n\nprocessing power or the action is just huge. Then it might be a good idea to separate this service \n\nfrom the API, so that the API just handles the data processing. This will also allow for horizontal \n\nscaling, just boot up some extra services listening to the same channel, the message broker will \n\nhandle the efficient distribution part. \n\n \n\nUse Case \nWe\u2019ll walk you through the steps on how to achieve this based on a simple use case. Say you want to \n\nimplement the following feature: \u201cA user can request an evaluation of all their projects using \n\nMachine Learning.\u201d  \n\nYou can imagine that processing these projects can take a lot of time, and we don\u2019t want to block the \n\nAPI with this. \n\nIf you read further down the steps to achieve this will be explained, but the general gist of how we \n\nare going to achieve this is creating two endpoints, one for the Front-End which receives the request, \n\nand one for an Analyzer Service, receiving the results. \n\nBecause this guide is focused on approaching the message broker and creating and subscribing to \n\nmessage broker channels, I\u2019m not going into depth on how to create endpoints/services withing the \n\nAPI. \n\nSo first you need to make two endpoints, the request for analysis, which is going to send the projects \n\nto analyze to a Worker Service (which we will create later), and an endpoint which the Worker \n\nService will approach to report back the results. \n\nPublishing the task to a new channel \nIn the code flow of the request endpoint, you will be going to send data to the message broker. To do \n\nthis you will first need to make a Model which is going to be data that you want to send. We will \n\nplace this class in the 09_MessageBrokerPublisher library class, in the Models folder. \n\n \n\n\n\n \n6 \n\nFor this example, we will keep it simple, do note that we will send the projects separately. Instead of \n\nsending all the projects in one data chunk, we will allow the message broker to distribute the \n\nseparate projects to services listening to the channel we will soon make. \n\nNow in the repository layer you will need to get an instance of the TaskPublisher class, we will get \n\nthis with Dependency Injection. Simple add it to the constructor of the repository. \n\nNow you are almost ready to start publishing projects to the message broker, we are however still \n\nmissing the channel to which we want to publish! This is however quite easy, just add a new ENUM \n\nto the Subject.cs Class in the MessagebrokerPublisher.cs library class. \n\n \n\nNow back to the repository layer, in the method you\u2019ve created you will need to make an instance of \n\nthe Model populated with data. This will probably be done within a loop which enumerates through \n\na list of projects to analyze. Then per project, the task needs to be sent to the channel we\u2019ve just \n\ncreated. \n\n \n\nThat\u2019s it, the first part is done! You\u2019ve just published projects to channel PROJECT_ANALYSE. Now the \n\nmessage broker is managing a queue where all the data chunks are lined up in. \n\n \n\n  \n\n\n\n \n7 \n\nConsuming packages from a channel \nNow we get to a slightly more complicated part. All the code for the configuration of consuming from \n\nthe message broker is already written, however you still need to write a new service that will parse \n\nthe payload, validate it and then executing a certain task (analyzing those projects!). Then when \n\ndone and no exception is thrown, the preconfigured code will send an acknowledgement to the \n\nmessage broker.  \n\nFor this example, we will be making a new project, a ASP.NET Core Worker Service, it will need a \n\nproject dependency on the 11_NotificationSystem and a package dependency on \n\nNetEscapades.Configuration.Validation . Moreover, this Worker Service is going to utilize \n\nenvironment variables to get credentials so that it can connect to the message broker.  We will use a \n\nDockerfile to build the application, and add it to the docker-compose.yml, where the environment \n\nvariables are going to be passed in. \n\nLet\u2019s hop in the process of creating the Worker Service and do all the tedious configuration with \n\nDocker. \n\n \n\nCreating the Worker Service \nCreate a new project in the solution and chose the Worker Service template. \n\n \n\nChoose the following settings, don\u2019t enable docker support, we\u2019ll be writing this ourselves. \n\n  \n\nYou\u2019ll end up with the following preset. \n\n\n\n \n8 \n\n \n\n \n\nConfig data \nWe\u2019ll start by creating a configuration folder with a config class and adjusting the appsettings.json \n\nfiles accordingly. This will allow the Worker Service to receive the credentials needed for connecting \n\nto the message broker, and for accessing them easily. \n\nIn the Config folder we create the following classes, we also use NuGet to install \n\nNetEscapades.Configuration.Validation. \n\n \n\n  \n\n\n\n \n9 \n\n \n\nWe use the validatable interface so that we confirm that the instance populated from either json or \n\nenvironment variables is correct, don\u2019t worry we\u2019ll get to it in time. \n\nYour folder should look like this now. \n\n \n\n  \n\n\n\n \n10 \n\nNext up is adding the values to appsettings.json and appsettings.development.json. Press the little \n\narrow next to appsettings.json to access the development variant. \n\n \n\n \n\n  \n\n\n\n \n11 \n\nGreat, now we\u2019ll do the configuration in the Program.cs! \n\n \n\nAlright a lot of extra code, basically we will be getting the environment from an environment \n\nvariable, this will make sure that if we are in development, the right appsettings.development.json is \n\nused. \n\nThen we use the Microsoft.Extension.Configuration to create an instance of the config file. It will \n\npopulate the instance with data from the appsettings or override them with data from environment \n\nvariables if these are present.  \n\nThen we add this config object scoped, so we can used dependency injection to access the data. \n\n  \n\n\n\n \n12 \n\nDocker \nNow we will make a Dockerfile so that the project can be build from the main docker-compose.yml \n\nfile, which we will adjust to add our service with configuration (including the environment variables). \n\nAlright create a Dockerfile in the root directory of our Worker Service and put in the following code. \n\n \n\nWe first specify where docker should work in, build the application in a building environment, then \n\ncopy only the necessary files for running the application and specify what the entry point should be. \n\nNow for the docker-compose.yml, we should add a new service, based on this Dockerfile, adding it to \n\nthe network of the message broker and providing the connection details with environment variables. \n\n \n\nTo quickly walk you through this, we first name our service and specify that the service should restart \n\nif it fails. This is important since if this service starts earlier than the RabbitMQ message broker, it will \n\ncrash, basically it now retries connecting until it does. Then we define how the service needs to be \n\nbuild and specify the environment variables. Lastly, we add it to the same network on which the \n\nRabbitMQ message broker runs.  \n\n  \n\n\n\n \n13 \n\nCode to execute on receiving a package \nNow we will be writing the code that will be executed when the message broker sends us a package. \n\nWe will be making a data model, which is the same as in the API. Why don\u2019t we just add a \n\ndependency on the API you ask? Well, the idea is that this service should be able to work entirely on \n\nitself, so that in the future we could maybe migrate it to another server, better tailored to the needs \n\nof this service. Then we only have to change hostname variable and now we can deploy it anywhere. \n\nMoreover, we need to make a service that implements the Interface specified in the \n\nNotificationSystem \n\n \n\nThe service needs to parse the json string to our Model class, validate if the data is correct and \n\nultimately execute the task. If anywhere in this flow an exception is thrown by our service, then now \n\nacknowledgement of completion will be sent to the message broker. This should only be done if the \n\ntask couldn\u2019t be executed because of a program specific problem, so that another instance of the \n\nservice can execute it.  \n\nFirst for the model, create a new folder Models and create the following class. \n\n \n\n\n\n \n14 \n\nNow for the service, create a folder Executors and create the following class: \n\n \n\n  \n\n\n\n \n15 \n\nWhen a package has arrived an instance of this class is going to used to call the three methods as \n\nspecified by the interface. We\u2019ll show you how this works next. When we created this Worker Service \n\nproject, the template made a Worker.cs class. We are going to adjust it like so: \n\n \n\nNotice the RabbitMQ and NotificationSystem usings. We utilize the premade methods in the \n\nNotificationSystem to quickly set up a listener that will execute the ParsePayload, ValidatePayload \n\nand ExecuteTask methods from our own ProjectAnalyzer class. \n\nFirst, we create a RabbitMQSubscriber, which we use to subscribe to our subject, remember, it\u2019s the \n\nsame one we used to publish the task. This allows us to create a listener to this channel. \n\nNow we need to configure the listener so that it uses our ProjectAnalyzer class when a package is \n\nreceived. We do this by first creating an instance of the ProjectAnalyzer class and then creating a \n\nConsumer that can be used by the listener. \n\n\n\n \n16 \n\nLastly, we invoke the StartConsumer method on the listener object, here we pass in the consumer \n\nand the subject. \n\nThat\u2019s it! Now every time a package is in the PROJECT_ANALYSE queue, our worker service gets \n\nnotified by the message broker and will process it. \n\n \n\n \n\n \n\n \n\n \n\n  \n\n\n\n \n17 \n\n \n\n \n\n\n"
        }
      ]
    },
    "Notificationsystem": {
      "hand-ins": [
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis notification system DEX \n\nContext \nThe Digital excellence platform needs a way to alert and notify users. Currently there is no \n\nimplementation for this feature. The system needs to be SOLID so that it would be possible to be \n\neasily reused and extended. Currently the platforms architecture is of a monolithic nature, however \n\nthere are plans to change this to a more microservice-like architecture. This needs to be kept in \n\nmind. \n\n \n\nFunctional Requirements \n- Authorized clients can create a subject which can be subscribed to \n\n- Authorized clients can create or use templates for notifications to be send \n\n- Authorized  clients can register notifications for specific subjects, sending them immediately \n\nor schedule them \n\n- Authorized clients can specify by which method the notification should be send \n\n- Users can be subscribed to subjects \n\n- Users can be unsubscribed for subjects \n\nNon-functional Requirements \n- The system should be a stand-alone service \n\n- The system needs to be able to utilize existing services like Sendgrid and Firebase Cloud \n\nMessaging \n\n \n\nProposal \nI did a bit more research in to this. I suggest just making a stand-alone notification handler, which can \n\nreceive contents and the notification type, handling the data and passing it along to another class \n\nthat handles the method of sending. \n\nIf designed correctly it would easily allow other systems to utilize the notification system. \n\nMoreover this way we could not only use it for sending emails, but maybe at a later point of time \n\nalso for real time notifications with web sockets or for push notifications. \n\nFor specific technologies, the best options in my opinion are to make it a REST API so that it can be \n\napproached from the web, or make it a service to make it more contained.  \n\nAs for the messaging by email right now, I looked into SendGrid and they allow easy dynamic \n\ntemplating. (SendGrid, z.d.) (z.d.) \n\nDedase, (2020) has created a resource that looks really extensive touching on how the architecture \n\nof such a service could look like. It's focussed on web shop notifications based on products etc but a \n\nlot of the core concepts can be used. \n\nFrom what I remember of design patterns and from research done I think we could utilize the \n\nobserver pattern in our design. Seniuk, (z.d.) has an extensive article which uses notifications as a \n\nmeans to explain this pattern. \n\nWhen searching online for existing implementations most try to indeed separate the data handler \n\nand the sending methods for obvious reasons, some examples: Codeproject, (2009) and   Josue S \n\n(z.d.). \n\n\n\nI'd like input as to what you all prefer or if there are better ways that I missed. My preference is as \n\nsummarized: a separate handler which can receive dynamic data and registers notifications based on \n\nthe observer pattern, when fired passing it along to one of the available actual notification methods \n\navailable. \n\n \n\nReferences \ncodeproject. (2009, 5 oktober). Email Notification Framework. \n\nhttps://www.codeproject.com/Articles/42843/Email-Notification-Framework \n\nDedase, A. (2020, 9 februari). Architecting a Scalable Notification Service - The Startup. Medium. \n\nhttps://medium.com/swlh/architecting-a-scalable-notification-service-778c6fb3ac28 \n\nJosue S. (z.d.). Sending Emails Automatically and periodically Using C#. Ipointsystems. Geraadpleegd \n\n1 oktober 2020, van https://www.ipointsystems.com/blog/2018/july/sending-mails-with-c \n\nS. (z.d.). sendgrid/email-templates. GitHub. Geraadpleegd 1 oktober 2020, van \n\nhttps://github.com/sendgrid/email-templates/tree/master/dynamic-templates \n\nSendGrid. (z.d.). How to send an email with Dynamic Transactional Templates. SendGrid \n\nDocumentation. Geraadpleegd 1 oktober 2020, van https://sendgrid.com/docs/ui/sending-\n\nemail/how-to-send-an-email-with-dynamic-transactional-templates/#design-a-dynamic-\n\ntransactional-template \n\nSeniuk, V. (z.d.). The Observer Design Pattern in Java. Stack Abuse. Geraadpleegd 1 oktober 2020, van \n\nhttps://stackabuse.com/the-observer-design-pattern-in-java/ \n\n \n\n\n"
        }
      ]
    },
    "OnderzoekenadviesRealtimeSentimentAnalyse": {
      "hand-ins": [
        null,
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantified Student \u2013 Real-time Sentiment \nAnalyse \nMax van Hattum\u2013 Delta \u2013 18-3-2021 \n\n\n\n \n1 \n\nTable of Contents \nIntroduction ............................................................................................................................................. 2 \n\nResearch question ................................................................................................................................... 2 \n\nResearch methods ................................................................................................................................... 2 \n\nWhat is Sentiment Analysis? ................................................................................................................... 3 \n\nTypes of sentiment analysis ................................................................................................................ 3 \n\nFine-grained sentiment analysis ...................................................................................................... 3 \n\nEmotion detection ........................................................................................................................... 3 \n\nAspect-based sentiment analysis .................................................................................................... 3 \n\nReal-time sentiment analysis. ......................................................................................................... 3 \n\nHow does Sentiment Analysis work? ...................................................................................................... 4 \n\nDrawbacks ........................................................................................................................................... 4 \n\nImplementations ................................................................................................................................. 4 \n\nIs Sentiment Analysis de best solution for the context? ......................................................................... 5 \n\nProof of Concept ................................................................................................................................. 5 \n\nConclusion and Advise ............................................................................................................................. 6 \n\nGlossary ................................................................................................................................................... 6 \n\n \n\n  \n\n\n\n \n2 \n\nIntroduction \nQuantified Student wants to enable students to get more insight in their learning process. We want \n\nto achieve this by gathering and processing all kinds of data available through the digital learning \n\nenvironment. \n\nThis should allow students to understand their learning process and adjust where deemed necessary.  \n\nOne of the concepts is to nudge a student if the text they are typing is predominantly negative or \n\npositive. Mainly while they are utilizing the Feedpulse module, which allows students to reflect on \n\ntheir day. \n\nAn example of such an implementation would be that when the student has typed multiple negative \n\nsentences in a short time, that they will get a pop-up saying: \u201cDid you really have such a bad day?\u201d. \n\nTo achieve this automatic Sentiment Analysis in real-time should be researched. \n\n \n\nResearch question \nThe main question is as follows: \n\nHow can Sentiment Analysis be used to instantly make a student aware of the \n\ntone of their text? \n\nTo be able to answer this question accurately, the question needs to be divided in several sub-\n\nquestions. It\u2019s important to first determine wat sentiment analysis is, how it works and if it fits the \n\ncontext. \n\n- What is Sentiment Analysis? \n\n- How does Sentiment Analysis work? \n\n- Is Sentiment Analysis the best fit for the context? \n\n \n\nResearch methods \nWe will be using literature-based research and the creation of a proof of concept. This will also be \n\nvalidated with the stakeholder to guarantee a right fit for the problem. Combining the three methods \n\nwill result in certainty and fit. \n\n  \n\n\n\n \n3 \n\nWhat is Sentiment Analysis? \nSentiment analysis is used for  discovering the polarity of a text, which can be defined as negative, \n\nneutral, or positive.  This could be done with a rule-based or machine learning approach (or a hybrid). \n\nThis could for example be used for quickly gaining insight into the sentiment of people towards a \n\ntweet or scanning product reviews in bulk.  \n\n \n\nTypes of sentiment analysis  \n\nFine-grained sentiment analysis  \nThis type of analysis focusses on a better, more defined assessment of the sentiment of the text in \n\nquestion. It will divide it in more categories e.g., very negative, negative, neutral, positive, and very \n\npositive. This is perfect for generating a star-based evaluation of a review.   \n\n \n\nEmotion detection  \nIt\u2019s also possible to focus on detecting emotion within a text. It might be more suitable for a problem \n\nto be quantified in types of emotion. While \u2018thrilling\u2019 and \u2018interesting\u2019 could both be construed as \n\npositive, they convene a different sentiment.   \n\nThis type of sentiment can be tricky though, since people express their emotions differently, e.g. \u2018I \n\nwould kill for that\u2019.   \n\n \n\nAspect-based sentiment analysis  \nA piece of text might be a reaction to an issue with multiple facets. You might be interested in \n\ndetermining which aspects are received negatively, and which are received positively. For this you \n\ncan use aspect-based sentiment analysis, which tries to determine multiple facets in the text and \n\nassign the sentiment accordingly  \n\n \n\nReal-time sentiment analysis.  \nIt could also be possible to monitor for example tweets or posts on a forum in real-time. You could \n\nset a threshold for when you should be notified and discover issues or trends quickly. (Monkeylearn, \n\nz.d.)  \n\nIn general rule-based solutions are faster in this than implementations based on Machine Learning: \n\nHowever, when compared to sophisticated machine learning techniques, the \n\nsimplicity of VADER carries several advantages. First, it is both quick and \n\ncomputationally economical without sacrificing accuracy. (Hutto & Gilbert, 2014, \n\np. 10) \n\n  \n\n\n\n \n4 \n\nHow does Sentiment Analysis work? \nRule-based programs generally have a lexicon with words that have a score and then use an \n\nalgorithm to determine the sentiment of a piece of text. Most of the time they don\u2019t take position or \n\ndouble negatives into account. Which can result in less accurate determinations. \n\nMost solution use machine-learning or a combination of rules and machine-learning. Most \n\nimplementations use a bag of words or bag-of-ngrams to vectorize the text.   \n\nA bag of words uses a list of predetermined words to search and quantify them from the text. A bag \n\nof n-grams search for a combination of words. Both methods spit out a vector representation of the \n\npiece of text, the vector represents which words or n-grams are present in the text. Then these \n\nvectors can be scored. (Brownlee, 2019)  \n\nTo score the vectors the vocabulary must be classified e.g., positive, neutral, negative.  \n\n \n\nDrawbacks  \nIt\u2019s hard to detect irony or sarcasm, furthermore sentiment must be placed in context. Because \n\nsentiment is subjective, it\u2019s also difficult to score words. This leads to needing to define the terms \n\nwith which the text is going to be scored.   \n\n \n\nImplementations   \nThere are multiple resources available for scoring text based on sentiment. There is Microsoft Azure \n\nText analytics, IBM Watson natural language detection, Amazon Comprehend and VaderSentiment.  \n\nInterestingly VaderSentiment is the only rule-based implementation between these noteworthy \n\nservices. However, it scores well in comparison to the above services: \n\nVADER performed as well as (and in most cases, better than) eleven other highly \n\nregarded sentiment analysis tools. (Hutto & Gilbert, 2014, p. 11) \n\nMost are extensive and complicated. There are also explanations available online on how to train \n\nyour own model (Jain, 2020)  \n\n \n\n  \n\n\n\n \n5 \n\nIs Sentiment Analysis de best solution for the context? \nFor the context, real-time sentiment analysis can be used. A rule-based implementation would be \n\npreferable since it is very fast and light weight. Feedpulse can be in different languages, however \n\nVaderSentiment also works with multiple languages, it states that it is accurate in determining \n\nsentiment even when a sentence is first translated to English and then analyzed. (C, z.d.). \n\nTaking these points in account VaderSentiment is a good fit for the context. It must be noted \n\nhowever that it is made for analyzing small pieces of text, namely focused on social media, and is \n\nprobably less accurate on larger texts.  \n\n \n\nProof of Concept \nAs a proof of concept, I\u2019ve created a simple JavaScript application that reads out a textbox. To extract \n\nsentences, it monitors usages of points (\u2018.\u2019). After every \u2018.\u2019 entry, it takes the text from the previous \n\npoint, up until the latest point. It uses Google\u2019s Compact Language detector to then determine the \n\nlanguage, if it\u2019s not English it first translates the sentence to English. \n\nThen it analyses this piece of text with the VaderSentiment JavaScript library. (V, z.d.-b). This results \n\nin four scores, the positivity, naturality and negativity.  \n\nThe final score is the compound score, it uses rules to give more weight to certain situations. For \n\nexample, in the following sentence: \u201cThe idea was great, however the execution was horrible.\u201d It \n\nregards the part after however as more important. \n\nhttps://github.com/Quantified-Student/POCs/tree/main/poc-sentiment-textbox  \n\n \n\n \n\nhttps://github.com/Quantified-Student/POCs/tree/main/poc-sentiment-textbox\n\n\n \n6 \n\nConclusion and Advise \nTo conclude this research, I can state with certainty that Sentiment Analysis could indeed be used to \n\nmake students aware of the tone of their text in real-time. Sentiment analysis is accurate and can be \n\ndone sufficiently quick, so that real-time analysis is possible. \n\nMy advice is to use a rule-based lexicon implementation, since it is light-weight giving fast results. \n\nThe most used and citated implementation is VaderSentiment, and it proves to fit for the context. \n\nUsing one of the VaderSentiment libraries, passing in a sentence, and then using the result to nudge \n\nthe student would be the desired course of action. \n\n \n\n \n\nGlossary \nBrownlee, J. (2019, 7 August). A Gentle Introduction to the Bag-of-Words Model. Machine Learning \n\nMastery. https://machinelearningmastery.com/gentle-introduction-bag-words-model/  \n\nMonkeylearn. (z.d.). Everything There Is to Know about Sentiment Analysis., van \n\nhttps://monkeylearn.com/sentiment-analysis/  \n\nJain, S. (2020, 5 June). Ultimate guide to deal with Text Data (using Python) \u2013 for Data Scientists and \n\nEngineers. Analytics Vidhya. https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-\n\ndeal-text-data-predictive-python/ \n\nHutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of \n\nSocial Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann \n\nArbor, MI, June 2014. \n\nC. (z.d.). cjhutto/vaderSentiment. GitHub. https://github.com/cjhutto/vaderSentiment#demo-\n\nincluding-example-of-non-english-text-translations \n\nV. (z.d.-b). vaderSentiment/vaderSentiment-js. GitHub. Geraadpleegd op 18 maart 2021, van \n\nhttps://github.com/vaderSentiment/vaderSentiment-js \n\n \n\n \n\nhttps://github.com/cjhutto/vaderSentiment#demo-including-example-of-non-english-text-translations\nhttps://github.com/cjhutto/vaderSentiment#demo-including-example-of-non-english-text-translations\nhttps://github.com/vaderSentiment/vaderSentiment-js\n\n"
        }
      ]
    },
    "OnderzoekenPOCsSentimentAnalyse": {
      "hand-ins": [
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode and results aspect discovering in longer text \nRandom review from amazon: \"For some reason these controllers from Amazon \n\njust aren't the same as the ones from wal-mart. They're lighter, the analog \n\nsticks are so loose, no stiffness at all, they just flop around, and they \n\nall die on me within a few months. There must be different manufacturers \n\ninvolved here.\" \n\n \n\nSee last result for filtered most important aspects. \n\n \n\n  \n\n\n\n \n\n \n\n  \n\n\n\n \n\n \n\n \n\n\n\n \n\n \n\n \n\n \n\n\n"
        },
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProofs of Concepts: Sentiment Analysis \nMax van Hattum \u2013 10-9-2020 \n\n  \n\n\n\nInhoudsopgave \nIntroduction ............................................................................................................................................. 1 \n\nAvailable Methods ................................................................................................................................... 1 \n\nGetting ready to code .............................................................................................................................. 1 \n\nCode! ....................................................................................................................................................... 2 \n\nConclusion ............................................................................................................................................... 4 \n\nGlossary ................................................................................................................................................... 4 \n\n \n\n\n\n \n1 \n\nIntroduction \nKnowing what the sentiment of a sentence or piece of text is could be of enormous value. Imagine \n\nanalysing thousands of reviews or tweets with manpower, or with an AI. Therefor I\u2019m going to \n\nexplore what the options already available for sentiment analysis are and how to utilize them. \n\n \n\nAvailable Methods \nOn a first glance when searching for natural language processing in combination with sentiment \n\nanalyses again Python shows up as most used language. Articles going in-depth about sentiment \n\nanalysis mostly show of its power by using it on Tweets. The library \u2013 trained AI \u2013 most mentioned \n\nseems to be VADER ( Valence Aware Dictionary and sEntiment Reasoner) (Brownlee, 2019). Since \n\nVADER performed especially well on tweets, it logical that this one is mentioned frequently. \n\nHowever there are more options available: NLTK (TutorialsPoint, z.d.) and SPACY both provide low \n\nlevel text analytics and provide options to custom train models or use pre-trained models. They are \n\nboth perfect for preparing text for use with for example deeplearn frameworks like: TensorFlow, \n\nPyTorch and scikit-learn (Spacy, z.d.). \n\n \n\nGetting ready to code \nTo begin with, I\u2019m going to use VaderSentiment to analyse a sentence inputted by an user. The \n\nlibrary is available with PIP and nothing else is required. \n\n \n\n\n\n \n2 \n\nCode! \n\nEnglish-only POC \n\n \n\nIt\u2019s honestly after reading the vader docs pretty straightforward. You import the sentiment analyzer \n\nand easily get sentences scored from -1 as most negative to 1 as most positive with a compound \n\nscore taking not only the word scores into account, but also grammar and punctuation.  See below \n\nfor output. \n\n \n\nAs you can see the compound score is higher, because it weighs the exclamation mark and the part \n\nafter \u2018but\u2019 heavier and multiplies the sentiment of the part of the sentence that connects to that. \n\n \n\n  \n\n\n\n \n3 \n\nAll languages POC \nTo me it feels kind of hacky, but the makers of VADER straight up recommend to just sent a sentence \n\nto a translator and then pass it to VADER. In their experience this gave accurate results. (Hutto, 2014) \n\n \n\n \n\nSee below output: \n\n \n\n \n\n \n\n \n\n \n\n\n\n \n4 \n\nConclusion \nGetting the sentiment polarity of a sentence is easily done and produces good results. For longer \n\npieces of text it would be possible to split up the text to sentence level and analyse these, however \n\nthere are more, maybe better fit, solutions for analysing bigger pieces of text. I\u2019ve found a guide \n\nexplaining how to do aspect-based sentiment analysis to get a polarity score based on \n\ncategories/subjects mentioned in the text. This use NLTK and StanfordNLP. This will be the next step \n\nin my journey to becoming a NLP master. \n\nGlossary \nBrownlee, J. (2019, 7 augustus). A Gentle Introduction to the Bag-of-Words Model. Machine Learning \n\nMastery. https://machinelearningmastery.com/gentle-introduction-bag-words-model/ \n\nTutorialsPoint. (z.d.). AI with Python \u00c3\u00a2\u00c2\u20ac\u00c2\u201c NLTK Package - Tutorialspoint. Geraadpleegd 10 \n\nseptember 2020, van \n\nhttps://www.tutorialspoint.com/artificial_intelligence_with_python/artificial_intelligence_with_pyth\n\non_nltk_package.htm \n\nSpacy. (z.d.). Models \u00b7 spaCy Models Documentation. Models. Geraadpleegd 10 september 2020, van \n\nhttps://spacy.io/models/ \n\nHutto, C. J. (2014, 17 november). cjhutto/vaderSentiment. GitHub - VaderSentiment. \n\nhttps://github.com/cjhutto/vaderSentiment#demo-including-example-of-non-english-text-\n\ntranslations \n\n \n\n \n\n \n\n \n\nhttps://www.tutorialspoint.com/artificial_intelligence_with_python/artificial_intelligence_with_python_nltk_package.htm\nhttps://www.tutorialspoint.com/artificial_intelligence_with_python/artificial_intelligence_with_python_nltk_package.htm\n\n"
        },
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentiment analysis\n\n\n \n\n  \n\nSENTIMENT ANALYSIS \nWHAT WHY HOW \n\nHattum,Max M.B. van \n3928780 \n\n3-9-2020 \n\nAbstract \nIn dit document geef ik een samenvatting van wat sentiment analyse is, waarom het nuttig is \n\nen hoe het gebruikt kan worden. Zowel daadwerkelijke technieken, maar ook in \nhypothetische situaties. \n\n\n\nInhoud \nWhat is sentiment analysis ...................................................................................................................... 1 \n\nTypes of sentiment analysis .................................................................................................................... 1 \n\nFine-grained sentiment analysis .......................................................................................................... 1 \n\nEmotion detection ............................................................................................................................... 1 \n\nAspect-based sentiment analysis ........................................................................................................ 1 \n\nReal-time sentiment analysis. ............................................................................................................. 1 \n\nHow to implement................................................................................................................................... 1 \n\nDrawbacks ............................................................................................................................................... 2 \n\nImplementations ..................................................................................................................................... 2 \n\nGlossary ................................................................................................................................................... 3 \n\n \n\n \n\n\n\n \n1 \n\nWhat is sentiment analysis \nSentiment analysis is used for  discovering the polarity of a text, which can be defined as negative, \n\nneutral or positive.  This could be done with a rule-based or machine learning approach (or a hybrid). \n\nThis could for example be used for quickly gaining insight into the sentiment of people towards a \n\ntweet or scanning product reviews in bulk. \n\n \n\nTypes of sentiment analysis \n\nFine-grained sentiment analysis \nThis type of analysis is focus an a better, more defined assessment of the sentiment of the text in \n\nquestion. It will divide it in more categories e.g. very negative, negative, neutral, positive and very \n\npositive. This is perfect for generating a star based evaluation of a review.  \n\n \n\nEmotion detection \nIt\u2019s also possible to focus on detecting emotion within a text. It might be more suitable for a problem \n\nto be quantified in types of emotion. While \u2018thrilling\u2019 and \u2018interesting\u2019 could both be construed as \n\npositive, they convene a different sentiment.  \n\nThis type of sentiment can be tricky though, since people express their emotions differently, e.g. \u2018I \n\nwould kill for that\u2019. \n\n \n\nAspect-based sentiment analysis \nA piece of text might be a reaction to an issue with multiple facets. You might be interested in \ndetermining which aspects are received negatively, and which are received positively. For this you \ncan use aspect-based sentiment analysis, which tries to determine multiple facets in the text and \nassign the sentiment accoridingly \n \n\nReal-time sentiment analysis. \nIt could also be possible to monitor for example tweets or posts on a forum in real-time. You could \n\nset a threshold for when you should be notified and discover issues or trends quickly. (Monkeylearn, \n\nz.d.) \n\n \n\nHow to implement \nRule-based programs could be used, but most of the time they don\u2019t take position or double \n\nnegatives into account. Most solution use machine-learning or a combination of rules and machine-\n\nlearning. Most implementations use a bag of words or bag-of-ngrams to vectorize the text.  \n\nA bag of words uses a list of predetermined words to search and quantify them from the text. A bag \n\nof n-grams search for a combination of words. Both methods spit out a vector representation of the \n\npiece of text, the vector represents which words or n-grams are present in the text. Then these \n\nvectors can be scored. (Brownlee, 2019) \n\nTo score the vectors the vocabulary must be classified e.g. positive, neutral, negative. \n\n\n\n \n2 \n\nDrawbacks \nIt\u2019s hard to detect irony or sarcasm, furthermore sentiment must be placed in context. Because \nsentiment is fairly subjective, it\u2019s also difficult to score words. This leads to needing to define the \nterms with which the text is going to be scored.  \n \n\nImplementations  \nThere are multiple resources available for scoring text based on sentiment. There is Microsoft Azure \n\nText analytics, IBM Watson natural language detection, Amazon Comprehend and VaderSentiment. \n\nMost are fairly extensive and complicated. There are also explanations available online on how to \n\ntrain your own model (Jain, 2020) \n\nThere are also pre-trained open source models available, earlier I mentioned VaderSentiment. This \n\nimplementation is surprisingly accurate for social media and smaller pieces of texts. (Hutto & Gilbert, \n\n2014, p. 1) \n\n \n\n \n\n  \n\n\n\n \n3 \n\nGlossary \n \n\nBrownlee, J. (2019, 7 augustus). A Gentle Introduction to the Bag-of-Words Model. Machine Learning \n\nMastery. https://machinelearningmastery.com/gentle-introduction-bag-words-model/ \n\nMonkeylearn. (z.d.). Everything There Is to Know about Sentiment Analysis. Geraadpleegd 3 \n\nseptember 2020, van https://monkeylearn.com/sentiment-analysis/ \n\nHutto, C. J., & Gilbert, E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of \n\nSocial Media Text. Association for the Advancement of Artificial Intelligence, 1\u201310. \n\nhttp://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf \n\nJain, S. (2020, 5 juni). Ultimate guide to deal with Text Data (using Python) \u2013 for Data Scientists and \n\nEngineers. Analytics Vidhya. https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-\n\ndeal-text-data-predictive-python/ \n\n \n\n \n\nhttp://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf\n\n"
        }
      ]
    },
    "POCElasticSearch": {
      "hand-ins": [
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProof of concept\u00a0\n\nElasticSearch\u00a0\n\n\u00a0 \u00a0\n\n\n\nVersion history\u00a0\n \n\n \n\nDistribution\u00a0\n \n \n\n\u00a0 \u00a0\n\nVersion Date Author(s) Changes State \n\n0.1 02-12-2020 Max van \nHattum & \nNiray Mak \n\nProof of concept for \nrecommendations \n\nIncomplete \n\n1.0 09-12-2020 Max van \nHattum & \nNiray Mak \n\nProof of concept for search \nfunction ElasticSearch \n\nComplete \n\nVersion Date To \n\n   \n\n\n\nIntroduction\u00a0\n \nMax van Hattum and Niray Mak worked together on making  proof of concepts for using \nelasticsearch with the DeX platform. We want to prove that it can be utilized for efficiently \nmaking project recommendations to users, and for improving search results.  \nTo achieve this goal we followed several steps: \n \nSteps to create the proof of concept: \n \n\n1. Setup ELK (Elastic, Logstash, Kibana) stack.  \n2. Create mock data \n3. Post mock data in ElasticSearch database \n4. Retrieve similar user \n5. Retrieve projects liked by similar user. \n6. Searching project with ElasticSearch \n\n \nAfter creating the proof of concept we also looked into how we could use the search function \nof ElasticSearch. This research can be found and the end of the document. \n\n\u00a0 \u00a0\n\n\n\n1. Setup ELK stack: \n \nTutorial on how our ELK stack was setup on our local machine can be found on the link \nbeneath: \n \nhttps://github.com/DigitalExcellence/dex-backend/wiki/ELK-stack-installation-guide \n \n\n2. Create mock data \nWe used the website \u200bhttps://www.mockaroo.com/\u200b to make mock data. We generated 500 \nrecords in the format beneath.  \n \n{\"Created\":\"03/19/2020\",\"Id\":1,\"ProjectName\":\"Zoolab\",\"Description\":\"Lorem ipsum dolor sit \namet, consectetuer adipiscing elit. Proin interdum mauris non ligula pellentesque \nultrices.\",\"Likes\":[21,45,18,34,24,28,36,45,45,47]} \n\n3. Post mock data \nTo get the data in the ElasticSearch we created a C# application which reads the .json file \nand casts this into a list of \u201cProjectResource\u201d objects which is the same format as mentioned \nin step 2. Thereafter the application posts every object into the ElasticSearch non relational \ndatabase. \n \n\n \nWe can keep in mind that we can reuse this code to synchronize our actual database with \nthe ElasticSearch database. \n\n4. Retrieve similar user \nWe wrote a query which retrieves a list of users who are similar to the user entered in the \nparameters. In our case this is hardcoded in the query and is user 10. As a top result we get \n\nhttps://github.com/DigitalExcellence/dex-backend/wiki/ELK-stack-installation-guide\nhttps://www.mockaroo.com/\n\n\nthe user itself who apparently has liked 90 projects in total. As a second result we get user \n44 who has liked 25 projects which are also liked by user 10. We can assume user 10 and \nuser 44 are \u201csimilar\u201d to each other and we will make recommendations out of that. \n \n\n \n  \n\n\n\n5. Retrieve project recommendations \nWe wrote a query which retrieves a list of projects liked by similar user 44 from step 4 but \nnot by user 10. In practice our system can make an assumption that user 10 could also be \ninterested in these projects and it will recommend these projects. \n \n\n  \n\n\n\n6.  Searching project with ElasticSearch \nIn elasticsearch the index can be configured to use multiple analyzers that process the data \nthat is being saved. It\u2019s also possible to analyse the search query.  \nThis results in more relevant search results when querying against the data. \nSpecifically we\u2019ve configured an analyzer that uses a n-gram model, which turns the query in \nbasically a sequence of characters as shown below. This analyzer is assigned to the \nprojectname field. \n \n\n \n(ElasticSearch, z.d.) \n \nWe also configured a separate analyzer for the Project Description that filters the text \nand allows for matching with synonyms. We\u2019ve chosen English as the main language \nfor the language processing, since most projects are in English. \nWe process the description on stopwords (\u201cA\u201d, \u201cBut\u201d, \u201cThe\u201d, etc.) and stem the \nwords, transforming conjugations of verbs into their base (\u201cran => run\u201d).  \n \nThe query underneath can be used to set the mapping for the document with analyzers.  \n \n{ \n\n    \u200b\"settings\"\u200b:\u200b \u200b{ \n\n        \u200b\"analysis\"\u200b:\u200b \u200b{ \n\n            \u200b\"analyzer\"\u200b:\u200b \u200b{ \n\n                \u200b\"autocomplete\"\u200b:\u200b \u200b{ \n\n                    \u200b\"tokenizer\"\u200b:\u200b \u200b\"autocomplete\"\u200b, \n\n                    \u200b\"filter\"\u200b:\u200b \u200b[ \n\n                        \u200b\"lowercase\" \n\n                    \u200b] \n\n                \u200b}, \n\n                \u200b\"autocomplete_search\"\u200b:\u200b \u200b{ \n\n                    \u200b\"tokenizer\"\u200b:\u200b \u200b\"lowercase\" \n\n                \u200b}, \n\n\n\n                \u200b\"description_index\"\u200b:\u200b \u200b{ \n\n                    \u200b\"tokenizer\"\u200b:\u200b \u200b\"lowercase\"\u200b, \n\n                    \u200b\"filter\"\u200b:\u200b \u200b[ \n\n                        \u200b\"synonym\"\u200b, \n\n                        \u200b\"english_stop\"\u200b, \n\n                        \u200b\"english_stemmer\" \n\n                    \u200b] \n\n                \u200b}, \n\n                \u200b\"description_search\"\u200b:\u200b \u200b{ \n\n                    \u200b\"tokenizer\"\u200b:\u200b \u200b\"lowercase\" \n\n                \u200b} \n\n            \u200b}, \n\n            \u200b\"filter\"\u200b:\u200b \u200b{ \n\n                \u200b\"synonym\"\u200b:\u200b \u200b{ \n\n                    \u200b\"type\"\u200b:\u200b \u200b\"synonym\"\u200b, \n\n                    \u200b\"format\"\u200b:\u200b \u200b\"wordnet\"\u200b, \n\n                    \u200b\"lenient\"\u200b:\u200b \u200btrue\u200b, \n\n                    \u200b\"synonyms_path\"\u200b:\u200b \u200b\"analysis/wn_s.txt\" \n\n                \u200b}, \n\n                \u200b\"english_stop\"\u200b:\u200b \u200b{ \n\n                    \u200b\"type\"\u200b:\u200b \u200b\"stop\"\u200b, \n\n                    \u200b\"stopwords\"\u200b:\u200b \u200b\"_english_\" \n\n                \u200b}, \n\n                \u200b\"english_stemmer\"\u200b:\u200b \u200b{ \n\n                    \u200b\"type\"\u200b:\u200b \u200b\"stemmer\"\u200b, \n\n                    \u200b\"language\"\u200b:\u200b \u200b\"english\" \n\n                \u200b} \n\n            \u200b}, \n\n            \u200b\"tokenizer\"\u200b:\u200b \u200b{ \n\n                \u200b\"autocomplete\"\u200b:\u200b \u200b{ \n\n                    \u200b\"type\"\u200b:\u200b \u200b\"edge_ngram\"\u200b, \n\n                    \u200b\"min_gram\"\u200b:\u200b \u200b2\u200b, \n\n                    \u200b\"max_gram\"\u200b:\u200b \u200b10\u200b, \n\n                    \u200b\"token_chars\"\u200b:\u200b \u200b[ \n\n                        \u200b\"letter\" \n\n                    \u200b] \n\n                \u200b} \n\n            \u200b} \n\n        \u200b} \n\n    \u200b}, \n\n    \u200b\"mappings\"\u200b:\u200b \u200b{ \n\n        \u200b\"properties\"\u200b:\u200b \u200b{ \n\n            \u200b\"Created\"\u200b:\u200b \u200b{ \n\n                \u200b\"type\"\u200b:\u200b \u200b\"date\" \n\n            \u200b}, \n\n\n\n            \u200b\"Id\"\u200b:\u200b \u200b{ \n\n                \u200b\"type\"\u200b:\u200b \u200b\"integer\" \n\n            \u200b}, \n\n            \u200b\"ProjectName\"\u200b:\u200b \u200b{ \n\n                \u200b\"type\"\u200b:\u200b \u200b\"text\"\u200b, \n\n                \u200b\"analyzer\"\u200b:\u200b \u200b\"autocomplete\"\u200b, \n\n                \u200b\"search_analyzer\"\u200b:\u200b \u200b\"autocomplete_search\" \n\n            \u200b}, \n\n            \u200b\"Description\"\u200b:\u200b \u200b{ \n\n                \u200b\"type\"\u200b:\u200b \u200b\"text\"\u200b, \n\n                \u200b\"analyzer\"\u200b:\u200b \u200b\"description_index\"\u200b, \n\n                \u200b\"search_analyzer\"\u200b:\u200b \u200b\"description_search\" \n\n            \u200b}, \n\n            \u200b\"Likes\"\u200b:\u200b \u200b{ \n\n                \u200b\"type\"\u200b:\u200b \u200b\"integer\" \n\n            \u200b} \n\n        \u200b} \n\n    \u200b} \n\n} \n\n  \n\n\n\nSynonyms \n \nThe query makes use of a file called wn_s.txt. This file is acquired from WordNet. (The \nTrustees of Princeton University, 2020) \n \nThe file exists out of 220k lines with references to synonyms. \n\n \n \n \n \n \n \n \n \n  \n\n\n\nGlossary\u00a0\n \n\nElasticSearch. (z.d.). \u200bN-Gram explained\u200b [Illustration]. Elastic. \n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/analysi\n\ns-ngram-tokenizer.html \n\nThe Trustees of Princeton University. (2020). \u200bWordNet\u200b. WordNet. \n\nhttps://wordnet.princeton.edu/ \n\n \n\n \n\n \n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-ngram-tokenizer.html\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-ngram-tokenizer.html\nhttps://wordnet.princeton.edu/\n\n"
        }
      ]
    },
    "ResearchtranslatingdemotofunctionalApp": {
      "hand-ins": [
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearch translating REST API demo to functional app\n\n\n \n\n  \n\nRESEARCH TRANSLATING \n\nREST API DEMO TO \n\nFUNCTIONAL APP \n      \n\nMax \n3928780 \n\n13-11-2020 \n\nOL \u2013 PSV Swimming \n\nCees van Tilborg \n\nV1 \n\nAbstract \nIn this document the current back-end is going to be analysed. Moreover this document is \n\ngoing to discuss the steps that need to be taken to translate this into a functioning back-end \nwith persistent data. Taking in mind security, scalability and maintainability.    \n\n\n\nInhoud \nIntroduction ............................................................................................................................................. 1 \n\nDOT Framework approach ...................................................................................................................... 1 \n\nWhat .................................................................................................................................................... 1 \n\nHow ..................................................................................................................................................... 1 \n\nWhy ..................................................................................................................................................... 1 \n\nResearch question ................................................................................................................................... 2 \n\nMain question ..................................................................................................................................... 2 \n\nSub questions ...................................................................................................................................... 2 \n\nWhat can they current demo already do? .............................................................................................. 2 \n\nWhat technologies does the current demo use? .................................................................................... 2 \n\nWhat needs to be added and/or change for the demo to reach functionality? ..................................... 3 \n\nHow to best implement the aforementioned additions? ....................................................................... 3 \n\nConclusion ............................................................................................................................................... 4 \n\nGlossary ................................................................................................................................................... 5 \n\n \n\n \n\n\n\n \n1 \n\nIntroduction \nFor Open Learning we get the opportunity to work for an external stakeholder, giving context to our \n\nlearning process. I\u2019ve chosen to work on an application for PSV Swimming. The main features this app \n\nneeds to offer are enabling swimmers to manage their event registrations, coaches to review this \n\nregistrations and administrators to see an overview of these registrations so they can process them. \n\nA previous Fontys group already worked on this project, leaving behind a demo existing of a PWA \n\nand REST API. This demo uses mock data, which is non persistent, to showcase how such an \n\napplication would look like. \n\nMy task is to analyse the REST API and identify what is needed to translate this to a fully functional \n\nAPI taking into account security, scalability and maintainability. \n\n \n\nDOT Framework approach \n\nWhat \nThis research will mostly take place in the application domain, since the current application needs to \n\nbe understood before it can be translated into a functional app. However, some research into the \n\navailable work domain will also be done. We take this approach because a lot of similar apps are \n\nalready made, so logically certain techniques will already be developed to solve the issues at hand. \n\n \n\nHow \nSome Field research will be done to get the know the context in which the application is going to be \n\nused. Then we will use the Library method to obtain knowledge about how to best approach the \n\nissue. Not only by doing literature research, but also by talking to experts. After this is done we move \n\non to the Lab approach, where the most work will be taking place. The reason for this is that a \n\nprevious group already did a lot of good research and documentation, however this leaves a huge \n\namount of implementation to be done. By doing presentations we will validate the application, so \n\nthe Showroom approach will also be used. \n\n \n\nWhy \nWe focus heavily on the Lab approach because we want to achieve the best fit possible for the \n\nsituation. The stakeholder expressed his wishes that he just wants something functional, even if it is \n\nbareboned. However we want the application to be secure and scalable, therefor we also do \n\nresearch to gain an overview and show our product to gain expertise.  \n\n  \n\n\n\n \n2 \n\nResearch question \n\nMain question \nTo give guidance to this research we formulate a main question: \n\n\u201cWhat work needs to be done to transform the existing demo into a functional \n\napplication?\u201d \n\nFor this question to be answered, multiple facets need to be researched. First it needs to be clear \n\nwhat the current demo can do and what technologies this uses. Then we need to do research about \n\nwhat is needed to make the demo functional. Afterwards knowledge needs to be attained about how \n\nto best implement the work that is needed for this transformation. \n\nSub questions \n- What can they current demo already do? \n\n- What technologies does the current demo use? \n\n- What needs to be added and/or changed for the demo to reach functionality? \n\n- How to best implement the aforementioned additions? \n\n \n\nWhat can they current demo already do? \nThe current demo is a REST API, where authentication is implemented using JWT. There are multiple \n\nendpoints which pertain to logic for creating users with roles, sending account creation mails, \n\nmanaging registration to events, managing trainings and creating news notifications.  \n\nThe demo does not utilize a way to persist data, meaning that the application only functions for the \n\nlifetime of the service. No database is connected, instead there are classes returning static data, \n\nwhich are retrieved and then worked with. \n\n \n\nWhat technologies does the current demo use? \nThe demo is written in Java, using the Spring Boot framework. Documentation is facilitated by \n\nSwagger. The authentication is done with Spring Boot Security using JWT. To build the application \n\nDocker is used, a Dockerfile with building instructions is present. For the account creation affirmation \n\nthe SendGrid API is used to send mails, the passwords were sent this way, however they were not \n\nencrypted. \n\nThere were a lot of unused dependencies installed, for example Liquibase, while there was no \n\ndatabase present.  \n\nAfter reading about the Spring Boot Framework I approve of the use of this framework. It is one of \n\nthe most used frameworks for building web applications and compares well to alternatives \n\n(Kudryashov, 2020). Moreover it has excellent documentation which facilitates ease of use (Webb, \n\nz.d.). \n\nSwagger is also a good choice for generating documentation for the available endpoints. It provides \n\nan interactive user interface for experimenting with the REST API and has good integration with \n\nSpring Boot. \n\n\n\n \n3 \n\nFor deployment Docker was configured. However it was only a Dockerfile building the \n\nwebapplication, not a docker-compose. If you go the route of using Docker to facilitate easy \n\ndeployment, and you have dependencies such as a database, you should definitely include and \n\nconfigure these services. This was not done, so it actually does not provide much extra value for \n\neasier deployment. Moreover the stakeholder mentioned that they have limited resources for \n\ndeployment, so in my opinion it would be even better to forgo Docker and just manually deploy the \n\napplication and database, resulting in less bloat. \n\nThe SendGrid API is a fine choice for sending e-mails. It is becoming the standard to use dedicated \n\nservices for sending mails, instead of directly accessing and maintaining your own SMTP-server. It has \n\nnative implementation for Java and makes sending emails easy after creating an account. You can \n\njust send a API call with your token and data to be send. There are alternatives available, however \n\nSendGrid is free for low usage and has excellent documentation, making it a good choice for this \n\nproject. \n\nRegarding the unused dependencies, libraries such as Liquibase,  Jakarta xml, fasterxml Jackson, \n\nglassfish, these are straight up not used and therefor strange to include and load. Liquibase seems \n\nstrange to already have configured, this is a service that facilitates creating schemas and logging \n\nthese changes. However the demo does not use a database. Moreover the XML libraries are unused \n\nand, in my opinion, unnecessary since the native XML parser is sufficient.  \n\n \n\nWhat needs to be added and/or change for the demo to reach \n\nfunctionality? \nFor the demo to reach functionality a database needs to be setup and a way to approach this \n\ndatabase. Moreover a password encoder will be needed to guarantee security. The security for \n\nauthorization and authentication should be better configured and should use more of the standard \n\navailable methods that Spring Boot Security provides. \n\nAlso the data models will need to be refactored, because these are not created with the use of a \n\ndatabase in mind. In the domain logic, the entities are still referenced by Id, instead of just including \n\nthe entity as a child. Moreover these entity reference where sometimes \u201cdynamic\u201d foreign keys, the \n\nID could be an ID for a coach or athlete entity, this is not possible in a relational database. \n\n \n\nHow to best implement the aforementioned additions? \nFor the database the only major choice to make is a relational or non-relational database. After \n\nresearching and keeping in mind the relative small scale of data that will be handled, the rigid, \n\npredefined data that needs to be saved and taking my previous expertise in mind, a relational \n\ndatabase will be set up (Smallcombe, 2018). MySQL is opensource, widely used and I have experience \n\nwith this, therefor the choice will be a MySQL database. \n\nTo approach this database multiple options are available. We could write custom plain SQL Queries \n\nor use an ORM. After discussing this with experts (Wilrik de Loose and Cees van Tilborg) an ORM is \n\npreferred, because this is a more scalable and maintainable solution. It takes the relative complex \n\nand long native queries, and simplifies them, making it easy to use for programmers so they can \n\nfocus on coding and implementing features. (Alvarez-Eraso, Danny & Arango-Isaza, Fernando 2016) \n\n\n\n \n4 \n\nWilrik suggested to look into Spring Data JPA, which I did. It Is the native implementation from the \n\nSpring Boot Framework itself. The documentation is once again superb and integration is flawless \n\nbecause of it being from the same makers. \n\nThis in combination with Hibernate, the advised ORM by the makers of Spring Data JPA, allows for on \n\nthe fly generation of DAO (Data Access Objects). After annotating the entities with Hibernate, Spring \n\nJPA can be used to dynamically create queries from method names, which are instantiated on \n\nruntime.  \n\nMoreover the security should be refactored and reconfigured, since with the current implementation \n\nusers could access each other\u2019s information, moreover after sending out the passwords, they were \n\nnot saved in an encoded and secure fashion.  \n\nUtilizing Spring Boot Security our own UserDetails implementation should be made, to access the \n\ncurrent users data easier and more securely. Also the passwords should be saved encoded, not in \n\nplaintext. The most mentioned and used tool for this, also default by Spring Boot, is BCrypt. \n\n \n\nConclusion \nSummarizing, the demo could adequately show what features a functional app would contain, but is \n\nnot ready for use because of non-persistent data and week security. \n\nThe demo used a number of technologies, the most important ones are that the code was written in \n\nJava, using the Spring Boot Framework to setup a web application. The main form of documentation \n\nwas made with Swagger, showing and demonstrating the endpoints. User account creation utilized \n\nSendGrid to notify the user of their credentials. For deployment Docker was used to create a Docker \n\nimage which could be deployed with Docker, this however did not simplify the deployment process a \n\nlot since it did not account for dependencies like a database. There were also unused dependencies \n\nlike Liquibase, which were unnecessary for what the demo was doing.  \n\nTo transform the demo into a functional application, a database should be added, security should be \n\nimproved on and the data models should be changed so that they are compatible with the database. \n\nWhen taking all the research in mind, the easiest way to achieve a functional application is to setup a \n\nMySQL Database and create a new project, using the same technologies but with better \n\nconfiguration and data model setup.  \n\nThis will make for easier work in the future, resulting in less code debt than if we would continue in \n\nthe existing project. In this new project the security should be configured maximising Spring Boot \n\nSecurity methods. Entities should be annotated with Hibernate and the DAO should be made with \n\nSpring Data JPA, using Hibernate as the ORM.  \n\nThe current demo can be used as guidance as to what logic needs to be implemented, after the initial \n\nsetup and configuration is done correctly.  \n\n  \n\n\n\n \n5 \n\n \n\nGlossary \nAlvarez-Eraso, Danny & Arango-Isaza, Fernando. (2016). Hibernate and spring - An analysis of \n\nmaintainability against performance. Revista Facultad de Ingenier\u00eda Universidad de Antioquia. 2016. \n\n10.17533/udea.redin.n80a11. \n\nKudryashov, R. (2020, 9 januari). Not only Spring Boot: a review of alternatives. Roman Kudryashov\u2019s \n\ntech blog. https://romankudryashov.com/blog/2020/01/heterogeneous-microservices/ \n\nSmallcombe, M. (2018, 29 november). SQL vs NoSQL: 5 Critical Differences. Xplenty. \n\nhttps://www.xplenty.com/blog/the-sql-vs-nosql-difference/ \n\nWebb, P. D. S. (z.d.). Spring Boot Reference Documentation. Spring Boot. Geraadpleegd op 26 \n\nnovember 2020, van https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/ \n\n \n\n\n"
        }
      ]
    },
    "RESTAPIAnalyseDesign": {
      "hand-ins": [
        {
          "text": "\n\nIntroductie\nBinnen het Delta programma van Fontys kregen wij de optie om een project voor Strijp-T te kiezen. Het project was extreem breed, het uitgangspunt was om concepten te ontwikkelen die kenmerken zijn voor de kernwaarden van Strijp-T. Deze zouden als visitekaartje moeten dienen voor het Strijp-T terrein.\nNa het bedenken van verscheidene concepten is er een conceptpresentatie gegeven waarna Boudie Hoogedeure, de stakeholder, de keuze maakte voor de app die het huidige Strijp-T meer in contact met zijn rijke verleden zou brengen. Het idee van de app is om onder andere doormiddel van AR beelden van vroeger, van Phillips, over de huidige werkelijkheid te weergeven. Daarnaast gaf hij aan dat zij veel video materiaal hebben waarin oud Phillips medewerkers op Strijp-T ge\u00efnterviewd worden.\nHet idee is om hotspots te defini\u00ebren op terrein Strijp-T, waarna als de gebruiker in de buurt is deze informatie krijgt over de huidige locatie.\n\nRequirements\nFunctional Requirements\nFR01 - Als gebruiker kan ik een overzicht zien van alle hotspots.\nK01.1 De hotspots zijn op een kaart als markers te zien.\nK02.2 De hotspots kunnen gesorteerd worden op basis van afstand ten opzichte van de gebruiker.\nFR02 \u2013 Als gebruiker kan ik de informatie van een specifieke hotspot zien.\nK02.1 De informatie is overzichtelijk opgedeeld op basis van het onderwerp of mediatype.\nFR03 \u2013 Als gebruiker kan ik zien op een kaart waar ik ben in relatie tot de hotspots.\nB03.1 De gebruiker moet toestaan dat hun locatie gebruikt wordt.\nFR04 \u2013 Als gebruiker kan ik wanneer ik bij een hotspot ben doormiddel van AR technologie op een interactieve manier informatie zien.\nB04.1 De gebruiker moet toestaan dat hun camera gebruikt wordt.\nFR05 \u2013 Als medewerker van Strijp-T kan ik mij authenticeren.\nB05.1 De medewerker moet een account hebben.\nFR06 \u2013 Als medewerker van Strijp-T kan ik nieuwe hotspots toevoegen.\nK06.1 De medewerker kan op een kaart de locatie van de nieuwe hotspot aangeven.\nB06.1 De medewerker moet geautoriseerd zijn.\nFR07 \u2013 Als medewerker van Strijp-T kan ik bestaande hotspots verwijderen.\nK07.1 De medewerker kan een overzicht zien van hotspots en deze terplekke verwijderen\nB07.1 De medewerker moet geautoriseerd zijn.\nFR08 \u2013 Als medewerker van Strijp-T kan ik de informatie van bestaande hotspots wijzigen.\nK08.1 De medewerker kan uit een overzicht kiezen welke hotspot te willen wijzigen.\nB08.1 De medewerker moet geautoriseerd zijn.\nNon-Functional Requirements\nNFR01 \u2013 Het al opgenomen beeldmaterial dat Strijp-T beschikbaar stelt wordt gebruikt\nNFR02 \u2013 De app is in de vorm van een PWA\n\nContext model\n \nUse Cases\n\n\n\n\n\n\n\n\n\nUse Case Diagram\n\n"
        },
        {
          "text": "\n\nIntroductie\nIn Delta we got the option to work on a project for the Strijp-T organization. This organization stands for Make/Create/Innovate. Our assignment was to come up with concepts which express these values and can be used as a calling card for the organization. \nAfter concepting the stakeholder chose the concept of a Hotspot App, this app would show users markers on a map of Strijp-T, tracking their geolocation in relation to the Hotspots. Each Hotspots will show details of the place, showing an interview at the location with an old employee of Phillips and showing through AR how it looked like in the past.\nContextmodel\n\n\n\nContainermodel\n\nComponentmodel\n\n\n\nCodemodel\nIk heb besloten om deze uiteindelijk niet toe te voegen aan het document, ik vind dat het niet duidelijk is en juist eerder voor meer verwarring zorgt. Dit omdat het diagram al gauw enorm kan worden en het makkelijk is om daadwerkelijk naar de code te gaan die eventueel ge\u00efnspecteerd moet worden.  "
        }
      ]
    },
    "RestAPIRealisatie": {
      "hand-ins": [
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStrijp-T Hotspot  - Rest API \nFontys Delta Semester 4 \n\nMax van Hattum \n\n25-2-2021\n\n\n\n \n1 \n\nInhoudsopgave \nIntroductie ............................................................................................................................................... 2 \n\nEerste stappen ......................................................................................................................................... 2 \n\nDatabase .................................................................................................................................................. 3 \n\nAuthenticatie, Autorisatie en veiligheid .................................................................................................. 4 \n\nHotspot Feature ...................................................................................................................................... 4 \n\nInteractieve documentatie ...................................................................................................................... 5 \n\nDocker ..................................................................................................................................................... 5 \n\n \n\n \n\n  \n\n\n\n \n2 \n\nIntroductie \nBinnen het Delta programma van Fontys kregen wij de optie om een project voor Strijp-T te kiezen. \n\nHet project was extreem breed, het uitgangspunt was om concepten te ontwikkelen die kenmerken \n\nzijn voor de kernwaarden van Strijp-T. Deze zouden als visitekaartje moeten dienen voor het Strijp-T \n\nterrein. \n\nNa het bedenken van verscheidene concepten is er een conceptpresentatie gegeven waarna Boudie \n\nHoogedeure, de stakeholder, de keuze maakte voor de app die het huidige Strijp-T meer in contact \n\nmet zijn rijke verleden zou brengen. Het idee van de app is om onder andere doormiddel van AR \n\nbeelden van vroeger, van Phillips, over de huidige werkelijkheid te weergeven. Daarnaast gaf hij aan \n\ndat zij veel video materiaal hebben waarin oud Phillips medewerkers op Strijp-T ge\u00efnterviewd \n\nworden. \n\nHet idee is om hotspots te defini\u00ebren op terrein Strijp-T, waarna als de gebruiker in de buurt is deze \n\ninformatie krijgt over de huidige locatie. \n\nVoor het systeem moet een API gerealiseerd worden, deze moet een gebruiker kunnen \n\nauthentiseren, hotspots kunnen weergeven/maken/aanpassen/verwijden. \n\nDe API moet geconfigureerd worden met Swagger, zodat een interactieve documentatie aanwezig is. \n\nVerder moet het EF Migrations gebruiken om de database up-to-date te houden. \n\n \n\nEerste stappen \nHet systeem is opgezet in ASP.NET Core, bestaande uit de volgende lagen: \n\nAPI, Viewmodels, Services, Models, Repositories en Data. Waar nodig zijn \n\nook test lagen gemaakt. \n\nIn de API laag is de configuratie te vinden, ook staan hierin de endpoints en \n\nhelperclasses.  \n\nDocumentatie van de architectuur is terug te vinden in het Design \n\ndocument. \n\n \n\n  \n\n\n\n \n3 \n\nDatabase \nDe API verbindt met een MySQL database en gebruikt de ORM EntityFramework om deze aan te \n\nspreken. Verder wordt Migrations gebruikt om de database te initialiseren en up-to-date te houden \n\nmet de modellen, hiervoor wordt een code-first approach gebruikt. \n\n \n\n \n\n \n\nDoormiddel van de EF Migrations CLI kunnen we files genereren vanuit de models. Deze worden bij \n\nstart-up gecontroleerd en waar nodig wordt een database gemaakt of geupdatet.  \n\nDoor het uitvoeren van onderstaande command kan dit gedaan worden: \n\n \n\nDeze moet uitgevoerd worden in de Data folder, aangezien we deze data gescheiden houden in een \n\nandere laag dan de API, moet het start-up project gedefinieerd worden. \n\nOnderstaand de gegenereerde files die Migrations vervolgens gebruikt: \n\n \n\n \n\n  \n\n\n\n \n4 \n\nAuthenticatie, Autorisatie en veiligheid \nAuthenticatie wordt gedaan doormiddel van JWT\u2019s, om bepaalde endpoints te mogen aanspreken, \n\nzoals updaten of maken van hotspots, moet er een valide JWT bij de request zitten.  \n\nDe AuthenticateHandler.cs service faciliteert het cre\u00ebren en inloggen van accounts. Vooralsnog \n\nwordt er een standaard administrator account gemaakt bij het opstarten van de API, het inloggen \n\nkan door het aanspreken van de login endpoint in de Authcontroller.cs.  \n\nVerder worden de ingebouwde Authenticatie en autorisatie modules van ASP.NET core gebruikt om \n\nendpoints daadwerklijk te beveiligen. \n\nVooralsnog bestaat de autorisatie maar uit een rol, namelijk administrator die alle rechten heeft. \n\nOm de veiligheid te garanderen wordt SSL enforced door de client te redirecten naar de https variant \n\nvan de API. Verder wordt het wachtwoord encrypted doormiddel van de Bcrypt Library, deze \n\ngebruikt het BLOWFISH protocol. \n\n \n\nHotspot Feature \nDe Hotspot feature is ge\u00efmplementeerd, dit houdt in dat er endpoints zijn voor het verkrijgen van \n\neen lijst van hotspoten individuele hotspots. Deze returnen Viewmodels met de benodigde data voor \n\nde client.  \n\nVerder wanneer geautoriseerd, is het ook mogelijk om hotspots te verwijderen, up te daten of aan te \n\nmaken.  \n\n \n\n  \n\n\n\n \n5 \n\nInteractieve documentatie \nDoormiddel van de tool Swagger worden de endpoints en viewmodels gemapped en zijn deze \n\nbeschikbaar om uit te testen via een UI. Via Swagger is het ook mogelijk jezelf te autoriseren met \n\nbehulp van een token die je verkrijgt via de auth endpoint. \n\n \n\n  \n\n\n\n \n6 \n\nDocker \nOm het systeem samen te bundelen en makkelijke te deployen wordt Docker gebruikt. Na het \n\nbouwen van de images kan gemakkelijk via de command; docker-compose up, het systeem gestart \n\nworden.  \n\n \n\nOm dit te bewerkstelligen heb ik eerst een Dockerfile geschreven die de benodigde lagen bouwt. \n\nHieruit worden alleen de benodigde files voor het runnen van de API gehaald en er wordt een SSL \n\ncertificaat voor development gebruik aangemaakt.   \n\n \n\n  \n\n\n\n \n7 \n\nHierna heb ik een Docker-compose.yml geschreven die de image gemaakt vanuit de dockerfile en \n\neen MySQL database bundelt. Door middel van environment variables te specificeren worden deze \n\nservice verder geconfigureerd. \n\n \n\n\n"
        }
      ]
    },
    "Sprintopleveringen": {
      "hand-ins": [
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeedback \n\nMartin \nHartstikke mooi ziet er goed uit, vooral het stukje beveiliging is belangrijk goed te krijgen. Zorg dit \n\nstukje front-end,  hier had de vorige groep hele demo van wat je nu laat zien, maar andere look en \n\nfeel. Hoeverre ben je het wiel opnieuw aan het uitvinden? Het is een groter project en niet fijn dat ik \n\nhet gevoel heb dat er eerst een stap terug dan stap weer naar voren gedaan wordt. \n\nGoed om contact te leggen met de gebruiker basis van PSV zelf. Puntje: zelfde kleurstelling als die \n\nvan de website te gebruiken. Zelfde kleuren vooral. \n\nDeadline voor het registreren van event/meet. Deadline voor zwemmers, deadline voor coaches voor \n\ngoedkeuring, tweede deadline voor zwemmers tot afmelden. \n\n \n\nBritt \nOp basis waarvandaan komen de nieuwe brandguide vandaan etc. Meer contact leggen met de \n\ngebruikers groep. \n\n \n\nCees \nVeel werk gedaan, goed aan de slag. Misschien een idee om oplevering ook aan de achterban van de \n\nzwemclub te doen. Focus leggen op combineren van front-end met back-end. We kunnen werken \n\nnaar testen met de gebruikersbasis.  \n\n\n"
        },
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSprint III Oplevering \u2013 REST API \nMax van Hattum \u2013 Open Learning PSV Zwemmen \u2013 17-11-20 \n\n \n\nIntroductie \nIn dit document beschrijf ik voor mijn docenten en groepsgenoot welk werk ik gedaan heb en mijn \n\nverantwoording hiervan. Dit is betreft de REST API, oftewel de back-end.  \n\n \n\nTechnische tegenslag \nHelaas was het niet mogelijk om de demo direct om te zetten naar een functionele applicatie. Het \n\ndesign van de demo was niet met het oog op veiligheid en het verwerken van grote hoeveelheden \n\ndata ontworpen. Daarom ben ik een nieuw project begonnen, die wel veel van de zelfde technieken \n\ngebruikt, maar met een sterkere basis.  \n\n \n\nVeiligheid, Optimalisatie en Database \nEr zijn een aantal toevoegingen gemaakt zodat er op een veiligere manier met data wordt omgegaan. \n\nVoorbeelden hier van zijn het encoden van wachtwoorden en het checken of de huidige gebruiker de \n\nrechten heeft om bepaalde data op te vragen. Hierbij moet je denken aan bijvoorbeeld het feit dat \n\nniet Gebruiker A de gegevens van Gebruiker B kan opvragen.  \n\nDaarnaast aangezien het project helemaal opnieuw opgezet is heeft het een betere basis met oog op \n\nmakkelijkere beheerbaarheid, uitbreidbaarheid en snelheid. \n\nVerder is de applicatie nu zo opgezet dat deze automatisch vanuit de code in de database de tabellen \n\ngenereert als deze nog niet bestaan, en vult deze met drie standaard gebruikers: \n\n \n\n\n\nHuidige functionele logica \n\nAccount maken \nEen administrator kan een nieuw account voor iemand aan maken. Hier wordt aangegeven wat voor \n\nrol deze nieuwe gebruiker heeft; SWIMMER, COACH, ADMIN. Hierna krijgt deze gebruiker een mail \n\nmet zijn of haar account gegevens. \n\n \n\n \n\n  \n\n\n\nOphalen van Meets met inschrijvingen en goedgekeurde inschrijvingen \nDe meets kunnen allemaal tegelijkertijd opgehaald worden. Deze data bevat naast de Meet info ook \n\neen lijst met \u2018PendingRequests\u2019 en \u2018ParticipatingAthletes\u2019. Daarnaast kunnen ook alle Meets horende \n\nbij een zwemmer opgehaald worden.  \n\n \n\n \n\nBeheren van inschrijvingen door Zwemmer en goedkeuringen door Coach \nEen zwemmer kan zich inschrijven op een Event, ook kan deze weer teruggetrokken worden. Daarna \n\nkan de coach de inschrijving goedkeuren of afkeuren. Hierna kan een zwemmer zich alsnog \n\nterugtrekken. \n\n \n\n  \n\n\n\nDeployment \nDe back-end kan heel gemakkelijk op praktisch elk soort OS (onder andere CentOs) gehost worden. \n\nDe applicatie kan zo gebuild worden dat er een bestand is, een zogenaamd .war bestand, die meteen \n\nopgestart kan worden zonder verdere installatie. Wel moet er een MySQL database beschikbaar zijn, \n\nof opgezet worden, met een schema genaamd \u2018psvzwemmen\u2019 en een gebruiker met credentials root \n\n\u2013 root. \n\nDeze credentials kunnen echter gewoon aangepast worden, net als de connectie met de database. \n\nDe database hoeft daardoor niet perse lokaal gehost te worden, al raad ik dit wel aan. \n\n \n\nDatabase ontwerp \n\n \n\n \n\nSuggesties verdere stappen \n\nMail \nHet account waarmee de automatische mails gemaakt worden moet over gezet worden naar een \n\naccount van de stakeholder. Hiervoor moet een account gemaakt worden op SendGrid, zodat dit in \n\nde applicatie omgezet kan worden. \n\n\n\n \n\nAanmaken/Importeren van Meets \nEen administrator moet Meets kunnen aanmaken door een lenex file te importeren. \n\n \n\nKoppelen zwemmer aan coach \nEen zwemmer zou uit een lijst een coach kunnen uitkiezen om zich bij aan te melden, waarna de \n\ncoach dit kan accepteren en/of vice versa. Hierna kan de logica zou aangepast worden dat alleen de \n\nzwemmer zijn of haar eigen coach de goedkeuringen van inschrijvingen beheert. \n\n \n\nTrainingen \nEen coach zou trainingen moeten kunnen aanmaken en hier zwemmers aan kunnen toevoegen, \n\nverder zouden zwemmers zich moeten kunnen inschrijven op trainingen en de coach ze kunnen \n\ntoelaten. Ook moet een coach op de dag van, of na, de training de aanwezigheid van zwemmers \n\nkunnen noteren. \n\nZwemmers moeten een overzicht van beschikbare trainingen kunnen zien en een overzicht van \n\ningeschreven trainingen. \n\n \n\nNotificaties \nEen administrator moet een bericht kunnen publiceren die zichtbaar is voor alle gebruikers of \n\ngeselecteerde gebruikers. Gebruikers moeten aan hun geadresseerde berichten kunnen inzien en \n\nverwijderen. \n\n \n\n\n"
        },
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPSV Zwemmen applicatie\n\n\nPSV Zwemmen applicatie\nDoor Max van Hattum & Stephanie Bolder\n\n\nProbleemstelling\n\nIngewikkeld proces registratie wedstrijden\n\nVeel menselijke components\n\nOnvriendelijke gebruikerservaring\n\nPSV Zwemmen applicatie\n\n\n\n\n\n\n\nFront-end\nBrandguide\n\tDesign standaarden, zoals fonts & kleurenpalet\n\nClickable prototype - XD\n\tGetest bij de doelgroep met UX & UI tests\n\t\tAlleen zwemmers getest\n\nClickable prototype \u2013 code\n\tOnboarding\n\tZwemmers\u2019 profiel\n\tRegistratie voor evenement\n\tNotificaties\nInterface designs\n\n\n\nBack-end\n\nNiet mogelijk om demo direct om te zetten naar functionele applicatie\n\nDesign niet met het oog op veiligheid en het verwerken van grote hoeveelheden data\n\n\nNieuw project begonnen met een sterkere basis\n\tVeiligheid & Optimalisatie\n\tAccount maken\n\tOphalen van Meets\n\tInschrijvingen & goedkeuringen\n\tDeployment\n\nTechnische tegenslag\n\n\nBack-end\n\nEncoden van wachtwoorden\nChecken of de gebruiker rechten heeft om data op te vragen\nGebruiker A mag niet de gegevens van Gebruiker B opvragen\n\nTabellen worden gegenereerd als deze nog niet bestaan, en vult deze aan met drie standaard gebruikers\n\tAdmin, Coach & Swimmer\n\n\nVeiligheid, Optimalisatie en Database\n\n\n\nBack-end\n\nDe administrator kan een nieuw account aanmaken\nHierbij wordt aangegeven wat voor rol de nieuwe gebruiker heeft\nAdmin, Coach of Swimmer\n\nDe gebruiker krijgt hierna een mail met zijn / haar account gegevens\n\nAccount maken\n\n\n\n\nBack-end\n\nAlle meets kunnen tegelijkertijd opgehaald worden\nOok een lijst met PendingRequests en ParticipatingAthletes\n\tPendingRequests = lopende registraties van de atleten\n\tParticipatingAthletes = atleten die mee doen aan de meet\nOok alle meets horende bij zwemmers\n\n\nOphalen van Meets\n\n\n\nBack-end\n\nBeheren van inschrijvingen van zwemmer\nZwemmer kan zich inschrijven & uitschrijven voor een event\n\nBeheren van goedkeuringen van coach\nCoach kan inschrijvingen van zwemmers goedkeuren \nen afwijzen\n\n\nInschrijvingen & goedkeuring\n\n\n\nBack-end\n\nBack-end kan gehost worden op elk soort OS\nOnder andere op CentOS\n\nDe database hoeft niet perse lokaal gehost worden, maar wordt wel aangeraden\n\nDeployment\n\n\nVolgende stappen\n\nFront-end\n\tCoach & Admin interfaces testen\n\tCoach & Admin interfaces implementeren\n\tFront-end koppelen aan back-end\n\nBack-end\n\tAanmaken / importeren van Meets\n\tKoppelen zwemmers aan coach\n\tTrainingen aanmaken & zwemmers toevoegen\n\tNotificaties publiceren, inzien & verwijderen\n\n\nSprint IIII\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/docProps/thumbnail.jpeg\n\n"
        }
      ]
    },
    "TranslerendemonaarfunctioneleAppobsolete": {
      "hand-ins": [
        {
          "text": "Feedback\nMartin\nHartstikke mooi ziet er goed uit, vooral het stukje beveiliging is belangrijk goed te krijgen. Zorg dit stukje front-end,  hier had de vorige groep hele demo van wat je nu laat zien, maar andere look en feel. Hoeverre ben je het wiel opnieuw aan het uitvinden? Het is een groter project en niet fijn dat ik het gevoel heb dat er eerst een stap terug dan stap weer naar voren gedaan wordt.\nGoed om contact te leggen met de gebruiker basis van PSV zelf. Puntje: zelfde kleurstelling als die van de website te gebruiken. Zelfde kleuren vooral.\nDeadline voor het registreren van event/meet. Deadline voor zwemmers, deadline voor coaches voor goedkeuring, tweede deadline voor zwemmers tot afmelden.\n\nBritt\nOp basis waarvandaan komen de nieuwe brandguide vandaan etc. Meer contact leggen met de gebruikers groep.\n\nCees\nVeel werk gedaan, goed aan de slag. Misschien een idee om oplevering ook aan de achterban van de zwemclub te doen. Focus leggen op combineren van front-end met back-end. We kunnen werken naar testen met de gebruikersbasis. "
        },
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSprint III Oplevering \u2013 REST API \nMax van Hattum \u2013 Open Learning PSV Zwemmen \u2013 17-11-20 \n\nTechnische tegenslag \nHelaas was het niet mogelijk om de demo direct om te zetten naar een functionele applicatie. Het \n\ndesign van de demo was niet met het oog op veiligheid en het verwerken van grote hoeveelheden \n\ndata ontworpen. Daarom ben ik een nieuw project begonnen, die wel veel van de zelfde technieken \n\ngebruikt, maar met een sterkere basis.  \n\n \n\nVeiligheid, Optimalisatie en Database \nEr zijn een aantal toevoegingen gemaakt zodat er op een veiligere manier met data wordt omgegaan. \n\nVoorbeelden hier van zijn het encoden van wachtwoorden en het checken of de huidige gebruiker de \n\nrechten heeft om bepaalde data op te vragen. Hierbij moet je denken aan bijvoorbeeld het feit dat \n\nniet Gebruiker A de gegevens van Gebruiker B kan opvragen.  \n\nDaarnaast aangezien het project helemaal opnieuw opgezet is heeft het een betere basis met oog op \n\nmakkelijkere beheerbaarheid, uitbreidbaarheid en snelheid. \n\nVerder is de applicatie nu zo opgezet dat deze automatisch vanuit de code in de database de tabellen \n\ngenereert als deze nog niet bestaan, en vult deze met drie standaard gebruikers: \n\n \n\n  \n\n\n\n \n\n \n\nHuidige functionele logica \n\nAccount maken \nEen administrator kan een nieuw account voor iemand aan maken. Hier wordt aangegeven wat voor \n\nrol deze nieuwe gebruiker heeft; SWIMMER, COACH, ADMIN. Hierna krijgt deze gebruiker een mail \n\nmet zijn of haar account gegevens. \n\n \n\n \n\n  \n\n\n\nOphalen van Meets met inschrijvingen en goedgekeurde inschrijvingen \nDe meets kunnen allemaal tegelijkertijd opgehaald worden. Deze data bevat naast de Meet info ook \n\neen lijst met \u2018PendingRequests\u2019 en \u2018ParticipatingAthletes\u2019. Daarnaast kunnen ook alle Meets horende \n\nbij een zwemmer opgehaald worden.  \n\n \n\n \n\nBeheren van inschrijvingen door Zwemmer en goedkeuringen door Coach \nEen zwemmer kan zich inschrijven op een Event, ook kan deze weer teruggetrokken worden. Daarna \n\nkan de coach de inschrijving goedkeuren of afkeuren. Hierna kan een zwemmer zich alsnog \n\nterugtrekken. \n\n \n\n  \n\n\n\nDeployment \nDe back-end kan heel gemakkelijk op praktisch elk soort OS (onder andere CentOs) gehost worden. \n\nDe applicatie kan zo gebuild worden dat er een bestand is, een zogenaamd .war bestand, die meteen \n\nopgestart kan worden zonder verdere installatie. Wel moet er een MySQL database beschikbaar zijn, \n\nof opgezet worden, met een schema genaamd \u2018psvzwemmen\u2019 en een gebruiker met credentials root \n\n\u2013 root. \n\nDeze credentials kunnen echter gewoon aangepast worden, net als de connectie met de database. \n\nDe database hoeft daardoor niet perse lokaal gehost te worden, al raad ik dit wel aan. \n\n \n\nSuggesties verdere stappen \n\nMail \nHet account waarmee de automatische mails gemaakt worden moet over gezet worden naar een \n\naccount van de stakeholder. Hiervoor moet een account gemaakt worden op SendGrid, zodat dit in \n\nde applicatie omgezet kan worden. \n\n \n\nAanmaken/Importeren van Meets \nEen administrator moet Meets kunnen aanmaken door een lenex file te importeren. \n\n \n\nKoppelen zwemmer aan coach \nEen zwemmer zou uit een lijst een coach kunnen uitkiezen om zich bij aan te melden, waarna de \n\ncoach dit kan accepteren en/of vice versa. Hierna kan de logica zou aangepast worden dat alleen de \n\nzwemmer zijn of haar eigen coach de goedkeuringen van inschrijvingen beheert. \n\n \n\nTrainingen \nEen coach zou trainingen moeten kunnen aanmaken en hier zwemmers aan kunnen toevoegen, \n\nverder zouden zwemmers zich moeten kunnen inschrijven op trainingen en de coach ze kunnen \n\ntoelaten. Ook moet een coach op de dag van, of na, de training de aanwezigheid van zwemmers \n\nkunnen noteren. \n\nZwemmers moeten een overzicht van beschikbare trainingen kunnen zien en een overzicht van \n\ningeschreven trainingen. \n\n \n\nNotificaties \nEen administrator moet een bericht kunnen publiceren die zichtbaar is voor alle gebruikers of \n\ngeselecteerde gebruikers. Gebruikers moeten aan hun geadresseerde berichten kunnen inzien en \n\nverwijderen. \n\n \n\n\n"
        },
        {
          "text": "\n\nIntroduction\nFor Open Learning we get the opportunity to work for an external stakeholder, giving context to our learning process. I\u2019ve chosen to work on an application for PSV Swimming. The main features this app needs to offer are enabling swimmers to manage their event registrations, coaches to review this registrations and administrators to see an overview of these registrations so they can process them.\nA previous Fontys group already worked on this project, leaving behind a demo existing of a PWA and REST API. This demo uses mock data, which is non persistent, to showcase how such an application would look like.\nMy task is to analyse the REST API and identify what is needed to translate this to a fully functional API taking into account security, scalability and maintainability.\n\nResearch method \n\n\n\n\nConclusie\n\n\nGlossary\n\n"
        },
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPSV Zwemmen applicatie\n\n\nPSV Zwemmen applicatie\nDoor Max van Hattum & Stephanie Bolder\n\n\nProbleemstelling\n\nIngewikkeld proces registratie wedstrijden\n\nVeel menselijke components\n\nOnvriendelijke gebruikerservaring\n\nPSV Zwemmen applicatie\n\n\n\n\n\n\n\nFront-end\nBrandguide\n\tDesign standaarden, zoals fonts & kleurenpalet\n\nClickable prototype - XD\n\tGetest bij de doelgroep met UX & UI tests\n\t\tAlleen zwemmers getest\n\nClickable prototype \u2013 code\n\tOnboarding\n\tZwemmers\u2019 profiel\n\tRegistratie voor evenement\n\tNotificaties\nInterface designs\n\n\n\nBack-end\n\nNiet mogelijk om demo direct om te zetten naar functionele applicatie\n\nDesign niet met het oog op veiligheid en het verwerken van grote hoeveelheden data\n\n\nNieuw project begonnen met een sterkere basis\n\tVeiligheid & Optimalisatie\n\tAccount maken\n\tOphalen van Meets\n\tInschrijvingen & goedkeuringen\n\tDeployment\n\nTechnische tegenslag\n\n\nBack-end\n\nEncoden van wachtwoorden\nChecken of de gebruiker rechten heeft om data op te vragen\nGebruiker A mag niet de gegevens van Gebruiker B opvragen\n\nTabellen worden gegenereerd als deze nog niet bestaan, en vult deze aan met drie standaard gebruikers\n\tAdmin, Coach & Swimmer\n\n\nVeiligheid, Optimalisatie en Database\n\n\n\nBack-end\n\nDe administrator kan een nieuw account aanmaken\nHierbij wordt aangegeven wat voor rol de nieuwe gebruiker heeft\nAdmin, Coach of Swimmer\n\nDe gebruiker krijgt hierna een mail met zijn / haar account gegevens\n\nAccount maken\n\n\n\n\nBack-end\n\nAlle meets kunnen tegelijkertijd opgehaald worden\nOok een lijst met PendingRequests en ParticipatingAthletes\n\tPendingRequests = lopende registraties van de atleten\n\tParticipatingAthletes = atleten die mee doen aan de meet\nOok alle meets horende bij zwemmers\n\n\nOphalen van Meets\n\n\n\nBack-end\n\nBeheren van inschrijvingen van zwemmer\nZwemmer kan zich inschrijven & uitschrijven voor een event\n\nBeheren van goedkeuringen van coach\nCoach kan inschrijvingen van zwemmers goedkeuren \nen afwijzen\n\n\nInschrijvingen & goedkeuring\n\n\n\nBack-end\n\nBack-end kan gehost worden op elk soort OS\nOnder andere op CentOS\n\nDe database hoeft niet perse lokaal gehost worden, maar wordt wel aangeraden\n\nDeployment\n\n\nVolgende stappen\n\nFront-end\n\tCoach & Admin interfaces testen\n\tCoach & Admin interfaces implementeren\n\tFront-end koppelen aan back-end\n\nBack-end\n\tAanmaken / importeren van Meets\n\tKoppelen zwemmers aan coach\n\tTrainingen aanmaken & zwemmers toevoegen\n\tNotificaties publiceren, inzien & verwijderen\n\n\nSprint IIII\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/docProps/thumbnail.jpeg\n\n"
        }
      ]
    },
    "VueAppGoogleMaps": {
      "hand-ins": [
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\nStrijp-T Hotspot  - Vue Map \nFontys Delta Semester 4 \n\nMax van Hattum \n\n13-4-2021\n\n\n\n \n1 \n\nVersion control \nVersion Author Date Adjustments \nV0.1 Max van Hattum  Initial document \n\nsetup, and description \nmaps \n\nV0.2 Max van Hattum  Testing description \nand outcomes \n\n \n\nInhoudsopgave \nVersion control ........................................................................................................................................ 1 \n\nIntroductie ............................................................................................................................................... 2 \n\nMaps ........................................................................................................................................................ 3 \n\nTesten ...................................................................................................................................................... 3 \n\n \n\n \n\n  \n\n\n\n \n2 \n\nIntroductie \nBinnen het Delta programma van Fontys kregen wij de optie om een project voor Strijp-T te kiezen. \n\nHet project was extreem breed, het uitgangspunt was om concepten te ontwikkelen die kenmerken \n\nzijn voor de kernwaarden van Strijp-T. Deze zouden als visitekaartje moeten dienen voor het Strijp-T \n\nterrein. \n\nNa het bedenken van verscheidene concepten is er een conceptpresentatie gegeven waarna Boudie \n\nHoogedeure, de stakeholder, de keuze maakte voor de app die het huidige Strijp-T meer in contact \n\nmet zijn rijke verleden zou brengen. Het idee van de app is om onder andere doormiddel van AR \n\nbeelden van vroeger, van Phillips, over de huidige werkelijkheid te weergeven. Daarnaast gaf hij aan \n\ndat zij veel video materiaal hebben waarin oud Phillips medewerkers op Strijp-T ge\u00efnterviewd \n\nworden. \n\nHet idee is om hotspots te defini\u00ebren op terrein Strijp-T, waarna als de gebruiker in de buurt is deze \n\ninformatie krijgt over de huidige locatie. \n\nHiervoor is het nodig om in de applicatie een kaart te hebben waarop de hotspots en gebruikers als \n\nmarkers te zien zijn. \n\n  \n\n\n\n \n3 \n\nMaps \nEr waren verscheidene opties zoals Google Maps API, OpenLayers en TomTom. Aangezien de pricing \n\npraktisch hetzelfde was en ze allemaal gratis varianten aan bieden was dit geen belangrijke factor. Ik \n\nhad wel al eerder met Google Maps API gewerkt, en hier was ook goede documentatie voor \n\nbeschikbaar, vandaar dat ik hier uiteindelijk voor gekozen heb. \n\n \n\nMarkers worden gegenereerd vanuit een prop die de MapComponent krijgt van de Home pagina. Er \n\nis ook een livetracking feature, wanneer de persoon in de buurt is van Strijp-T, focust het scherm op \n\nwaar de gebruiker is. Op het moment zie je mij in Tilburg. \n\nTesten \nEr is specifiek voor gekozen om niet automatische testen te schrijven, dit omdat het niet mogelijk \n\nleek te zijn om programmatisch deze geocoordinaten aan te passen. Het is dan wel mogelijk om een \n\nMock object in te voeren, maar dan begin je de gegeven API en functionaliteit te testen wat niet \n\nwenselijk is en weinig toegevoegde waarde heeft. \n\nOm de feature toch te testen zijn de Chrome Dev Tools gebruikt, in specifiek om met de Sensor tool \n\nde locatie aan te passen. De locatie van de gebruiker wordt real-time ge\u00fcpdatet als de waardes met \n\nde pijltjes ernaast worden verandert, zie onderstaand twee verschillende locaties van de gebruiker. \n\n \n\n\n\n \n4 \n\n \n\n \n\n \n\n\n"
        }
      ]
    },
    "VueAppkoppelingbackend": {
      "hand-ins": [
        {
          "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\nStrijp-T Hotspot  - Vue Koppeling Back-end \nFontys Delta Semester 4 \n\nMax van Hattum \n\n26-2-2021\n\n\n\n  \n\nInhoudsopgave \nIntroductie ............................................................................................................................................... 3 \n\nKoppeling Back-end ................................................................................................................................. 4 \n\n \n\n \n\n  \n\n\n\n  \n\nIntroductie \nBinnen het Delta programma van Fontys kregen wij de optie om een project voor Strijp-T te kiezen. \n\nHet project was extreem breed, het uitgangspunt was om concepten te ontwikkelen die kenmerken \n\nzijn voor de kernwaarden van Strijp-T. Deze zouden als visitekaartje moeten dienen voor het Strijp-T \n\nterrein. \n\nNa het bedenken van verscheidene concepten is er een conceptpresentatie gegeven waarna Boudie \n\nHoogedeure, de stakeholder, de keuze maakte voor de app die het huidige Strijp-T meer in contact \n\nmet zijn rijke verleden zou brengen. Het idee van de app is om onder andere doormiddel van AR \n\nbeelden van vroeger, van Phillips, over de huidige werkelijkheid te weergeven. Daarnaast gaf hij aan \n\ndat zij veel video materiaal hebben waarin oud Phillips medewerkers op Strijp-T ge\u00efnterviewd \n\nworden. \n\nHet idee is om hotspots te defini\u00ebren op terrein Strijp-T, waarna als de gebruiker in de buurt is deze \n\ninformatie krijgt over de huidige locatie. \n\nOm de data te managen is er een Rest API, hieraan moet de Vue app dus gekoppeld worden. \n\n \n\n  \n\n\n\n  \n\nKoppeling Back-end \nOm de hotspots op te vragen bij de back-end moeten er API calls gedaan worden via HTTPS, deze \n\nrequest zouden gedaan kunnen worden met de standaard Fetch API. Er zijn echter ook libraries \n\naanwezig die het proces versimpelen. Denk hierbij aan makkelijker gebruik en ingebouwde error \n\nhandling. Axios is een erg populaire library, deze gebruik ik dan ook om requests te maken. \n\n \n\n \n\nIn een aparte file instantieer ik de methodes die in de rest van de applicaties gebruikt kunnen \n\nworden. Ik cre\u00eber een Axios instantie die een base URL gebruikt waarnaar de requests verzonden \n\nworden. Verder waar nodig wordt een Authorization header toegevoegd. \n\n  \n\n\n\n  \n\nDan wanneer een component gemaakt wordt, roept deze de bijbehorende methode aan om de data \n\nte krijgen. \n\n \n\n \n\n\n"
        }
      ]
    }
  }
}