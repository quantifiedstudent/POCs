{
    "18233": {
        "AddingimportLenexfilefunctionality": {
            "hand-ins": [null]
        },
        "AdviseonMessageBrokerSystem": {
            "hand-ins": [{
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\nAdvise Message Brokers \nFor a notification system implementation for the DeX platform \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nMax van Hattum, Martin Markov \n\n5-10-2020 \n\n  \n\n \n\n\n\nTable of Contents \nVersion History 1 \n\nIntroduction 2 \n\nResearch questions 2 \n\nMain question 2 \n\nSub questions 2 \n\nDOT Framework 3 \n\nWhat 3 \n\nHow 3 \n\nWhy 3 \n\nWhat does the DeX platform want to achieve? 3 \n\nWhat is a message broker? 3 \n\nWhy use a message broker? 4 \n\nWhat are important aspects of a message broker service? 4 \n\nWhat are currently available message broker services? 4 \n\nComparing the important aspects of these services 5 \n\nVerifying requirements with a prototype 5 \n\nRabbitMQ 6 \n\nWorker 6 \n\nTask 8 \n\nResult 9 \n\nApache Kafka 11 \n\ndocker-compose.yml 11 \n\nProducer 12 \n\nConsumer 13 \n\nResult 13 \n\nFinal comparison 14 \n\nConclusion 14 \n\nReferences 15 \n\n \n\n \n\n \n\n\n\nVersion History \n\n \n\n \n\n \n\n1 \n \n\nVersion Date Author Changes \n\nV1 5-10-2020 Max van Hattum Setup document, introduction, research \nquestions, what is a message broker, \naspects and start of currently available \nservices. \n\nV2 11-10-2020 Martin Markov Answered to the question why we need a \nmessage broker and added comparison \ntable of the different options plus \nconclusion \n\nV3 13-10-2020 Max van Hattum Added Validating requirements with a \nprototype  \n\nv4 18-11-2020 Max van Hattum Added DOT framework section and sub \nquestion regarding DeX specifically. \n\nv5 20-11-2020 Martin Markov Added PoC with Apache Kafka and \ncompared it to RabbitMQ plus small other \nimprovements \n\n\n\nIntroduction \nFor the DeX platform it will be necessary to send users notifications by means of different message \n\nservices. For this to be released one component is needed to manage notifications intended for \ndifferent subscribers. This component should not only store the data to be sent but should also be \n\nresponsible for load balancing and making sure messages are delivered.  \n\nMany existing services implement a so-called message broker service to handle this. (Google, z.d.) \n\n \n\nResearch questions \nMain question \nTo give direction to this research a main question to be answered will be formulated. There are \n\nseveral aspects that need to be kept in mind when researching what the preferred solution for the \nDeX platform will be. The solution should be secure, scalable, functional, and compatible with the \ncurrent system. Keeping these criteria in mind results in the following research question: \u200b\u201cWhat is a \nsecure, scalable and compatible message broker service best suited for the DeX platform?\u201d \n\nSub questions \nTo get to the answer to this question, the subject will be divided in multiple smaller subjects. The \n\nfollowing questions will be answered: \n\n- What does the DeX platform want to achieve? \n- What is a message broker? \n\n- Why use a message broker service? \n\n- What are important aspects of a message broker service? \n- What are currently available message broker services? \n\n- How do currently available message broker services compare? \n\n  \n\n2 \n \n\n\n\nDOT Framework \nWhat \nThis research is going to take place in both the application domain and the available work domain. \nThe current software system of the DeX Platform needs to be analysed to get the best service fit for \n\nthe context. However, before this can be done research needs to be done about message broker \n(services) in general so that a better understanding can be achieved, allowing us to find the best fit \n\nfor the current context.  \n\n \n\nHow \nTo get an overview of what exactly we are trying to achieve we first applied the \u200bField\u200b method in a \ntalk with one of the stakeholders and project leader Brend Smids and another team member Niray \nMak. Then we focused heavily on the \u200bLibrary \u200bmethod to gather existing information and expertise \nregarding message brokers. Finally, we compared the gathered information, and setup prototypes for \n\nthe most promising options which belongs to the \u200bShowroom\u200b research. \n\n \n\nWhy \nThese methods were chosen because we want to attain a good overview of what is needed for the \nDeX Platform and how existing work might already solve the issue. When this is clear we can focus on \n\nachieving the best fit for the context, leaving room for potential improvements. \n\n \n  \n\n3 \n \n\n\n\nWhat does the DeX platform want to achieve? \nTo get a grip on what the DeX Platform wants, a meeting was scheduled with Brend Smits, Niray Mak, \n\nMartin Markov and Max van Hattum. There Brend explained that currently there is no way to send \nnotifications to users. He wanted a scalable solution for this keeping in mind that maybe in the future \n\nmultiple ways of sending notifications are going to be used. \n\nHe went on to describe how currently the architecture of the REST API is a monolithic structure, but \n\nhow they might want to gradually change to a more microservice oriented architecture. He wants a \nsolution where a service in the API can register notifications to a message broker, and then let the \n\nmessage broker handle things from that point, distributing it to a specified notification service.  \n\nMoreover, he stated that he would prefer it if this was all locally hosted, using Docker and that the \n\nsolution should be simple to use.  \n\n \n\nWhat is a message broker? \nA message broker is sometimes also called Integration Broker or interface engine. It is a service that \nminimally message transformation and routing services, communication program to program. \n\n(Gartner, z.d.) \n\nMost often it is able to store messages, keep track of which messages need to be delivered and \n\nbalance the load of delivering messages. \n\n  \n\nWhy use a message broker? \nMessage brokers make the process of data exchange simple and reliable. They use different \n\nprotocols that show how the message should be transmitted, processed and consumed. They allow \nasynchronous communication which allows both the producer and the consumer to interact directly \nwith the message broker and not between each other. While a producer can enqueue new messages \n\na consumer may read from it simultaneously without blocking it. Message brokers also allow us to \n\nbetter scale the communication between different services on demand. \n\n \n\nIn the context of DeX, we are planning to use a message broker so we can process notifications to our \nusers asynchronously. The producer of the eventual message broker will be our API which will \n\nenqueue the notification message, how it should be sent (ex. via email), to which user and when \n\nshould it be sent. The consumer will be the service responsible for sending the message. \n\n \n\nWhat are important aspects of a message broker service? \nForemost the message broker service should be able to validate, transform and route messages. The \n\nmain goal of the message broker architecture is decoupling programs while facilitating \n\ncommunication between these programs, while keeping them unaware of each other\u200b (Ejsmont, \n2015, pp. 275\u2013276).  \n\nMoreover cost, scalability, compatibility, hosting, ease of use and speed are important subjects to \n\nresearch. \n\n4 \n \n\n\n\n \n\nWhat are currently available message broker services? \nAWS SQS \n\n- 1 million requests free \n- Hosted by Amazon \n- Compatible with .NET \n\n- Good documentation \nhttps://docs.aws.amazon.com/sdk-for-net/v3/developer-guide/sqs-apis-intro.html \n\n- Official Library available \u200bhttps://aws.amazon.com/sqs/?did=ft_card&trk=ft_card \n\nGoogle Cloud Pub/Sub \n\n- 10GB gratis, 40$ per 1TiB after \n\n- Hosted by google \n- Compatible with .NET \n- Great documentation: \u200bhttps://cloud.google.com/pubsub/docs/apis \n- Official Library available \n\nRabbitMQ \n\n- Own hosting, hosting specific pricing \n- Good documentation: \u200bhttps://www.rabbitmq.com/tutorials/tutorial-one-dotnet.html \n- Official Library available for .NET \n\n- Open source \n- Big community \n \n\nPulsar 2.0 \n\n- Own hosting, hosting specific pricing \n\n- Great documentation: \u200bhttps://pulsar.apache.org/docs/en/pulsar-2.0/ \n- Ported library available for ASP.NET \n- Open source \n\n \nApache Kafka \n\n- Own hosting, hosting specific pricing \n\n- Good documentation: \u200bhttps://kafka.apache.org/documentation/ \n- Community library available for .NET \n- Open source \n\n \n\n  \n\n5 \n \n\nhttps://docs.aws.amazon.com/sdk-for-net/v3/developer-guide/sqs-apis-intro.html\nhttps://aws.amazon.com/sqs/?did=ft_card&trk=ft_card\nhttps://cloud.google.com/pubsub/docs/apis\nhttps://www.rabbitmq.com/tutorials/tutorial-one-dotnet.html\nhttps://pulsar.apache.org/docs/en/pulsar-2.0/\nhttps://kafka.apache.org/documentation/\n\n\nComparing the important aspects of these services \nTo compare the services, we take into account several aspects; ease of use, scalability, hosting, cost, \n\nsupport and if the service is open source. The project leader communicated that own hosting and \n\nease of use are the most important aspects. Cost certainly plays an important role too.  \n\n \n\n \n\n  \n\n6 \n \n\n Ease of use Scalability Hosting Cost Open Source Support \n\n \nAWS SQS \n\n \nGood \n\n \n\n \nGreat \n\n \nAmazon \n\n$0.50 per \nmillion \n\nrequests \n\n \nNo \n\n \nAverage \n\nGoogle \nCloud \n\nPub/Sub \n\n \nGreat \n\n \n\n \nGreat \n\n \n\n \nGoogle \n\n \n$40 per TB \n\n \nNo \n\n \nGood \n\n \nRabbitMQ \n\n \nGreat \n\n \n\n \nGreat \n\nOwn / \nhosting \noptions \n\n \nFree \n\n \nYes \n\n \nGreat \n\n \nPulsar 2.0 \n\n \n\n \nGreat \n\n \nGood \n\n \nOwn \n\n \nFree \n\n \nYes \n\n \nGood \n\nApache \nKafka \n\n \nGood \n\n \n\n \nGreat \n\nOwn / \nhosting \noptions \n\n \nFree \n\n \nYes \n\n \nGood \n\n\n\nVerifying requirements with a prototype \nTo verify that the requirements are being met and to analyse the resource usage by the service, we \n\nset up a local demo.  \n\nRabbitMQ \nThe system requirements are not high, a minimum of 256mb of RAM always needs to be free and at \n\nleast 50MB of disk space must always be available to prevent failures. \u200b(RabbitMQ, z.d.) \n\nInstallation for a demo is simple when using docker: \n\ndocker run -it --rm --name rabbitmq -p 5672:5672 -p 15672:15672 rabbitmq:3-management \n\nThis includes a monitoring tool (not fit for production, but other tools are available for this). \n\nRunning this exposes the rabbitmq service on the default port and gives access to the \n\nmonitor tool with credentials: \u200bguest - guest. \n\nTo validate that the service fulfils the needs of the system and to monitor system usage it is \n\nnecessary to set up a publisher and one or more clients. The publisher will register multiple \n\nmessages, the clients will consume them. \n\nSince the clients will be doing a task based on a message, we have incorporated the \n\ncompeting consumer pattern. A consumer will only get a new task when it is finished with \n\nanother. \n\nTwo windows console apps are created, one for publishing one thousand messages and one \n\nfor receiving.  \n\n7 \n \n\n\n\nConsumer \n\n \n\n \n\n  \n\n8 \n \n\n\n\nProducer \n\n \n\n \n\n  \n\n9 \n \n\n\n\nResult \n\nStart up two workers by entering \u200bdotnet run\u200b  \u200binto a console opened in the folder where the worker \napp resides. Then start up one publisher with a message with \u200bdotnet run Message:..  \n\nThe number of dots represent the amount of seconds this task takes to run. \n\nThe broker now adds one thousand messages to the queue and sends them one by one to connected \n\nconsumers. It only sends a new message after it has received acknowledgment from the worker that \n\nthe task is completed.  \n\n \n\nAs you can see the messages are distributed between the workers. When checking out the \n\nmonitoring tool for ram and disk space usages, the values stay low.  \n\n \n\nBefore registering messages \n\n \n\n10 \n \n\n\n\n \n\nAfter registering messages with 2 seconds delay per task \n\n \n\n \n\nAfter subscribing six more workers \n\n  \n\n11 \n \n\n\n\nApache Kafka \nWhile there are not official system requirements on the official documentation, Confluent, the most \n\nfamous platform to use Kafka with, is recommending a minimum of 6GB RAM.  \n\nTo run Kafka locally we have to run Apache ZooKeeper alongside with it for maintaining the \nconfiguration information. To ease the process of local configuration and ensure that the correct \nservices are used we are going to use Docker and docker-compose. We have set-up the following  \n\ndocker-compose.yml \n\u00a0\n\n \n\u00a0\nThen to run it we are going to execute \u200bdocker-compose up\u200b . \n \nFrom this image it can be seen that the configuration of the Kafka is not as straight forward as the \none for running RabbitMQ. Also, there is not a monitoring tool coming out-of-the-box as the one that \nRabbitMQ provides, so the performance is not easily measurable. \n \nTo produce the same example as with RabbitMQ we are going to implement a pub-sub \ncommunication. \n\n12 \n \n\n\n\nProducer \n\n \n\n \n\n13 \n \n\n\n\nConsumer \n\n \n\n \n\nResult \n\n \n \n\n14 \n \n\n\n\nIn the result we can see that we launched one producer with two consumer instances which read \nfrom the same Kafka topic. \n\nFinal comparison \nWhile both tools could be used as a message broker, it appears that RabbitMQ is the better solution \n\nfor DeX as it is only a message broker and it is doing its job really efficient. Kafka is a great platform, \nbut it adds plenty of overhead when used only as a message broker which is the use case of DeX. \n\nPlus, the system requirements are much higher compared to RabbitMQ. \n\n \n\nConclusion \nWe conclude that based on the comparison of the different technologies and the research, the \nselected technology to use for the message broker of the DeX notification system is \u200bRabbitMQ\u200b. We \nadvise this service because it supports its own hosting, has good integration for .NET Core and has an \n\nactive community surrounding the service.  \n\n15 \n \n\n\n\nReferences \n\nEjsmont, A. (2015). \u200bWeb Scalability for Startup Engineers\u200b. McGraw-Hill Education. \n\nGartner. (z.d.). \u200bDefinition of IB (Integration Broker) - Gartner Information Technology \n\nGlossary\u200b. Geraadpleegd 5 oktober 2020, van \n\nhttps://www.gartner.com/en/information-technology/glossary/ib-integration\n\n-broker \n\nGoogle. (z.d.). \u200bCloud Pub/Sub |\u200b. Google Cloud. Geraadpleegd 5 oktober 2020, van \n\nhttps://cloud.google.com/pubsub#customers \n\n \n\n16 \n \n\n\n"
            }]
        },
        "AnalyseandDesignnotificationsystem": {
            "hand-ins": [{
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n1 \n\nC4 Notification System \nDeX Project  \n\nMartin Markov \u2013 Max van Hattum \n\n  \n\n\n\n \n2 \n\nInhoudsopgave \nContext Diagram ...................................................................................................................................... 2 \n\nContainer Diagram .................................................................................................................................. 3 \n\nComponent Diagram ............................................................................................................................... 4 \n\nCode Diagram .......................................................................................................................................... 5 \n\n \n\n \n\nContext Diagram \nA message broker is used to decouple the email service from the web application. The email service \n\nthen handles sending the notification to the email distributor which in turn sends the notification as \n\nan email. \n\n \n\n  \n\n\n\n \n3 \n\nContainer Diagram \nThe DeX Backend will be responsible for registering notifications to be send. Then the message \n\nbroker passes this to subscribed email services. \n\n \n\n  \n\n\n\n \n4 \n\nComponent Diagram \nThere is a NotificationService available to register messages to the RabbitMQ message broker. Within \n\nthe Notification Service there is a SubscribeService responsible for making a connection to the \n\nRabbitMQ message broker, and a ListenerService for listening to new messages and triggering the \n\nEmailService. \n\n \n\n \n\n\n\n \n5 \n\nCode Diagram \nIn the notification service the following classes are most important for the functionality. \n\n \n\n\n\n \n6 \n\nFor registering a notification the following class can be imported and used. \n\n \n\n\n"
            }]
        },
        "AnalyseenAdviesInfrastructuur": {
            "hand-ins": [{
                "text": "\nDeX\nArchitecture \nAnalysis and Advice\n\nDelta - Max van Hattum\n06.04.2021\n\n\n\nVersion History\n\nDocument Dependency\n\n\n\n\nTable of Content\n\n\n\n1. Introduction\nDeX is a platform that wants to enable employees and students of learning institutions to share their products, project ideas or even research papers and thesis\u2019s. \nCurrently there are several groups working on realizing and adding features to this platform. There are two project leaders guiding the project, Niray Mak and Ruben Fricke, they are responsible for maintaining contact with stakeholders, management, and deliveries. They also lead the software and UX teams within Delta. Moreover, an associate degree team and a cybersecurity team work on DeX. \nAt the time of writing this document, DeX is more than a year old. At the beginning multiple decisions have been made about the infrastructure and software structures/patterns. Now DeX has grown immensely and new features have been created, moreover stakeholders and criteria may have changed. \nTherefor an analysis of these structures and patterns is necessary to identify if these are still the best options for DeX. Based on this analysis advice should be given about steps to undertake that could improve DeX based on defined criteria.\n\n\n\n2. Research questions\nTo give guidance to this research, a main research question will be formulated. This document should ultimately give a concise and accurate answer to this question. However, since there are multiple facets that need to be looked at, sub-questions will also be defined. These must first be answered before an accurate answer to the main question can be given.\n2.1 Main research question\nThe main goal of this research will be to give advice on where the DeX project can improve its infrastructure and software, focusing on the architectonical aspects. This can be concisely formulated in the following way:\nWhat are steps that can be taken to improve the infrastructure and software architecture?\n2.2 Sub questions\nThere are multiple facets that need to be identified and researched before an answer to the main question can be given. \nFirst and foremost, all the stakeholders involving the infrastructure and software architecture should be identified, defining their wants and needs. Based on these stakeholders\u2019 criteria can be discovered and given a score of priority per stakeholder. This results in the following sub-questions:\nWho are the stakeholders involved with the DeX infrastructure and software architecture?\nWhat are criteria that the previously identified stakeholders find important?\nBased on the discovered criteria an analysis of the current patterns and architecture can be done, where the criteria give weight to scoring the architecture that is currently in place. Before this scoring can be done, a general analysis should  be done, identifying the architecture, and supplying relevant available information about this architecture.\nWhat is the current structure and what are the dis- and advantages of this structure?\nHow does the current structure score, based on the criteria defined by the stakeholders?\n\n\nNext up research must be done on defining alternatives, describing what these alternatives are and how they could fit in the current context. They should also be scored with the same criteria as the current structure. \nWhat are alternative patterns or structures and how would these fit in the DeX context?\nHow do the previously identified structures score, based on the criteria defined by the stakeholders?\nLastly the highest scoring option should be explored more in-depth defining what must and will happen within DeX if the choice to switch to this option would be made. Not only the steps to achieve the switch, but also the impacts, positive and negative should be defined.\nWhat needs to be done to make a switch to the best scoring structure?\nWhat impact on DeX would this switch to the new structure have?\nThen, ultimately the main research question can be answered giving advice on what steps should be taken for improving the infrastructure and software architecture.\n\n3. Research Methods\nMultiple research methods are going to have to be used to get a complete and correct research. Therefor a breakdown of research methods necessary for answering the sub questions will be listed below, organized per sub question.\n3.1 Research methods sub questions one and two\nFor identifying the stakeholders and discovering what their priorities and criteria for the infrastructure and software architecture are, a mix of qualitative and quantitative data should be collected. Identifying all the stakeholders will result in primary qualitative data collected using interviews.\nWhen dealing with a larger volume of the same type of stakeholder, such as the software engineers, a questionnaire will be used to decide the common important criteria quickly and accurately. This will then result in primary quantitative data.\n\n\n3.2 Research methods sub questions three and four\nFor the sub question three a qualitative approach should be taken, since the primary objective of this question is to identify the current architecture and what common dis- and advantages are. It is not yet relevant to approach these dis- and advantages in a numerical way since it will not yet be a comparison of data.\nThere will however be a need of primary and secondary information. The primary facet would be defining the current structure in a descriptive way. The secondary facet will be the definition of both the dis- and advantages, a literature study should be conducted to accurately define these.\nThen, after the current architecture with its dis- and advantages has been identified, a more quantitative approach should be taken to score the architecture based on the earlier defined criteria. \n\n3.3 Research methods sub questions five and six\nYet again a qualitative approach will be taken for sub question five, identifying alternative structures and patterns, however now this will be secondary data obtained by doing literature research. \nNow for the sixth sub question quantitative data needs to be used, using the primary data collected while answering the first two sub questions. This must then be used for scoring the alternative patterns and architectures.\n\n3.4 Research methods sub questions seven and eight\nFor the last two questions experiments must be conducted, creating prototypes of the new pattern or architectures, and measuring what the effects are. The data resulting from these experiments could be both quantitative and qualitative.  It is possible to quantitatively measure performance by tracking for example CPU usage and response times. However, aspects such as ease of use and understandability is data that will be presented in a more descriptive, qualitative way. \nThese results need to be compared to the current system in place. This comparison will make it possible to identify what steps need to be taken to transform the current situation to the new preferred situation.\nConclusion\n\nDiscussion\n\nGlossary\n\n\n"
            }]
        },
        "C3ModelRESTAPI": {
            "hand-ins": [{
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPSV Swimming App C3 Model \nFontys Open Learning Semester 3 \n\nMax van Hattum\n\n\n\n \n2 \n\nInhoud \nIntroduction ............................................................................................................................................. 3 \n\nContext Diagram ...................................................................................................................................... 3 \n\nCurrent situation ................................................................................................................................. 3 \n\nPreferred ultimate situation ................................................................................................................ 4 \n\nContainer diagram ................................................................................................................................... 5 \n\nComponent Diagram ............................................................................................................................... 6 \n\nCode diagram .......................................................................................................................................... 8 \n\n \n\n  \n\n\n\n \n3 \n\nIntroduction \nFor Open Learning we get the opportunity to work for an external stakeholder, giving context to our \n\nlearning process. I\u2019ve chosen to work on an application for PSV Swimming. The main features this app \n\nneeds to offer are enabling swimmers to manage their event registrations, coaches to review this \n\nregistrations and administrators to see an overview of these registrations so they can process them. \n\nPart of this application is a REST API, which is responsible for handling and processing data. To \n\nprovide more insight in this software system, a C3 model is chosen to document the architecture. \n\n  \n\nContext Diagram \n\nCurrent situation \nThe existing application has three actors, with limited interaction. It also utilizes the SendGrid service \n\nfor sending emails.  \n\n \n\n \n\n  \n\n\n\n \n4 \n\nPreferred ultimate situation \nThe preferred ultimate situation would be that the administrator will only have to upload meets to \n\nthe PSV Swimming application, and that the application then also handles the synchronization of this \n\ndata with the European Swimming Federation. Moreover, they should be able to make notifications. \n\nThe coach should also be able to create and manage trainings, plus only being able to approve meet \n\nregistration of their own athletes. \n\nAthletes should now also be able to see notifications and manages registrations to trainings. \n\n \n\n \n\n  \n\n\n\n \n5 \n\nContainer diagram \nThe software system consists of an PWA made with Vue.js, a Web Server serving the PWA, a Rest API \n\nwhich handles data requests, and a MySQL database for persisting data. Moreover, SendGrid is uses \n\nto send emails. \n\n \n\n\n\n \n6 \n\nComponent Diagram \nThis diagram contains all endpoints of the API showing the data flow and the components used. \n\nIt is quite an extensive application, therefor the diagram is extensive as well. Services are utilized by \n\nendpoints; these services contain the business logic of the application. Repositories are used for \n\npersisting data to the database.  \n\n(See next page for the diagram, with landscape orientation to allow the diagram to utilize as much \n\nscreen space as is available, zoom in where necessary.)\n\n\n\n \n7 \n\n\n\n \n8 \n\nCode diagram \nIn my opinion a code diagram does not provide extra value or understanding into the project. If \n\nnecessary, this can be (and has been) generated by the IDE. However, this diagram is enormous and \n\nhard to comprehend. Consequently, it does not fit in this document. \n\n\n"
            }]
        },
        "CICD": {
            "hand-ins": [{
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReport CI/CD Strijp-T Rest APi \nMax van Hattum \u2013 27-11-2020 \n\nIntroduction \nFor Strijp-T we are making an application that is going to make use of a REST API that is going to \n\nmanage the data and business logic.  \n\nTo set up this project correctly and ensuring a good workflow with CI/CD (continuous integration and \n\ncontinuous delivery), I\u2019ve set up a azure devops account and project. There were also other options \n\navailable like FHICT\u2019s own gitlab, or GitHub\u2019s actions. However since we get credit from school to use \n\nAzure, and it seemed really interesting to me soI\u2019ve decided to go with them. \n\nMy goals were to learn how to automate testing on commits or merges, automate the building, \n\nautomate the publishing and protect branches with enforced pull requests.  \n\n \n\nConfiguring pipeline with a yaml file \nAzure-pipeline.yml \n\n \n\nThis file is responsible for setting up the pipeline. First I defined the triggers, these are the branches \n\non which the pipeline should be run when changes are detected. Now it will trigger on master, dev \n\n\n\nand every branch starting with feature/. This could be improved on by making a separate pipeline for \n\nthe feature branches, which only does testing, since now it also builds. \n\nNext up is the pool, this is where the commands will be run on. I\u2019ve chosen for Microsoft hosted \n\nagent, not a private one. Since we have no specific or complex requirements this will be sufficient.  \n\nThen we define the steps that are to be taken for the pipeline to complete. First we use the \n\nDotNetCore Command Line Interface to run the tests available. We define the paths to the test \n\nprojects and define the build configuration. \n\nThen we use Docker to build an image from the defined Dockerfile and push these to a container \n\nregistry for artifacts I\u2019ve setup, these are then ready for deployment.  \n\n \n\nResult coverage \nAfter the pipeline has completed a report is ready about the tests. \n\n \n\n \n\n \n\n\n\nPublished to registry container after pipeline \nAs you can see below after the pipeline triggers it stores the images with version control in a artifact registry.  \n\n \n\n\n\n \n\n \n\n \n\n \n\n \n\n  \n\n\n\nBranch policies  \n\n \n\nTo ensure code quality in release, I\u2019ve set up branch policies to enforce code reviews so that releases will only go through after a review and a pipeline that \n\nneed to have the tests to be run successful.  \n\n \n\n\n"
            }]
        },
        "CompetenceDocument": {
            "hand-ins": [{
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompetence document\n\n\n \n\n  \n\nCOMPETENCE DOCUMENT \nDit document gaat inzicht geven in hoe mijn competenties gegroeid zijn in het \n\nkomende semester. \n\nHattum,Max M.B. van \n3928780 \n\n31-08-2020 \n\nOpen Learning semester 3 \n\n\\*Docent naam TBD*\\ \n\nVersie 6 \n\nAbstract \nIn dit document beschrijf ik mijn startpunt en beschrijf en lever ik bewijs van mijn groei in \n\ncompetenties. Daarnaast zijn de projecten waar ik aan gewerkt heb hier in terug te vinden, \nbenoem ik wat ik hier voor heb gedaan en reflecteer ik hier op.  \n\n\n\n  \n\nInhoud \nPersona .................................................................................................................................................... 1 \n\nContext .................................................................................................................................................... 2 \n\nChallenge description Open Learning ..................................................................................................... 2 \n\nFull stack development PSV Swimming (project 1) ......................................................................... 2 \n\nChallenge descriptions Delta ................................................................................................................... 2 \n\nDeX Platform (Project 2) ...................................................................................................................... 2 \n\nStrijp-T (Project 3) ............................................................................................................................... 2 \n\nMagicLeap (Project 4) .......................................................................................................................... 2 \n\nCompetence profile ................................................................................................................................. 3 \n\nHuidig niveau ....................................................................................................................................... 3 \n\nGeambieerd niveau ............................................................................................................................. 3 \n\nKPI-table with proof ................................................................................................................................ 5 \n\nOpen Learning ..................................................................................................................................... 5 \n\nDelta .................................................................................................................................................... 5 \n\nRetrospective Delta ................................................................................................................................. 6 \n\nSprint retrospectives Open Learning ....................................................................................................... 8 \n\nSprint 1 ................................................................................................................................................ 8 \n\nSprint 2 ................................................................................................................................................ 8 \n\nStarr reflectie 1 ................................................................................................................................ 8 \n\nStarr reflectie 2 ................................................................................................................................ 9 \n\nStarr reflectie 3 .............................................................................................................................. 10 \n\nConclusie ....................................................................................................................................... 10 \n\nSprint 3 .............................................................................................................................................. 11 \n\nSprint 4 .............................................................................................................................................. 11 \n\nSprint 5 .............................................................................................................................................. 11 \n\nEvaluation and Reflection ...................................................................................................................... 12 \n\nFeedpulse .............................................................................................................................................. 12 \n\n \n\n \n\n  \n\n\n\n  \n\nVersiebeheer \n\nVersienummer Datum Auteur Veranderingen \n\n1 31-08-2020 Max van Hattum Voorblad, layout, eerste vier \nonderwerpen ingevuld. \n\n2 1-09-2020 Max van Hattum Pagina nummers toegevoegd (wel zo \nhandig met een inhoudsopgave) \n\n3 2-09-2020 Max van Hattum Extra challenge toegevoegd die \ninteressant lijkt. \n\n4 30-09-2020 Max van Hattum Persona uitgebreid met voorgaande \nrichting en huidige constructie. \nGekozen challenge uitgelicht. \nCompetence profile huidig niveau \ntoegelicht en beschreven hoe ik denk \nhet geambieerde niveau te bereiken. \n \n\n5 2-10-2020 Max van Hattum Retrospective toegevoegd \n\n6 4-11-2020 Max van Hattum Competence document layout beter \nopgesplitst in OL en Delta, bewijs \nDelta en OL uitgebreid, specifieke \nDelta retrospective toegevoegd en \nsprint 2 retrospectives toegevoegd met \nstarr reflecties. \n\n \n\n  \n\n\n\n  \n\n \n1 \n\nPersona \n \n\nIk ben Max van Hattum, vier\u00ebntwintig jaar oud en studerende aan \n\nhet Fontys ICT, geboren en getogen in Tilburg. Hiervoor heb ik al \n\ndrie jaar Biomedische Technologie gestudeerd, door ziekte moest \n\nik helaas uitvallen en heb ik besloten om ICT op HBO niveau te \n\ngaan doen. Hier heb ik gekozen in semester 2 voor de richting \n\nsoftware, en mijn propedeuse met een outstanding behaald. \n\nICT was altijd al mijn hobby, op mijn twaalfde heb ik een boek \n\nover PHP 5 voor mijn verjaardag gevraagd, dit zegt vast al genoeg \n\nover mijn interesse in software development. \n\nVerder hou ik van lezen, fitness, zwemmen en zo af en toe \n\ngamen. Graag onderneem ik ook dingen met vrienden, een \n\nterrasje in de zon of iets anders is altijd leuk. Daarnaast kijk ook \n\ngraag naar sport, denk hierbij aan voetbal, hockey, basketbal, \n\nFormule 1 of League of Legends. \n\nVoor semester drie combineer ik het Delta excellentie \n\nprogramma en open learning. Martijn heeft aangegeven dat ik \n\nzelf mijn tijd kan en moet verdelen. Ik heb er voor gekozen om \n\ntwee dagen aan het open learning, PSV Swimming app, te \n\nbesteden en mijn individuele project alleen te doen wanneer ik tijd over heb. \n\n \n\n  \n\n\n\n  \n\n \n2 \n\nContext \nMijn interesse ligt op het moment bij het ontwikkelen van software applicaties. Op het moment heb \n\nik al wat ervaring in het ontwikkelen van webapplicaties en ik wil dit graag doorzetten naar mobile \n\napplicaties. Daarnaast vind ik een stukje user interaction ook interessant, dus hier wil ik ook wat van \n\nmeepakken. \n\n \n\nChallenge description Open Learning \nFull stack development PSV Swimming (project 1) \nEr is een huidige app waarmee zwemmers zich kunnen aanmelden voor verschillende evenementen. \n\nDeze is echter verouderd en moet daarom verbeterd worden. Er moet aan een back-end gewerkt \n\nworden, en aan een front-end in de vorm van een mobiele applicatie. Er is een huidige userbase \n\nwaarmee getest kan worden. \n\nHet is uiteindelijk dit project geworden, dit was niet mijn eerste keuze maar hier ben ik in ingedeeld. \n\nAl met al ben ik wel tevreden hier mee, het is een uitdagende challenge. Hier in kan ik veel van de \n\nsoftware doelstellingen aantonen. Daarnaast is het systeem wat nu gebruikt wordt interessant en \n\ningewikkeld, dus is het leuk puzzelen om een goede plan van aanpak te construeren waarmee dit \n\ngeleidelijk vervangen kan worden door een effici\u00ebnter en gebruiksvriendelijker systeem. \n\nNatuurlijk is het ook leuk om voor een \u2018grote\u2019 naam zoals PSV een opdracht te doen en is het een \n\nnieuwe situatie voor mij om een project dat al half staat te continueren en dus gebruik te moeten \n\nmaken van vooraf onbekende programmeer talen, tools en frameworks. \n\n \n\nChallenge descriptions Delta \n\nDeX Platform (Project 2) \nHet DeX Platform is een webapplicatie die een centrale plaats moet worden voor het weergeven van \n\nprojecten. Het doel hiervan is om het vinden van projecten makkelijker te maken, waardoor \n\nsamenwerken makkelijker wordt.  \n\nEr is hiervoor een front-end, back-end en full-stack team, ik ben toegewezen aan het back-end team. \n\n \n\nStrijp-T (Project 3) \nStrijp-T als organisatie heeft bepaalde kernwaarden en wilt deze graag naar voren laten komen op \n\nhet terrein. De opdracht is erg breed, er is ons gevraagd om een concept uit te werken dat dit op een \n\nof andere manier voor elkaar krijgt.  \n\n \n\nMagicLeap (Project 4) \nDe MagicLeap is een AR-bril, het doel van dit project is exploratie. Wij mogen zelf met een idee \n\nkomen zolang het maar nieuw en uitdagend is. \n\n \n\n\n\n  \n\n \n3 \n\nCompetence profile \n\nHuidig niveau \nIk heb het eerste jaar gekozen voor de software , hiervoor moesten wij de onderstaande criteria \n\naantonen.  \n\nEr waren twee dagen beschikbaar voor een proftaak die voor een bedrijf gedaan moest worden \n\nwaarin de professionele vaardigheden aangetoond moesten worden en drie dagen voor een \n\nindividuele challenge waarin de software specifieke doelstellingen bewezen moesten worden. \n\nBeide waren mijn coaches erg enthousiast over, zowel mijn software kennis en professionele \n\nvaardigheden zaten zeer boven het verwachte niveau. waardoor ik dus ook de hoogste score behaald \n\nheb. De stakeholder voor de proftaak was zelfs zo enthousiast dat zij mij samen met twee anderen \n\nuit het groepje een contract wilden aanbieden om het project intern voor te zetten.  \n\nHiernaast heb ik ook al eerder drie jaar biomedische technologie gestudeerd aan de TU/e. Hier kwam \n\nook enkele overlap met ICT voor. Hier heb ik ook al tijd gehad om mijn professionele vaardigheden te \n\noefen, al hoewel hier minder de focus op lag dan nu. \n\n \n\nCurrent profile  \n\n Managing Analysing Advising \nDesignin\n\ng \nRealising  \n\nCommuni-\n\ncation \nLearning \n\nJudge- \n\nment \n\nUser interaction       x x x \n\nBusiness \n\nprocesses \n\n      \n\nSoftware x x x x x  \n\nHardware       \n\nInfrastructure       \n\n \n\nGeambieerd niveau \nIk wil sowieso mijn software kennis naar het volgende niveau tillen, hiernaast wil ik ook voor user \n\ninteraction kennis aantonen. Verder wil ik zoveel mogelijk van de professionele vaardigheden verder \n\nontwikkelen. \n\nDoordat ik meer dan een project heb door Delta heb ik veel ruimte om diverse doelstellingen aan te \n\ntonen, er was mij ook verteld dat ik werk gedaan binnen Delta kon gebruiken als bewijs voor KPI\u2019s. Ik \n\nheb op het moment twee projecten lopen waarin we van de grond af aan moeten beginnen en ik ook \n\nsamenwerk met van origine media studenten. Ik doe hier op het moment mee met de concepting, \n\nonderzoek en adviesgeving. Ik verwacht dus dit te kunnen gebruiken als bewijs voor UX. \n\nVerder lopen er ook twee projecten, Dex en PSV Swimming, waarin ik veel van de software \n\nvaardigheden kan aantonen. Beide zijn projecten die al lopen en al een basis, of meer, hebben staan. \n\nDit is een mooie gelegenheid om mijn software vaardigheden te toetsen binnen al een gedeeltelijk of \n\ngeheel bestaand systeem.  \n\n\n\n  \n\n \n4 \n\nBij Dex wordt gewerkt met een CI/CD, issues en pull requests met code reviews. Daarnaast wordt \n\nhierbinnen gebruikt gemaakt van een ORM, integratie tests doormiddel van postman en unit tests \n\ndie gebruik maken van een mocking framework. Na elke sprint wordt de master branch bijgewerkt, \n\nwaarna automatisch het project wordt gebuild, getest en gedeployed, gebruikmakend van docker. \n\nHiernaast heb ik hier ook de mogelijkheid in om nieuwe componenten te onderzoeken, designen en \n\nte implementeren, met in ooghoudend het bestaande systeem en de ambitie om over te stappen van \n\neen monolitische architectuur naar microservices.  \n\nBinnen deze verscheidene projecten, met contact met meerdere stakeholders verwacht ik dat ik veel \n\nof zelfs alle professionele vaardigheden naar het volgende niveau kan brengen.  \n\n \n\nIntended development v1 (After every adjustment to your competence profile (you may change this over time \n\nbecause it\u2019s dynamic) you\u2019ll add the new version below the other ones. In this way we can see the development \n\nof your profile.) \n\n Managing Analysing Advising \nDesignin\n\ng \n\nRealisin\n\ng \n \n\nCommuni-\n\ncation \nLearning \n\nJudge- \n\nment \n\nUser interaction x x x x x  x x x \n\nBusiness \n\nprocesses \n\n      \n\nSoftware x x x x x  \n\nHardware       \n\nInfrastructure       \n\n \n\nFinal development \n\n Managing Analysing Advising \nDesignin\n\ng \n\nRealisin\n\ng \n \n\nCommuni-\n\ncation \nLearning \n\nJudge- \n\nment \n\nUser interaction          \n\nBusiness \n\nprocesses \n\n      \n\nSoftware       \n\nHardware       \n\nInfrastructure       \n\n \n\nLevel  1 Level  2 Level 3 \n\n \n\n\n\n  \n\n \n5 \n\nKPI-table with proof \n\nOpen Learning \nBij Open Learning werk ik aan het project PSV Zwemmen App. Ik verwacht dat ik hier binnen bewijs \n\nkan aanleveren dat verscheidene software KPI\u2019s aantoont.  \n\nIk heb veel geleerd over Java, Spring Boot, Autowired, JPA en Hibernate. Deze waren nodig om te \n\nsnappen hoe de demo werkte, hoe ik deze kon verbeteren en uiteindelijk kon omtoveren in een \n\nfunctionele app. Dit is uiteindelijk toch niet de plan van aanpak geworden, waarna ik een nieuw \n\nproject heb gestart. \n\nBinnen dit nieuwe Spring Boot project heb ik geleerd hoe hier authenticatie en autorisatie te \n\nconfigureren doormiddel van JWT, hierbij kwam ook endpoint configuratie aanbod. Dit ging een stuk \n\nanders dan hoe ik het gewend ben in ASP.NET Core, het is ingewikkelder maar staat wel veel meer \n\naangepaste configuratie toe. \n\n \n\nDelta \nBij Delta werk ik aan drie projecten, waarvan twee het meest relevant zijn voor het aantonen van \n\nKPI\u2019s. Binnen het DeX project verwacht ik veel software KPI\u2019s aan te kunnen tonen. Voor het project \n\nStrijp-T denk ik dat ik UX KPI\u2019s kan halen en nog ondersteuning voor software KPI\u2019s.  \n\nBinnen DeX heb ik al werk gedaan wat gebruikt kan worden voor software KPI\u2019s, namelijk het \n\nreviewen van code, het opstellen van een advies document of een notificatie systeem en het \n\nopstellen van de C4 modellen voor ditzelfde notificatie systeem. \n\nBij Strijp-T hebben wij concepten uitgedacht en deze gepresenteerd aan de stakeholder, hier is een \n\nkeuze in gemaakt. Nu kan ik hier beginnen met de opzet van een PWA. \n\n \n\nKPI Proof Rating \n\nPL-2.1, PL-2.2, PL-2.3, TI-2.1, TI-2.2, TI-\n\n2.3, TI-2.4, TI-2.5. \n\nProject 1 \n\nSprint opleveringen aan docenten en \n\nstakeholder. \n\nhttps://fhict.instructure.com/courses/10657/a\n\nssignments/164899?module_item_id=586631 \n\n \n\nAnalysis-S2.1, Analysis-S2.2, FOO-2.1, \n\nFOO-2.2, FOO-2.5 \n\n. \n\nProject 1 \n\nEen onderzoek naar hoe een bestaande demo \n\nvan een vorige project groep omgezet kan \n\nworden naar een functionele app. \n\nhttps://fhict.instructure.com/courses/10657/a\n\nssignments/169490?module_item_id=596662 \n\n \n\nhttps://fhict.instructure.com/courses/10657/assignments/164899?module_item_id=586631\nhttps://fhict.instructure.com/courses/10657/assignments/164899?module_item_id=586631\nhttps://fhict.instructure.com/courses/10657/assignments/169490?module_item_id=596662\nhttps://fhict.instructure.com/courses/10657/assignments/169490?module_item_id=596662\n\n\n  \n\n \n6 \n\nManage&Control-S2.1, \n\nManage&Control-S2.2 \n\nProject 3 \n\nVoor een goede workflow heb ik een CI/CD \n\nopgezet met automatisch testen en deployen.  \n\nhttps://fhict.instructure.com/courses/10657/a\n\nssignments/168667?module_item_id=592308 \n\n \n\nAdvise-2.1, Advise-2.2, Advise-2.3, IPS-\n\n2.1, IPS-2.2, IPS-2.3, IPS-2.4 \n\nProject 2 \n\nVoor het DeX Platform moest een notificatie \n\nsysteem opgezet worden, hiervoor heb ik een \n\nadvies document gemaakt. \n\nhttps://fhict.instructure.com/courses/10657/a\n\nssignments/167651?module_item_id=588845 \n\n \n\nDesign-S2.1, Design-S2.2, Design-S2.3 Project 2 \n\nVoor het DeX Platform moest een notificatie \n\nsysteem opgezet worden, hiervoor heb ik een \n\ndesign document gemaakt die gebruikt maakt \n\nvan het C4 model. \n\nhttps://fhict.instructure.com/courses/10657/a\n\nssignments/167655?module_item_id=588858 \n\n \n\n \n\n \n\n \n\nRetrospective Delta \nBij Delta ligt de verantwoordelijkheid erg bij jezelf, wat ik wel fijn vind. Als ik terugkijk op de eerste \n\nhelft van dit semester vind ik dat de manier van werken goed bij mij past. Ik heb veel initiatief \n\ngetoond, waardoor ik lekker bezig kon blijven. Veel projecten hadden wel een lastige opstart, dit had \n\nook wel erg te maken met de huidige Corona situatie. Het duurde wel even voordat ik echt lekker \n\nbezig was met de Delta projecten. Bij DeX kon ik het snelste aan de slag, hierna Strijp-T en daarna \n\nMagic Leap.  \n\nBij DeX staat al een heel project en een gestructureerde manier van werken, na een eerste sprint \n\nhiermee kennis te maken, verliep het soepel. Ik kan binnen dit project veel leren van de manier van \n\nwerken, en heb hier al veel van opgestoken. Doordat er wel met een extensief review systeem \n\ngewerkt wordt, kan het soms wel even duren voor je weer verder kan met je taak. Dit was echter niet \n\nerg omdat ik toch andere projecten had om ook aan te werken. \n\nBinnen DeX kan ik echt mij ei kwijt qua software engineering, ik heb het gevoel dat ik kennis breng, \n\nmaar dat er ook zeker veel op te steken valt voor mij. Het samenwerken met de andere studenten \n\ngaat ook lekker, ik werk vooral in paren wat zorgt voor beter kwaliteit werk. \n\nStrijp-T was leuk samenwerken, plus dat ik hier wat anders tegenkom dan ik normaal gewend ben \n\nvanuit software. Mijn groepsgenoten komen allemaal vanuit de Media kant, en we moesten hier ook \n\nhttps://fhict.instructure.com/courses/10657/assignments/168667?module_item_id=592308\nhttps://fhict.instructure.com/courses/10657/assignments/168667?module_item_id=592308\nhttps://fhict.instructure.com/courses/10657/assignments/167651?module_item_id=588845\nhttps://fhict.instructure.com/courses/10657/assignments/167651?module_item_id=588845\nhttps://fhict.instructure.com/courses/10657/assignments/167655?module_item_id=588858\nhttps://fhict.instructure.com/courses/10657/assignments/167655?module_item_id=588858\n\n\n  \n\n \n7 \n\nbeginnen met het verzinnen en uitwerken van concepten. Ik vind dit uitdagend en interessant om te \n\ndoen, ook verfrissend omdat het weer eens wat anders is dan het standaard software systemen \n\ndesignen. \n\nMagic Leap ben ik tot nu toe het minst tevreden mee, ik behandel dit op het moment ook als meer \n\neen hobby project. We hebben hier tot nu toe veel brainstorming gedaan. Het is verder vind ik lastig \n\nom voor de Magic Leap concreet software te maken, we zijn al wel zo ver dat we een app kunnen \n\ndeployen en hier eigen geanimeerde objecten in kunnen zetten. De documentatie is echter erg slecht \n\nnaar mijn mening en dit maakt het frustrerend om mee te werken. \n\nAl met al heb ik ook de projecten samen goed gemanaged om een persoonlijk niveau. Ik moet erg \n\nletten op mijn energie niveau en wat mijn grenzen zijn. Dit is mij goed gelukt en ik heb het idee dat ik \n\nveel heb kunnen bijdragen, terwijl ik tegelijkertijd goed heb kunnen letten op mijn mentale \n\ngezondheid en energie niveau.  \n\nDe organisatie binnen Delta is wat mijn idee net iets t\u00e9 losjes, van mij mag er meer structuur in \n\nzitten. Maar ik heb dit goed los kunnen laten en mijn eigen structuur op kunnen zetten. \n\n \n\n  \n\n\n\n  \n\n \n8 \n\nSprint retrospectives Open Learning \nAfter every sprint demo, you will have a retrospective. You will reflect on your process, work method \n\nand the communication within your group. Include a summary of each retrospective in this \n\ndocument. \n\nSprint 1 \nVoor mij persoonlijk ging de eerste sprint vlot. Ik heb meteen in het begin veel initiatief genomen en \n\ncontact gezocht met stakeholders en projectleden. Helaas waren enkele projectleden minder \n\ngemotiveerd en deden niet mee zoals ik verwacht van medestudenten. De coaches hebben hier een \n\ngoede oplossing voor gevonden door het groepje in twee\u00ebn te splitsen. \n\nVerder is de samenwerking tussen mij en Stephanie wel goed gegaan, we hebben een klik en weten \n\nvan elkaar wat we binnen dit project willen bereiken. Van mij mag de communicatie over de taken en \n\nhuidige werkzaamheden wel beter, hier hebben we het over gehad en we gaan dit meteen de eerste \n\ndag van de nieuwe sprint oppakken. \n\nDe externe stakeholder was nog niet bereikbaar wat ik wel jammer vond, dit nam toch een stukje \n\nmotivatie bij mij weg. Ik had een beetje het idee dat ik niet wist wat ik daadwerkelijk kon doen, \n\nvooral omdat het ook veel inlezen was over wat de vorige groep had staan. \n\nMaar Wilrik heeft het goed opgepakt als acting stakeholder en hierdoor werd ik weer enthousiaster. \n\nMaar voor mij is het dus wel een belangrijk punt om op te letten, dat ik zelfs met tegenslagen \n\nprobeer de motivatie er in te houden.  \n\nVoor de rest wil ik toekomstige retrospectives op een meer methodische manier gaan benaderen en \n\nwil ik de Starr-reflectie methode gaan toepassen. \n\nAl met al ben ik tevreden met hoe het proces is verlopen, maar ik verwacht wel dat ik in de volgende \n\nsprint meer concreets kan opleveren. \n\n \n\nSprint 2 \n\nStarr reflectie 1 \nSituatie \n\nDit speelde zich af binnen een teams meet, met Stephanie, Cees, Martin en ik. Het was ons eerste \n\ngesprek met de externe stakeholder. \n\nTaak \n\nMijn taak was om samen met Stephanie kennis te maken met de stakeholder, te laten zien wat wij \n\ntot nu gedaan hebben en vragen stellen zodat wij erachter konden komen waar de prioriteiten van \n\nde stakeholder exact lagen. \n\nActie \n\nIk sprong te snel in het gedeelte over wat we al gedaan hadden en verwaarloosde het kennismaking \n\ngedeelte. Cees wees mij terplekke terecht er op dat het misschien een goed idee was om wat \n\nuitgebreidere kennis te maken alvorens we naar de zakelijke inhoud gingen. Ik bood mijn excuses aan \n\nen gaf toen een wat uitgebreidere uitleg van wie ik ben en wat ik wil leren. Ook Martin gaf wat meer \n\ninzicht in wie hij is en waarom hij naar Fontys is gekomen met deze challenge. \n\n\n\n  \n\n \n9 \n\nResultaat \n\nWe hadden een korte onderbreking in het gesprek omdat we dus door mij moesten schakelen van \n\nhet ene naar het andere onderwerp, echter ging dit snel en soepel. Uiteindelijk hadden we een goed \n\nen fijn gesprek. Waarin we elkaar en de opdracht beter leerde kennen. \n\nReflectie \n\nIk vond het slecht van mijzelf dat ik veel te snel doorging naar het zakelijke gedeelte en het \n\n\u2018menselijke\u2019 gedeelte verwaarloosde. Ik schaamde mij ook wel een beetje toen ik hier (terecht) op \n\ngewezen werd. Verder ben ik wel tevreden met hoe ik het heb aangepakt, ik heb het goed opgepakt \n\nen het resultaat was erna een fijn gesprek. De essentie is wel dat ik beter moet letten op wat mijn \n\ndoelgroep is en goed moet opletten dat ik niet door mijn enthousiasme voor het een, andere \n\nbelangrijke zaken verwaarloos. Een volgende keer ga ik beter van te voren voor mijzelf doornemen \n\nwat nou de bedoeling van de situatie is, wie de doelgroep is en hoe ik het het beste kan aanpakken. \n\n \n\nStarr reflectie 2 \nSituatie \n\nIn de week na de herfstvakantie moesten wij opleveren aan de coaches en de stakeholder. De week \n\nvoor de herfstvakantie was Stephanie al niet aanwezig door ziekte, de maandag na de herfstvakantie \n\nliet ze weten dat ze nog steeds (erg) ziek was. Zij was verantwoordelijk voor de oplevering deze keer.   \n\nTaak \n\nMijn originele taak was om materiaal aan te leveren voor de oplevering, echter nu Stephanie ziek \n\nwas moest er gecommuniceerd worden met de coaches en stakeholder voor een oplossing. \n\nAangezien ik het enige verdere lid ben in de groep lag deze (onverwachte) verantwoordelijkheid dus \n\nbij mij. Ik moest dus een oplossing vinden. \n\nActie \n\nIk kon het snelste contact hebben met Britt, na de Open Learning general stand-up. Toen heb ik \n\ndoorgegeven wat de situatie was en om input gevraagd. \n\nResultaat \n\nUiteindelijk was de conclusie dat het dan maar meer een update ging worden en dat zulke \n\ntegenslagen nou eenmaal bij de realiteit horen. \n\nReflectie \n\nIk vind dat ik het goed heb gedaan, ik heb echt zo snel mogelijk er werk van gemaakt. Hier ben ik wel \n\ntrots op omdat, ook al was het gewoon een tegenslag, ik mij dit niet liet weerhouden van juist \n\nhandelen. Dit zorgde er voor dat sneller bij mij de onrust weggepakt werd omdat ik meteen een \n\ngeruststellend antwoord kreeg van de coach. Ik ben dus tevreden met hoe het gelopen was. \n\nIn essentie was dit dus gewoon de juiste aanpak en moet ik het direct aanpakken van problemen \n\ndoorzetten in mijn toekomstige werkwijze.  \n\n \n\n\n\n  \n\n \n10 \n\nStarr reflectie 3 \nSituatie technische tegenslag -> zelf eerst gedaan wat ik kon doen -> niet te lang doorgegaan maar \n\nbesproken met cees -> goede feedback -> andere richting in gegaan -> goed gehandeld omdat elke \n\nverandering weer meer werk met zich meebracht, maar vervelend gevoel want voelt als opnieuw \n\nbeginnen. \n\nSituatie \n\nIk had een face to face gesprek met Cees met als onderwerp dat het te (onnodig) moeilijk was om de \n\nhuidige demo om proberen te zetten naar een functionele app. \n\nTaak \n\nMijn taak was om de huidige demo zo om te bouwen dat de data in een database werd opgeslagen \n\ndoormiddel van een ORM. Voor ik hier aan begon verwachte ik dat dit goed te doen zou moeten zijn, \n\nook al had ik nog geen eerdere ervaring met Spring Boot, het leek heel erg op ASP.NET waar ik wel \n\nveel ervaring in heb. Van mijzelf verwachte ik dus ook dat ik dit met minimale moeite kon \n\nbewerkstelligen. \n\nActie \n\nHet bleek uiteindelijk veel lastiger dan ik van te voren dacht. Door verscheidene tegenslagen deed \n\nelke stap die ik zette weer extra werk opleveren. Mijn conclusie was hierdoor dat het makkelijker zou \n\nzijn om het Spring Boot project opnieuw te configureren en vanaf begin af aan te beginnen, natuurlijk \n\nwel met de huidige demo als leidende draad. Voor ik dit wilde doen vond ik dit wel een beslissing die \n\nbesproken moest worden met Cees. Vandaar dat ik dit onderwerp met hem aansneed en hem mijn \n\nproblemen uitlegde met de voor- en nadelen van het opnieuw opzetten van een project. \n\nResultaat \n\nHet resultaat was dat Cees goed met mij meedacht en kritische vragen stelde. Uiteindelijk merkte hij \n\nop dat hij vond dat ik er goed over nagedacht had en hij het dus prima vond als ik een nieuw project \n\nwilde opzetten.  \n\nReflectie \n\nIk vind dat ik het goed aangepakt heb, ik ben niet te lang blijven doorploeteren, maar ik heb ook niet \n\nmeteen opgegeven. Daarnaast heb ik ook wel degelijk geleerd van het werken in de demo en \n\nhierdoor nieuw inzichten gekregen die ik anders niet zou hebben. Ook vind ik dat ik een goede \n\nbeslissing heb genomen in het betrekken van Cees er bij, vooral omdat ik geen mede Software \n\nstudenten heb om het mee te bespreken. Ik heb geleerd om goed op te blijven letten naar de \n\nvooruitgang en niet nodeloos door te werken puur door koppig- of stugheid. Een volgende keer zou \n\nik misschien zelfs al eerder aan de bel moeten trekken, al vond ik dat in deze situatie ik niet zonder \n\nwaarde doorgewerkt heb.  \n\nConclusie \nIn zijn geheel heb ik een voor mij leerzame sprint gehad. Ik heb veel bijgeleerd in Java, Spring Boot en \n\nbijbehorende libraries. Daarnaast heb ik correcte conclusies kunnen trekken over wat een goede plan \n\nvan aanpak is. Ik vind het wel jammer dat door ziekte, technische tegenslag en internet uitval we ging \n\nechte oplevering hebben kunnen doen. Het opleveren is wel altijd iets speciaals aan een sprint waar \n\nik persoonlijk echt naar toe werk, dat deze dan uitvalt vind ik teleurstellend. \n\n\n\n  \n\n \n11 \n\nSprint 3 \nDeze sprint was voor mij echt bikkelen, er was enorm veel werk te doen aan de back-end. Ik heb een \n\ntotaal nieuw project opgezet en de functionaliteiten die in de demo beschikbaar waren werkend, \n\nmet database en oog op veiligheid, gekregen. Hier heb ik ook een research document voor opgezet.  \n\nPersoonlijk had ik er een goed gevoel bij, ik heb hard gewerkt en ook enorm veel voor elkaar \n\ngekregen. De oplevering met Martin ging wat mij betreft goed, maar Martin zelf zag helaas niet veel \n\nvooruitgang. Dit had te maken met wat vorige projectgroepen al gedaan hadden en hij het gevoel \n\nhad dat al dit werk opnieuw gedaan werd. Vanuit zijn oogpunt is dit zeker begrijpelijk, maar dit \n\nneemt vind ik ons gedane werk niet weg. Veel werk dat ik gedaan heb ook aan de back end is ook \n\npraktisch \u2018onzichtbaar\u2019 voor een niet-expert stakeholder. \n\nGelukkig zien onze docenten wel het werk dat wij verzetten en nam Cees het ook voor ons op.  \n\nDe docenten zijn ook gewoon tevreden met het werk dat wij doen, maar hopelijk hebben we in \n\nsprint 4 een meer interactieve demo waar de stakeholder ook  meer aan heeft. \n\nHet samenwerken met Stephanie gaat wat mij betreft ook prima, tot nu toe hebben we veel \n\ngescheiden kunnen werken, maar we houden wel contact. Deze komende sprint moeten wij meer \n\ngaan samenwerken, om de koppeling tussen back-end en front-end te verwezenlijken. Ik verwacht \n\ndat dit gewoon goed gaat verlopen, afgaande op het fijne samenwerken al tot nu toe. \n\n \n\nSprint 4 \nSprint 4 ging persoonlijk goed, ik heb een grote nieuwe feature kunnen implementeren, namelijk het \n\nimporten van Meets vanuit Lenex files. Ook heb ik veel aan de automatische testen gedaan. \n\nHelaas heeft Stephanie het niet gehaald om tot implementatie van de design te komen aan de front-\n\nend kant, dit vind ik wel jammer omdat ik wel een groot systeem heb opgezet voor data verwerking. \n\nMaar dit niet gebruikt kan worden zonder een user interface.  \n\nWe hebben het er wel goed over gehad, ook over hoe we dit aan de stakeholder brengen, dus de \n\ncommunicatie is wel fijn gebleven. \n\nVerder heb ik ook een groot gedeelte van de presentatie gedaan en hier kreeg ik goede feedback op \n\nvan Britt. Het advies was om de volgende keer (als we dan weer gescheiden demo\u2019s back-end front-\n\nend hebben), om dan eerst het front-end te laten zien. Dit maakt het makkelijker en concreter om te \n\nbegrijpen. \n\nAl met al met ik tevreden met wat ik bereikt heb en gerealiseerd heb, ik heb ook een duidelijk idee \n\nvan wat ik in de laatste sprint wil gaan doen. Ik hoop dat we dan tot een echte realisatie komen waar \n\nalles samen komen. \n\n \n\nSprint 5 \n \n\n  \n\n\n\n  \n\n \n12 \n\nEvaluation and Reflection \nAdd an evaluation and a reflection of your whole Open Innovation semester. Your evaluation \n\ndescribes what went good and bad during your process and how you dealt with that. Your reflection \n\ndescribes how you have grown as a person, and what you will take with you in your further \n\nprofessional career. In the reflection you should also shine a light on the following aspects: \n\nResponsible \n\n- Acts consciously with concern for the greater good with contemplation of relevant approaches. \n\n- Makes an assessment of different interests. \n\nInnovativeness \n\n- Focuses on renewal, improvement and making new connections. \n\n- Spots or creates opportunities and seizes them. \n\nResilience  \n\n- Challenges own ideas. \n\n- Perseveres in finding a result or solution. \n\n \n\n \n\nFeedpulse \nAdd a screenshot of your feedpulse overview.  \n\n \n\n \n\n\n"
            }]
        },
        "Concepting": {
            "hand-ins": [null]
        },
        "ConceptsResearchandAdvise": {
            "hand-ins": [{
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantified Student  \nAdvise Regarding Concepts \nRuben Fricke \u2013 Niek van Dam \u2013 Max van Hattum \n\n1-4-2021 \n\n  \n\n\n\nDependency Matrix \nDocument Name \n\nTopic Analysis \n\nReal-Time Sentiment Analysis \n\nResearch Proficiency Analysis \n\n \n\nTable of Contents \nDependency Matrix  ............................................................................................................................ 2 \n\nIntroduction ............................................................................................................................................. 2 \n\nAnalyzing the language proficiency of the students hand-ins. ............................................................... 3 \n\nNudging students based on the sentiment of sentences they write  in FeedPulse. ............................... 3 \n\nConnecting students with each other based on similar assignments they\u2019ve created. .......................... 4 \n\nCreating a word web based on the student's course which shows their areas of expertise. ................. 4 \n\n \n\n \n\nIntroduction \nQuantified Student wants to enable students to get more insight in their learning process. We want \n\nto achieve this by gathering and processing all kinds of data available through the digital learning \n\nenvironment.  This should allow students to understand their learning process and adjust where \n\ndeemed necessary.   \n\nThe four concepts that are up for the discussion are as follows: \n\n- Analyzing the language proficiency of the student\u2019s hand-ins. \n\n- Nudging students based on the sentiment of sentences they write  in Feedpulse. \n\n- Connecting students with each other based on similar assignments they\u2019ve created. \n\n- Creating a word web based on the student\u2019s course which shows their areas of expertise. \n\n \n\n  \n\n\n\nAnalyzing the language proficiency of the student\u2019s hand-ins. \nThe grading of student\u2019s language proficiency can be done with readability indexes, which grade the \n\nentirety of the text based on the occurrence of sentences, syllables, and words. In my research  I \n\nlooked at one NLP solution and 5 widely applied readability indexes: \u2018ARI\u2019, \u2018Gunning Fog Index\u2019 \u2018Flesh-\n\nKincaid Grade\u2019, and \u2018Flesh readability Index\u2019.  \n\nThe NLP solution matches input texts to textbooks of that level and language, and afterwards took \n\nthe level of proficiency needed to read the textbook. Although this is a very innovative solution, the \n\nsetup and implementation are too big for our scope and would take longer than wished for to realize.  \n\nCompared to the NLP solution, the readability indexes are compact and powerful, usually appearing \n\nas one formula which can be calculated based on metadata from the text. This formula outputs the \n\n\u2018proficiency level\u2019 required to read the given text, which therefore also requires a person to write at \n\nthis level. I would still advise against using the readability indexes, as these are language-specific \n\n(mostly English only). If you attempt to use a language for which the readability index is not made, \n\nyou will get warped results which not properly represent the text. To overcome this, we will have to \n\nresearch and develop our own set of tools for the Dutch which require a great level of linguistic \n\nexpertise.   \n\n \n\nNudging students based on the sentiment of sentences they write  in \n\nFeedpulse. \nIt would indeed be possible to achieve this, when using a rule-based approach of analyzing the \n\nsentiment of sentences. This results in speedy and accurate results, allowing it to be used in real-\n\ntime.  \n\nThe sentence score could then be used to nudge the student, for example if they\u2019ve typed multiple \n\nnegative sentences, they could be nudged with a pop-up: \u201cWas your day really that bad?\u201d. This could \n\ngive the student more insight in their state of mind. \n\n \n\n  \n\n\n\nConnecting students with each other based on similar assignments \n\nthey\u2019ve created. \nThe research conducted leans to the conclusion that it would indeed be possible to achieve this feat \n\nwith Topic Modeling. Machine learning could be applied to create a model that assigns documents, in \n\nthis case the assignments, to categories. Then when a student creates a new assignment, the same \n\nmodel can be used to categorize the new one.  \n\nBased on which category the new assignment belongs too, the student can be shown other students, \n\nwith extra data such as the grading/expertise,  that also have assignments in this category. Allowing \n\nthem to get in to contact with each other. \n\nHowever, there are some flaws in the research, mainly that the data used to create the POC are \n\nprofessionally written articles, while the data that is going to be used are written on a lower level by \n\nstudents.  \n\n \n\nCreating a word web based on the student's course which shows their \n\nareas of expertise. \nThe last concept that is wanted is creating a word web for students, so they are giving insight into \n\ntheir own skills. An important aspect for this concept is TF-IDF. TF-IDF is a natural language \n\nprocessing and information retrieval method. With this algorithm, terms will get weighted on their \n\nimportance which gives a more insightful image of the analyzed text.  \n\nFor creating a word web, TF-IDF is recommended. It can create a selection of important terms that \n\ncan be used for filling the word web that represent the student in a meaningful and insightful way. \n\nThe research also conducted that TF-IDF is a perfect candidate for preprocessing that can also be \n\nused in other concepts to improve the outcome. \n\n \n\n\n"
            }, {
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantified Student \u2013 Real-time Sentiment \nAnalyse \nMax van Hattum\u2013 Delta \u2013 18-3-2021 \n\n\n\n \n1 \n\nTable of Contents \nIntroduction ............................................................................................................................................. 2 \n\nResearch question ................................................................................................................................... 2 \n\nResearch methods ................................................................................................................................... 2 \n\nWhat is Sentiment Analysis? ................................................................................................................... 3 \n\nTypes of sentiment analysis ................................................................................................................ 3 \n\nFine-grained sentiment analysis ...................................................................................................... 3 \n\nEmotion detection ........................................................................................................................... 3 \n\nAspect-based sentiment analysis .................................................................................................... 3 \n\nReal-time sentiment analysis. ......................................................................................................... 3 \n\nHow does Sentiment Analysis work? ...................................................................................................... 4 \n\nDrawbacks ........................................................................................................................................... 4 \n\nImplementations ................................................................................................................................. 4 \n\nIs Sentiment Analysis de best solution for the context? ......................................................................... 5 \n\nProof of Concept ................................................................................................................................. 5 \n\nConclusion and Advise ............................................................................................................................. 6 \n\nGlossary ................................................................................................................................................... 6 \n\n \n\n  \n\n\n\n \n2 \n\nIntroduction \nQuantified Student wants to enable students to get more insight in their learning process. We want \n\nto achieve this by gathering and processing all kinds of data available through the digital learning \n\nenvironment. \n\nThis should allow students to understand their learning process and adjust where deemed necessary.  \n\nOne of the concepts is to nudge a student if the text they are typing is predominantly negative or \n\npositive. Mainly while they are utilizing the Feedpulse module, which allows students to reflect on \n\ntheir day. \n\nAn example of such an implementation would be that when the student has typed multiple negative \n\nsentences in a short time, that they will get a pop-up saying: \u201cDid you really have such a bad day?\u201d. \n\nTo achieve this automatic Sentiment Analysis in real-time should be researched. \n\n \n\nResearch question \nThe main question is as follows: \n\nHow can Sentiment Analysis be used to instantly make a student aware of the \n\ntone of their text? \n\nTo be able to answer this question accurately, the question needs to be divided in several sub-\n\nquestions. It\u2019s important to first determine wat sentiment analysis is, how it works and if it fits the \n\ncontext. \n\n- What is Sentiment Analysis? \n\n- How does Sentiment Analysis work? \n\n- Is Sentiment Analysis the best fit for the context? \n\n \n\nResearch methods \nWe will be using literature-based research and the creation of a proof of concept. This will also be \n\nvalidated with the stakeholder to guarantee a right fit for the problem. Combining the three methods \n\nwill result in certainty and fit. \n\n  \n\n\n\n \n3 \n\nWhat is Sentiment Analysis? \nSentiment analysis is used for  discovering the polarity of a text, which can be defined as negative, \n\nneutral, or positive.  This could be done with a rule-based or machine learning approach (or a hybrid). \n\nThis could for example be used for quickly gaining insight into the sentiment of people towards a \n\ntweet or scanning product reviews in bulk.  \n\n \n\nTypes of sentiment analysis  \n\nFine-grained sentiment analysis  \nThis type of analysis focusses on a better, more defined assessment of the sentiment of the text in \n\nquestion. It will divide it in more categories e.g., very negative, negative, neutral, positive, and very \n\npositive. This is perfect for generating a star-based evaluation of a review.   \n\n \n\nEmotion detection  \nIt\u2019s also possible to focus on detecting emotion within a text. It might be more suitable for a problem \n\nto be quantified in types of emotion. While \u2018thrilling\u2019 and \u2018interesting\u2019 could both be construed as \n\npositive, they convene a different sentiment.   \n\nThis type of sentiment can be tricky though, since people express their emotions differently, e.g. \u2018I \n\nwould kill for that\u2019.   \n\n \n\nAspect-based sentiment analysis  \nA piece of text might be a reaction to an issue with multiple facets. You might be interested in \n\ndetermining which aspects are received negatively, and which are received positively. For this you \n\ncan use aspect-based sentiment analysis, which tries to determine multiple facets in the text and \n\nassign the sentiment accordingly  \n\n \n\nReal-time sentiment analysis.  \nIt could also be possible to monitor for example tweets or posts on a forum in real-time. You could \n\nset a threshold for when you should be notified and discover issues or trends quickly. (Monkeylearn, \n\nz.d.)  \n\nIn general rule-based solutions are faster in this than implementations based on Machine Learning: \n\nHowever, when compared to sophisticated machine learning techniques, the \n\nsimplicity of VADER carries several advantages. First, it is both quick and \n\ncomputationally economical without sacrificing accuracy. (Hutto & Gilbert, 2014, \n\np. 10) \n\n  \n\n\n\n \n4 \n\nHow does Sentiment Analysis work? \nRule-based programs generally have a lexicon with words that have a score and then use an \n\nalgorithm to determine the sentiment of a piece of text. Most of the time they don\u2019t take position or \n\ndouble negatives into account. Which can result in less accurate determinations. \n\nMost solution use machine-learning or a combination of rules and machine-learning. Most \n\nimplementations use a bag of words or bag-of-ngrams to vectorize the text.   \n\nA bag of words uses a list of predetermined words to search and quantify them from the text. A bag \n\nof n-grams search for a combination of words. Both methods spit out a vector representation of the \n\npiece of text, the vector represents which words or n-grams are present in the text. Then these \n\nvectors can be scored. (Brownlee, 2019)  \n\nTo score the vectors the vocabulary must be classified e.g., positive, neutral, negative.  \n\n \n\nDrawbacks  \nIt\u2019s hard to detect irony or sarcasm, furthermore sentiment must be placed in context. Because \n\nsentiment is subjective, it\u2019s also difficult to score words. This leads to needing to define the terms \n\nwith which the text is going to be scored.   \n\n \n\nImplementations   \nThere are multiple resources available for scoring text based on sentiment. There is Microsoft Azure \n\nText analytics, IBM Watson natural language detection, Amazon Comprehend and VaderSentiment.  \n\nInterestingly VaderSentiment is the only rule-based implementation between these noteworthy \n\nservices. However, it scores well in comparison to the above services: \n\nVADER performed as well as (and in most cases, better than) eleven other highly \n\nregarded sentiment analysis tools. (Hutto & Gilbert, 2014, p. 11) \n\nMost are extensive and complicated. There are also explanations available online on how to train \n\nyour own model (Jain, 2020)  \n\n \n\n  \n\n\n\n \n5 \n\nIs Sentiment Analysis de best solution for the context? \nFor the context, real-time sentiment analysis can be used. A rule-based implementation would be \n\npreferable since it is very fast and light weight. Feedpulse can be in different languages, however \n\nVaderSentiment also works with multiple languages, it states that it is accurate in determining \n\nsentiment even when a sentence is first translated to English and then analyzed. (C, z.d.). \n\nTaking these points in account VaderSentiment is a good fit for the context. It must be noted \n\nhowever that it is made for analyzing small pieces of text, namely focused on social media, and is \n\nprobably less accurate on larger texts.  \n\n \n\nProof of Concept \nAs a proof of concept, I\u2019ve created a simple JavaScript application that reads out a textbox. To extract \n\nsentences, it monitors usages of points (\u2018.\u2019). After every \u2018.\u2019 entry, it takes the text from the previous \n\npoint, up until the latest point. It uses Google\u2019s Compact Language detector to then determine the \n\nlanguage, if it\u2019s not English it first translates the sentence to English. \n\nThen it analyses this piece of text with the VaderSentiment JavaScript library. (V, z.d.-b). This results \n\nin four scores, the positivity, naturality and negativity.  \n\nThe final score is the compound score, it uses rules to give more weight to certain situations. For \n\nexample, in the following sentence: \u201cThe idea was great, however the execution was horrible.\u201d It \n\nregards the part after however as more important. \n\nhttps://github.com/Quantified-Student/POCs/tree/main/poc-sentiment-textbox  \n\n \n\n \n\nhttps://github.com/Quantified-Student/POCs/tree/main/poc-sentiment-textbox\n\n\n \n6 \n\nConclusion and Advise \nTo conclude this research, I can state with certainty that Sentiment Analysis could indeed be used to \n\nmake students aware of the tone of their text in real-time. Sentiment analysis is accurate and can be \n\ndone sufficiently quick, so that real-time analysis is possible. \n\nMy advice is to use a rule-based lexicon implementation, since it is light-weight giving fast results. \n\nThe most used and citated implementation is VaderSentiment, and it proves to fit for the context. \n\nUsing one of the VaderSentiment libraries, passing in a sentence, and then using the result to nudge \n\nthe student would be the desired course of action. \n\n \n\n \n\nGlossary \nBrownlee, J. (2019, 7 August). A Gentle Introduction to the Bag-of-Words Model. Machine Learning \n\nMastery. https://machinelearningmastery.com/gentle-introduction-bag-words-model/  \n\nMonkeylearn. (z.d.). Everything There Is to Know about Sentiment Analysis., van \n\nhttps://monkeylearn.com/sentiment-analysis/  \n\nJain, S. (2020, 5 June). Ultimate guide to deal with Text Data (using Python) \u2013 for Data Scientists and \n\nEngineers. Analytics Vidhya. https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-\n\ndeal-text-data-predictive-python/ \n\nHutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of \n\nSocial Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann \n\nArbor, MI, June 2014. \n\nC. (z.d.). cjhutto/vaderSentiment. GitHub. https://github.com/cjhutto/vaderSentiment#demo-\n\nincluding-example-of-non-english-text-translations \n\nV. (z.d.-b). vaderSentiment/vaderSentiment-js. GitHub. Geraadpleegd op 18 maart 2021, van \n\nhttps://github.com/vaderSentiment/vaderSentiment-js \n\n \n\n \n\nhttps://github.com/cjhutto/vaderSentiment#demo-including-example-of-non-english-text-translations\nhttps://github.com/cjhutto/vaderSentiment#demo-including-example-of-non-english-text-translations\nhttps://github.com/vaderSentiment/vaderSentiment-js\n\n"
            }, {
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearch proficiency analysis\n\n\n   \n \n\n   \n \n\n \n\n  \n2021 \n\nResearch proficiency \nanalysis \n\nRESEARCH THE POSSIBILITY OF ANALYSING TEXT PROFICIENCY \n\nNIEK VAN DAM \n\n\n\nResearch proficiency analysis  Niek van Dam \n\n \n1 \n\nVERSION CONTROL \n\n \n\nVersion Author Date Adjustments \nV0.1 Niek van Dam   Initial document setup \nV0.2 Niek van Dam 03/03/2021 Added research \nV1.0 Niek Van Dam 29/03/2021 Implementing new \n\nfeedback \n \n\n \n\nTABLE OF CONTENTS  \n\nVersion Control ........................................................................................................................................................ 1 \n\nIntroduction ............................................................................................................................................................. 2 \n\nResearch questions .............................................................................................................................................. 2 \n\nResearch ................................................................................................................................................................... 3 \n\nPaper 1: Predicting Proficiency levels .................................................................................................................. 3 \n\nAlternatives .......................................................................................................................................................... 3 \n\nAnalysing proficiency based on grammar mistakes ......................................................................................... 3 \n\nCalculating the ARI (Automated readability index) ......................................................................................... 4 \n\nGunning fog index ............................................................................................................................................ 4 \n\nFlesch-Kincaid grade......................................................................................................................................... 4 \n\nPoC: Using translated texts ...................................................................................................................................... 5 \n\nDiscusison ................................................................................................................................................................. 2 \n\nWhy is there no PoC regarding research paper ................................................................................................... 2 \n\nFinal thoughts ........................................................................................................................................................... 3 \n\nPaper 1 ................................................................................................................................................................. 3 \n\nARI ........................................................................................................................................................................ 2 \n\nGunning Fog Index ............................................................................................................................................... 2 \n\nFlesch-Kincaid ...................................................................................................................................................... 2 \n\nConclusion ................................................................................................................................................................ 1 \n\nSources ..................................................................................................................................................................... 2 \n\n \n\n  \n\n\n\nResearch proficiency analysis  Niek van Dam \n\n \n2 \n\nINTRODUCTION \n\nWithin the Quantified Student project, we thought of a new feature where we would analyse the written text \n\nfrom the students and detect whether their proficiency level in that language is up to standards. Before we can \n\ngo into creating PoCs about this new feature, we first must research whether this is even feasible.  \n\nThe research will be conducted by incorporating Literature study as main research method.  \n\nRESEARCH QUESTIONS \n\nThe main question I want to get answered in this research is the following: \n\n- Is it possible to analyse someone\u2019s language level based using NLP? \n\n- Is the approach that is proposed feasible? \n\n- Is the approach able to be implemented for the target languages (Dutch and English)? \n\n  \n\n\n\nResearch proficiency analysis  Niek van Dam \n\n \n3 \n\nRESEARCH \n\n \n\nPAPER 1:  PREDICTING PROFICIENCY LEVELS \n\nThis paper covers the measuring of proficiency levels by comparing the linguistic complexity to that of a \n\ntextbook at the given level. The language used in the textbooks is recorded per proficiency level, measured by \n\nthe CEFR (Common European Framework of Reference for languages). This level goes from A to C, respectively \n\nbeing ranked from basic to proficient user. These grades can further be subdivided according to the needs of \n\nthe local context. The idea behind this research method is that the model can read the input data from the \n\nstudent, normalize it to its basal form and compare the formatted text to the proficiency level. \n\nThe focus within this research paper was put on the correcting and normalizing of grammar mistakes, which \n\nled to greater performance within the network. This gave it a better accuracy and comprehension level of the \n\ntext it is analysing. The reason why it focusses so much around the cleaning of data is because the method \n\napplied in the paper has a really narrow error margin, making it underperform significantly on poorly prepared \n\ndata.   \n\nThere is a possibility of implementing this into FeedPulse, as the comparison of languages against a textbook \n\ncan be done in every language. There is an argument to be made regarding the workload this would bring with \n\nit, but the possibility remains.   \n\n \n\nALTERNATIVES \n\nDuring my research I saw a lot of instances where the NLP model used was way over the top for our \n\nimplementation, which, although interesting, did not meet the requirements needed (mostly since the \n\napproach was not feasible and too avant-garde). There were some more basic approaches which were covered \n\nthroughout my research, which seemed more in-line with what we are trying to achieve. The most promising \n\nmethods have been noted down below. \n\n \n\nANALYSING PROFICIENCY BASED ON GRAMMAR MISTAKES  \n\nThe analysing of proficiency based on grammar mistakes is a more feasible goal than the methods which are \n\nproposed in the research papers. Mostly because the analysing of grammar mistakes is a lot easier to do and \n\ndoes not require the same amount of preparation to be done beforehand. One of the easiest ways to \n\nimplement this metric is by calculating the average rate of grammar mistakes per sentence. We can measure \n\nthese against a predefined scale, which can later be given back to the student as feedback.  \n\n \n\nThe reason this is easier to implement than one of these research papers is because we can incorporate pre-\n\nexisting software for this, and do not require us to build our own solution from scratch. \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\nResearch proficiency analysis  Niek van Dam \n\n \n4 \n\n \n\nCALCULATING THE ARI (AUTOMATED READABILITY INDEX) \n\nThe readability index is a simple but powerful formula which rates the readability of the given text. The \n\nformula only requires basic metrics, which can easily be extracted from the data. The formula is as follows: \n\n4.71 (4,71 (\n\ud835\udc36\n\n\ud835\udc64\n) + 0,5 (\n\n\ud835\udf14\n\n\ud835\udc60\n) \u2212 2 \n\nWhere c is the number of letters and numbers, w is the number of \n\nspaces, and s is the number of sentences (not necessarily the number \n\nof periods present in a piece of text).  \n\nThis formula outputs a number between 1-14, which can be used to \n\ncalculate the grade level at which the text has been written. The \n\nscores can be a decimal number, in which case they will be rounded \n\nup; causing a 10.1 to be interpreted as an 11. The meaning of the \n\nscores is in the table to the left, taken from the Wikipedia page \n\nregarding ARI. \n\n \n\n \n\n \n\n \n\nGUNNING FOG INDEX \n\nIn linguistics, the Gunning fog Index is a readability formula for English writing which estimates the years of \n\nformal education needed to understand the text on first reading. This method has been developed in 1952 by \n\nRobert Gunning and has been used since. The fog index is commonly used to confirm that text can be read \n\neasily by the intended audience and can also be used to indicate the level (proficiency) of your writing skills.  \n\nThe Gunning fog index is calculated with the following formula:  \n\n0.4[(\n\ud835\udc64\ud835\udc5c\ud835\udc5f\ud835\udc51\ud835\udc60\n\n\ud835\udc60\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc52\ud835\udc60\n) + 100 (\n\n\ud835\udc50\ud835\udc5c\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52\ud835\udc65 \ud835\udc64\ud835\udc5c\ud835\udc5f\ud835\udc51\ud835\udc60\n\n\ud835\udc64\ud835\udc5c\ud835\udc5f\ud835\udc51\ud835\udc60\n)] \n\nIn comparison to the ARI, we do not round this number up or down \n\nwhen checking the end score, if the number is a decimal, it should be \n\ntreated as \u2018in between\u2019 the levels. The results of this index can be seen \n\nin the table to the left.  \n\n \n\n \n\n \n\n \n\n \n\nFLESCH-KINCAID GRADE \n\nThe Flesch-Kincaid grading system is a widely used readability formula which assesses the approximate reading \n\nlevel of a given text. It has been developed by the US Navy who worked with the Reading Ease formula first but \n\nScore Age Grade level \n\n1 5-6 Kindergarten \n\n2 6-7 1st/2nd grade \n3 7-9 3rd grade \n\n4 9-10 4th grade \n\n5 10-11 5th grade \n\n6 11-12 6th grade \n\n7 12-13 7th grade \n\n8 13-14 8th grade \n\n9 14-15 9th grade \n\n10 15-16 10th grade \n\n11 16-17 11th grade \n\n12 17-18 12th grade \n\n13 18-24 College student \n14 24+ Professor \n\nFog Index Reader level by grade \n\n17 College graduate \n\n16 College senior \n\n15 College junior \n14 College sophomore \n\n13 College first-year \nstudent \n\n12 High school senior \n\n11 High school junior \n\n10 High school sophomore \n\n9 High school first-year \nstudent \n\n8 8th grade \n\n7 7th grade \n\n6 6th grade \n\nhttps://en.wikipedia.org/wiki/Automated_readability_index\n\n\nResearch proficiency analysis  Niek van Dam \n\n \n5 \n\nhas been converted to the analysis of reading levels instead of reading ease. The obsolete version still exists; \n\nhowever, it is not as clear in its grading as the current formula. \n\nThe formula uses the same principle as the Gunning fog index but uses different variables and therefore also \n\noutputs a different result. The result which this grade gives is more generalised, whereas the fog index is \n\nspecific enough to classify writer grades.  \n\nThe formula for this grade is as follows:  \n\n0.39 (\n\ud835\udc64\ud835\udc5c\ud835\udc5f\ud835\udc51\ud835\udc60\n\n\ud835\udc60\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc52\ud835\udc60\n) +  11.8 (\n\n\ud835\udc60\ud835\udc66\ud835\udc59\ud835\udc59\ud835\udc4e\ud835\udc4f\ud835\udc59\ud835\udc52\ud835\udc60\n\n\ud835\udc64\ud835\udc5c\ud835\udc5f\ud835\udc51\ud835\udc60\n) \u2013  15.59  \n\nWhere all variables describe the count of a property within the \n\ngiven text (words -> total words, syllables -> total syllables). \n\nThis formula results in a number between 0-18 and displays the \n\ndata in 6 different sections, in value pairs of 3.  \n\n \n\n \n\nPOC: USING TRANSLATED TEXTS  \n\nThe alternative solution: \u2018readability indexes\u2019 discussed above, all give a clear and direct grading of the text to \n\nbe analysed, but have one prominent requirement which could be tricky. This is regarding the input language. \n\nThe readability indexes are all made with the mindset of grading English texts. In theory these readability \n\nindexes can be modified to a desired language, as the majority of these readability indexes solely require the \n\namount of: syllables, words and sentences. Before we can reliably apply this to every language, I decided to \n\nvalidate it myself with the following pieces of text, found on the \u2018Dutch dummy text generator\u2019: \n\n\u201cIk ben makelaar in koffi, en \n\nwoon op de Lauriergracht. Het is \n\nmijn gewoonte niet, romans te \n\nschrijven, of zulke dingen, en het \n\nheeft dan ook lang geduurd, voor \n\nik er toe overging een paar riem  \n\npapier extra te bestellen, en het \n\nwerk aan te vangen, dat gij, lieve \n\nlezer, in de hand hebt genomen, \n\nen dat ge lezen moet als ge \n\nmakelaar in koffie zijt, of als ge \n\nwat anders zijt. Niet alleen dat ik \n\nnooit iets schreef wat naar een \n\nroman geleek, maar ik houd er \n\nzelfs niet van, iets dergelijks te \n\nlezen.\u201d (Max Havelaar - Multatuli)  \n\n \n\n \n\n \n\n \n\n\u201cDe volle maan, tragisch dien \n\navond, was reeds vroeg, nog in \n\nden laatsten dagschemer \n\nopgerezen als een immense, \n\nbloedroze bol, vlamde als een \n\nzonsondergang laag achter de \n\ntamarindeboomen der Lange \n\nLaan en steeg, langzaam zich \n\nlouterende van hare tragische \n\ntint, in een vagen hemel op. Een \n\ndoodsche stilte spande alom als \n\neen sluier van zwijgen, of, na de \n\nlange middagsi\u00ebsta, de avondrust \n\nzonder overgang van leven \n\nbegon.\u201d  (De Stille Kracht \u2013 Louis \n\nCouperus)  \n\n \n\n \n\n \n\n \n\n \n\n\u201cOnbegrijpelijk veel mensen \n\nhebben familiebetrekkingen, \n\nvrienden of kennissen te \n\nAmsterdam. Het is een \n\nverschijnsel dat ik eenvoudig \n\ntoeschrijf aan de veelheid der \n\ninwoners van die hoofdstad. Ik \n\nhad er voor een paar jaren nog \n\neen verre neef. Waar hij nu is, \n\nweet ik niet. Ik geloof dat hij naar \n\nde West gegaan is. Misschien \n\nheeft de een of ander van mijn \n\nlezers hem wel brieven \n\nmeegegeven. In dat geval hebben \n\nzij een nauwgezette, maar \n\nonvriendelijke bezorger gehad, \n\nals uit de inhoud van deze \n\nweinige bladzijden waarschijnlijk \n\nduidelijk worden zal.\u201d (Camera \n\nObscura \u2013 Hildebrand) \n\nThe method I used for testing these is by programming the readability indexes myself and putting these texts \n\nthrough it. I used google translate for the translation of the texts from Dutch to English, as their translation \n\nservice is \u2013 in most situations \u2013 of high quality. Once translated, I process both versions of the text through the \n\nreading indexes and return the grades. The grades have been noted in the table below, where the highest \n\nscoring language is indicated with green and the lower one with red.  \n\n \n\nhttps://projects.haykranen.nl/dummytekst/?sourcechoice=cameraobscura&nr=3&choice=paragraphs&Submit=Genereer+een+tekst\n\n\nResearch proficiency analysis  Niek van Dam \n\n \n1 \n\n Text sample 1 Text sample 2 Text sample 3 \n\n EN NL EN NL EN NL \nARI 11,46 13,54 17,04 19,29 6,62 8,86 \n\nFRE 66,40 42,89 40,62 13,73 61,27 44,69 \n\nFKG 15,14 11,45 19,66 16,16 7,65 10,18 \n\nGFI 52,06 52,81 53,93 53,68 44,97 45,35 \n\n \n\nThis data shows us that the reading samples do not return consistent data, as I expected would be the case. In \n\nour results we can see that the highest grading language and index varies from language to language, which \n\nshows inconsistency in the indexes. Following this logic we can hypothesise the following:  \n\nIf student a were to write a piece of Dutch text and student b  were to copy the same text but in English, \n\nstudent b would be graded harsher and have more nudges regarding the quality/readability while the two \n\ntexts are theoretically the same. \n\nThis situation is unwished for, therefore we cannot use the barebones reading indexes and use them on Dutch. \n\nThere are still a lot of variations on the reading indexes, mostly manipulated for different languages, implying \n\nthat there might still be reading indexes which could be applied to the Dutch language.   \n\n\n\nResearch proficiency analysis  Niek van Dam \n\n \n2 \n\nDISCUSISON \n\nAny further questions/inquiries can be discussed here. \n\n \n\nWHY IS THERE NO POC REGARDING RESEARCH PAPER  \n\nThe reasoning for not having a PoC regarding the research paper is because of the fact that the research is too \n\nbroad and comprehensive to actually realise in this short amount of time. We set out a time schedule for \n\nourselves to which we have to stick. When writing our own NLP solution and prepare this much data would \n\ntake far longer than the time available for this research. For this reason I have decided to not write a PoC, the \n\nconclusion of the document itself already gave us most of the important information already. \n\n  \n\n\n\nResearch proficiency analysis  Niek van Dam \n\n \n3 \n\nFINAL THOUGHTS \n\nAfter researching and reading every possible method, what are the final thoughts regarding this research and \n\neach individual method? \n\nPAPER 1 \n\n \n\nThis feature is not something which I think we should be focussing on right now, as this is supposed to be a \n\nsecondary feature, not something which we should direct all our time and resources towards. However, it is \n\ngood to keep in mind that these things can still improve performance within NLP.  \n\nWhen asking the question whether this is feasible to implement into our own systems, I would have to say no. \n\nThere are multiple reasons for this, the main one being that this method takes up a lot of resources and time \n\nto properly implement, which seems over-the-top for a small feature. Besides, this application has only been \n\ntested on one language (Swedish), which has an easier grammar system than the languages we would have to \n\nimplement this software into. In our implementation, we would have to teach the network two languages \n\n(assuming people fill in their FeedPulse in either English or Dutch), this adds another dimension of difficulty as \n\nthese languages have a more complex grammar system than the language used in the paper.  \n\nIn conclusion, I do not think this is a viable approach to use, however, it did explain a lot about the data \n\ncleaning process going on behind the scenes in NLP.  \n\nALTERNATIVES \n\n \n\nARI: \n\nThe ARI looks like a good metric to use to determine proficiency of the writer, without having to train a model \n\nto predict it for us. Since this is only a basic formula, it can more easily be implemented into the system, \n\nrequiring basic text analysis at best.  \n\nGunning Fog Index: \n\nIn conclusion, this index can be used within our application, besides one critical point. This index is only \n\napplicable to the English language, which goes against our target languages (Dutch and English). Therefore, we \n\nwould only be able to apply this to the English side of canvas.  \n\nFlesch-Kincaid: \n\nThis grade level has a lot more potential than the Gunning fog index, as it can be widely applied to every \n\nlanguage for as far as I can tell. This gives it significant pros over the other linguistical grading formulas. \n\n  \n\n\n\nResearch proficiency analysis  Niek van Dam \n\n \n1 \n\nCONCLUSION \n\nAfter reading a handful of academic papers and researching a wide array of language grading systems I can \n\nsafely say that the academic papers are above our reach. The methods which are being used are advanced \n\nenough to be branched into their own little project, which is over the top for the small feature we are trying to \n\nimplement. \n\nThe grading systems and readability indexes, however, show a more promising result. These grading systems \n\nare more generic, which respectively, leads to an easier implementation. Besides that, the data which is \n\nrequired to work for these formulas is significantly lower than what would be needed for the NLP approaches. \n\nWhere the NLP approaches would require several batches of textbook examples, combined with data cleaning; \n\nthe formulas only require primitive parameters which can be extracted from the text with relative ease. \n\nTherefore, I think that we should incorporate the grading systems into this feature, instead of using an NLP \n\nsolution. \n\nWhen choosing between the indexes, there is one which significantly stands out above the rest in terms of \n\nusefulness in my opinion: the Flesch-Kincaid grade. Where the other tools output a grade, which is specifically \n\nfocussed on the American schooling system, the Flesch grade can be represented in more \u2018global\u2019 terms of \n\ndifficulty. The downfall of these methods is in the fact that it is limited to one language, meaning that you will \n\nhave to develop unique reading indexes per language. After trying to fix this issue by using translated text, we \n\nstill got sub-par results which cannot be used in a professional environment. \n\nIn conclusion, I do not think it is feasible to use these proficiency analysis tools as is. Further research can be \n\ndone regarding language-specific reading indexes, but using one reading index for multiple languages is not \n\nfeasible and yields untrustworthy results. If we do get the possibility of realising these indexes, my preference \n\nwould go out to the Flesch-Kincaid grading system, combined with the ARI.  \n\n  \n\n\n\nResearch proficiency analysis  Niek van Dam \n\n \n2 \n\nSOURCES \n\nFlesch, R. (2016, July 12). How to write plain english. Www.Mang.Canterbury.Ac.Nz. \n\nhttps://web.archive.org/web/20160712094308/http://www.mang.canterbury.ac.nz/writing_guide/writin\n\ng/flesch.shtml \n\nKincaid, J., Fishburne, R., Rogers, R., & Chissom, B. (1975, January). Derivation of new readability formulas \n\n(automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel \n\nUniversity of Central Florida. \n\nhttps://stars.library.ucf.edu/cgi/viewcontent.cgi?article=1055&context=istlibrary \n\nThe gunning fog readability formula. (2021). Readabilityformulas.Com. \n\nhttps://www.readabilityformulas.com/gunning-fog-readability-formula.php \n\nUniversity of Gothenburg Sweden. (2016, November). Predicting proficiency levels in learner writings by \n\ntransferring a linguistic complexity model from expert-written coursebooks (No. 1). \n\nhttps://www.aclweb.org/anthology/C16-1198.pdf \n\n \n\n\n\tVersion Control\n\tIntroduction\n\tResearch questions\n\n\tResearch\n\tPaper 1: Predicting Proficiency levels\n\tAlternatives\n\tAnalysing proficiency based on grammar mistakes\n\tCalculating the ARI (Automated readability index)\n\tGunning fog index\n\tFlesch-Kincaid grade\n\n\n\tPoC: Using translated texts\n\tDiscusison\n\tWhy is there no PoC regarding research paper\n\n\tFinal thoughts\n\tPaper 1\n\tAlternatives\n\n\tConclusion\n\tSources\n\n"
            }, {
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm Frequency - Inverse Document Frequency Quantified Student\n\n\nRuben Fricke\n\nTerm Frequency - Inverse Document Frequency\nQuantified Student\n\n\n\nTable of Contents\n\nTable of Contents 2\n\nVersion control 3\n\nIntroduction 5\n\nResearch questions 6\nMain question 6\nSub-question 6\n\nResearch method 6\n\nWhat is TF-IDF 7\nDefinition 7\n\nHow does TF-IDF work? 8\nProof of Concept 10\n\nWhat are the differences with common alternatives? 12\nKeyness 12\nTopic models 12\nAutomatic text summarization 12\n\nWhat are common usages for tf-idf 13\nVisualization & Exploratory 13\nSimilarities 13\nPre-processing 13\n\nIs TF-IDF the best solution in the requested context? 14\n\nConclusion 15\n\nSources 16\n\n2\n\n\n\nVersion control\n\nVersion Author Date Changes\n\n1.0 Ruben Fricke 11-03-2021 Initial document setup\n\n1.1 Ruben Fricke 12-03-2021 How is it calculated\n\n1.2 Ruben Fricke 25-03-2021 Added proof of concept\n\n3\n\n\n\nDependency matrix\n\nVersion Date Dependency\n\n1.2 25-03-2021 Research topic analysis\n\n4\n\n\n\nIntroduction\n\nFor the Quantified Student project, we had a meeting with Eric Slaats. Together we came up\n\nwith a feature that would create some sort of word web giving insight into your skills. It\n\nwould use NLP to analyze your assignments, feed pulse, KPI-matrixes, and other data on\n\nyour individual course to find out what your skills are and at what level these skills are.\n\nFirst application of this feature would be to give more insight into your own progression\n\nand skills. It would be nice to know what topics you have experience in. Another application\n\nfor this feature could be to ask questions to the right person. For example, you\u2019re stuck on\n\na python hosting problem and you already asked a few people and they could not help you\n\nany further. With this feature, the correct student that has knowledge could be found. But\n\nnot only students, but this is also applicable for teachers. Teachers that have more\n\nknowledge about certain topics could give better feedback on a certain assignment on this\n\ntopic. This way, you bring knowledge to the correct place.\n\nAs part of this, research should be done about Term Frequency - Inverse Document\n\nFrequency to see what this means, if we can apply this in this context and to see if it\u2019s\n\napplicable.\n\n5\n\n\n\nResearch questions\n\nMain question\n\nThe main question for this research is as followed:\n\n\u201cHow could TF-IDF contribute to giving more insight into the student\u2019s skill level at their topics\u201d\n\nSub-question\n\nTo answer this question, we have to answer the following sub-questions.\n\n1. What is TF-IDF?\n\n2. How does TF-IDF work?\n\n3. What are the differences with common alternatives?\n\n4. What are common usages for tf-idf\n\n5. Is TF-IDF the best solution in the requested context?\n\nResearch method\n\nThe following research has been performed with the help of the DOT-framework. This\n\nprovides convincing and thorough solutions, research will be performed in several\n\nresearch-areas depending on the question.\n\nTo answer the subquestions, a lot of new knowledge needs to be acquired. Therefore, a big\n\npart of these subquestions will exist in the literature research-area.\n\nAlso, the stakeholder (Eric Slaats) should give fiat to my proposal and a proof of concept will\n\nbe built. This combines different research areas.\n\n6\n\n\n\nWhat is TF-IDF\n\nDefinition\n\nTerm Frequency - Inverse Document Frequency (tf-idf) is a natural language processing and\n\ninformation retrieval method.\n\nIt has many uses, most importantly in automated text analysis, and is very useful for\n\nscoring words in machine learning algorithms for Natural Language Processing (NLP).\n\nTerm Frequency - Inverse Document Frequency was introduced in a 1972 paper by Karen\n\nSp\u00e4rck Jones under the name \u201cterm specificity. It was invented for document search and\n\ninformation retrieval. It works by increasing proportionally to the number of times a word\n\nappears in a document but is offset by the number of documents that contain the word\n\ninstead of by its raw frequency (number of occurrences) or its relative frequency (term\n\ncount divided by document length). So, words that are common in every document, such as\n\nthis, what, and if, rank low even though they may appear many times since they don\u2019t mean\n\nmuch to that document in particular.\n\nSo, the overall effect of this weighting scheme is to avoid a common problem when\n\nconducting text analysis: the most frequently used words in a document are often the most\n\nfrequently used words in all of the documents. (What is TF-IDF?, 2019, Scott, 2019)\n\n7\n\n\n\nHow does TF-IDF work?\n\nA term might be:\n\n1. Frequently used in a language like English, and especially frequent or infrequent in\n\none document\n\n2. Frequently used in a language like English, but used to a typical degree in one\n\ndocument\n\n3. Infrequently used in a language like English, but distinctly frequent or infrequent in\n\none document\n\n4. Infrequently used in a language like English, and used to a typical degree in one\n\ndocument\n\nTo understand how words can be frequent but not distinctive, or distinctive but not\n\nfrequent, let\u2019s look at a text-based example.\n\nRank Term Count\n\n1 The 32\n\n2 she 23\n\n3 python 20\n\n4 TF-IDF 4\n\n5 also 3\n\nWhen you use tf-idf term weighting, this new list will get created\n\nRank Term Count\n\n1 TF-IDF 35\n\n2 python 15\n\n3 she 6\n\n4 The 3\n\n5 also 3\n\n8\n\n\n\nTf-idf can be implemented in many different ways. In this report I will focus on a calculation\n\nthat parallels Scikit-Learn\u2019s tf-idf implementation. This is a popular way and later I will use\n\nthat implementation to create my proof of concept.\n\n\ud835\udc56\ud835\udc51\ud835\udc53\n\ud835\udc56\n\n= \ud835\udc59\ud835\udc5b[(\ud835\udc41 + 1)/\ud835\udc51\ud835\udc53\n\ud835\udc56\n] + 1\n\nThe above formula is the most direct formula to calculate the inverse document frequency\n\nfor each term. N represents the total number of documents that are available. Many\n\nimplementations normalize the results. Scikit-Learn\u2019s implementation represents N as N+1,\n\ncalculates the logarithm of (N+1)/dfi and finally adds 1.\n\nOne's idfi is calculated, tf-idfi is tfi multiplied by idfi. This results in the following formula.\n\n\ud835\udc61\ud835\udc53 \u2212 \ud835\udc56\ud835\udc51\ud835\udc53\n\ud835\udc56\n\n= \ud835\udc61\ud835\udc53\n\ud835\udc56\n \ud835\udc65 \ud835\udc56\ud835\udc51\ud835\udc53\n\n\ud835\udc56\n\n(Lavin, 2019; What is TF-IDF?, 2019)\n\n9\n\n\n\nProof of Concept\n\nI created a proof of concept to validate the functionality of this algorithm. I downloaded a\n\nset of articles for testing the tf-idf functionality. I store all the text files in a variable (line 4).\n\nIn line 18 I create a vectorizer. I use this to convert documents from a list of strings to tf-idf\n\nscores. I had to specify a few parameters. The first parameters are the max_df and min_df\n\nparameter. These parameters control the minimum number of documents a term must be\n\nfound in to be included and the maximum number of documents a term can be found in\n\norder to be included. Setting max_df below 0.9 will most of the time remove all the\n\nstopwords.\n\n10\n\n\n\nThe next parameter is the stop_words parameter. In my code snippet I used\n\nstopwords=None but stopwords=\u2019english\u2019 is also available. This will filter out words using a\n\npreselected list of high frequency function words as \u2018to\u2019 and \u2018of\u2019. In my example I did not use\n\nthis because I tested this on multiple languages. Therefore, to keep this proof of concept\n\nsimple to comprehend I did not specify this (in our topic modeling proof of concept we\n\nused another way to remove stopwords).\n\nAnd the last parameter I specified is the norm parameter. This is used to affect the range of\n\nnumerical scores that the tf-idf algorithm outputs.\n\nAnd finally, from line 30 I will output all the results in its own file. This results in a list of\n\nwords and their according score.\n\n(Scott, 2019, Scikit Learn TfidfVectorizer, n.d.)\n\n11\n\n\n\nWhat are the differences with common alternatives?\n\nTf-idf can be compared with several other ways to rank important term features in a\n\ndocument or collection of documents.\n\nKeyness\n\nAlright, the first alternative we will look into is keyness. Keyness is a broad term for a set of\n\nstatistical measurements that attempt to indicate the numerical significance of a term to a\n\ndocument or a set of documents in comparison with a larger set.\n\nTopic models\n\nTopic modeling and tf-idf are radically different, however online you can read that\n\nnewcomers often just want to run topic modeling on a corpus as a first step and in some\n\ncases running tf-idf would be preferable. Tf-idf is appropriate if you are looking for a way to\n\nget a bird\u2019s eye view of your corpus. Topic models can help explore corpora and they have\n\nseveral advantages. They suggest broad categories or communities of text, this is a general\n\nadvantage of unsupervised clustering methods. Topic models are good because documents\n\nare assigned scores for how well they fit each topic, and because topics are represented as\n\nlists of terms, this provides a good sense of how terms relate to the groupings.\n\nAutomatic text summarization\n\nThe final alternative I will look into is text summarization, for example TextRank. TextRank\n\nand tf-idf are different in their approach to retrieve information. Although, the goal of both\n\nalgorithms has overlap.\n\n(Lavin, 2019)\n\n12\n\n\n\nWhat are common usages for tf-idf\n\nTf-idf is mainly used for information retrieval and weighting term frequencies against\n\nnorms in a larger corpus. Tf-idf is suited for a particular set of tasks, these tasks usually fall\n\ninto one of the following groups.\n\nVisualization & Exploratory\n\nA common usage for tf-idf is as an exploratory tool or visualization technique. Terms lists\n\nwith tf-idf scores for each document that is available can be a strong interpretive aid on its\n\nown, they can help generate hypotheses or research questions. Word lists can also be used\n\nfor browsing strategies and visualizations. (Stray, 2010, Duplan, 2019)\n\nSimilarities\n\nOther use cases for tf-idf are tasks involving textual similarity, because tf-idf will produce\n\nhigh scores for terms related to the topic of the text. A search index can be performed\n\nusing tf-idf and return results to users searched by looking for documents with the highest\n\nsimilarity to the user\u2019s search string. (Scott, 2019)\n\nPre-processing\n\nFinally, another good application for tf-idf is pre-processing. Features that have been\n\ntransformed using tf-idf tend to have more predictive values compared to raw term\n\nfrequencies. This is especially the case when you use a supervised machine learning model\n\nbecause it tends to increase the weight of topic words and reduce the weight of high\n\nfrequency function words. (Lavin, 2019)\n\n13\n\n\n\nIs TF-IDF the best solution in the requested context?\n\nThe three use cases could all be used in our project, however I recommend to primarily use\n\nit for pre-processing and for visualizations.\n\nI do not recommend tf-idf for similarities. A use case for similarities for us could be to ask\n\nother students help on a topic they have more knowledge about. Tf-idf works at the word\n\nlevel, so a document that is about helmets might be far from a document about bikes when\n\nrepresented using tf-id. On the other hand, in topic analysis we\u2019ll be able to find groups of\n\nrelated words, which you can't do in tf-idf. This would work better for the specified use\n\ncase.\n\nPre-processing would be one of the best use cases for our project. As said in the last\n\nsection,  if you use a supervised machine learning model it tends to increase the weight of\n\ntopic words and reduce the weight of high frequency function words. This is a great\n\ncombination of two techniques we might use.\n\nAnd finally another use case is visualisation. Tf-idf is great for this. Tf-idf can get a selection\n\nof important words and is able to create a visualisation for this. In our project, this would\n\nbe a perfect option to create a wordweb. An easy implementation could be to use the\n\nWordCloud library to create a word cloud for a selection of the words.\n\n14\n\n\n\nConclusion\n\nFor creating a word web I can recommend using tf-idf. Tf-idf can be a perfect candidate for\n\ndoing this. In my research I can also conclude that tf-idf can also be used for\n\npre-processing, especially for supervised machine learning.\n\n15\n\n\n\nSources\n\nWhat is TF-IDF? (2019, May 10). MonkeyLearn Blog.\n\nhttps://monkeylearn.com/blog/what-is-tf-idf/\n\nScott, W. (2019, May 21). TF-IDF from scratch in python on real world dataset.\n\nMedium.\n\nhttps://towardsdatascience.com/tf-idf-for-document-ranking-from-scra\n\ntch-in-python-on-real-world-dataset-796d339a4089\n\nDuplan, S. (2019, November 6). Visualizing Topic Models with Scatterpies and\n\nt-SNE. Medium.\n\nhttps://towardsdatascience.com/visualizing-topic-models-with-scatterp\n\nies-and-t-sne-f21f228f7b02\n\nLavin, M. J. (2019, May 13). Analyzing Documents with TF-IDF. Programming\n\nHistorian.\n\nhttps://programminghistorian.org/en/lessons/analyzing-documents-wi\n\nth-tfidf\n\n16\n\n\n\nStray, A. (2010, December 10). A full-text visualization of the Iraq War Logs.\n\nJonathan Stray.\n\nhttp://jonathanstray.com/a-full-text-visualization-of-the-iraq-war-logs\n\nScikit learn TfidfVectorizer. (n.d.). Https://Scikit-Learn.Org/.\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.feature_extr\n\naction.text.TfidfVectorizer.html\n\n17\n\n\n"
            }, {
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantified Student \u2013 Topic Analysis \nMax van Hattum & Ruben Fricke \u2013  Delta \u2013  22-3-2021 \n\n\n\n \n1 \n\nTable of Contents \nDependency matrix .................................................................................................................................. 2 \n\nIntroduction.............................................................................................................................................. 3 \n\nResearch question .................................................................................................................................... 3 \n\nResearch-methods ................................................................................................................................... 3 \n\nWhat is Topic Analysis? ............................................................................................................................ 4 \n\nApplications for Topic Analysis............................................................................................................. 4 \n\nAdvantages and disadvantages ............................................................................................................ 4 \n\nTopic Modeling vs Topic Classification ................................................................................................. 4 \n\nHow does Topic Analysis work? ............................................................................................................... 5 \n\nTopic modeling ..................................................................................................................................... 5 \n\nLatent Semantic Analysis (LSA) ........................................................................................................ 5 \n\nLatent Dirichlet Allocation (LDA) ...................................................................................................... 6 \n\nTopic classification ............................................................................................................................... 7 \n\nRule-based system ............................................................................................................................ 7 \n\nMachine learning system ................................................................................................................. 7 \n\nIs Topic Analysis the right fit for the context? ......................................................................................... 8 \n\nDiscussion ............................................................................................................................................... 11 \n\nConclusion .............................................................................................................................................. 11 \n\nSources ................................................................................................................................................... 12 \n\n \n\n\n\n \n2 \n\nDependency matrix \nDependency Dependency version \n\nQuantified Student - TF-IDF research 1.2 \n\n  \n\n\n\n \n3 \n\nIntroduction \nQuantified Student wants to enable students to get more insight in their learning process. We want \n\nto achieve this by gathering and processing all kinds of data available through the digital learning \n\nenvironment. \n\nThis should allow students to understand their learning process and adjust where deemed necessary.  \n\nOne of these concepts is to be able to connect students with each other based on the assignments \n\nthey create. This will allow them to more easily get in contact with peers who already worked on a \n\nsimilar assignment. \n\nThis means that research needs to be done about Topic analysis, what it is, if it\u2019s applicable for our \n\ncontext and how it should be implemented. \n\nAls onderdeel hiervan moet er onderzoek gedaan worden naar Topic Analysis, wat dit inhoudt, of het \n\ntoepasselijk is en hoe het toegepast kan worden in de context. \n\n \n\nResearch question \nThe main questions is as follows: \n\nHow could Topic Analysis contribute to connecting students with each other based on \n\nsimilar assignments? \n\nTo be able to answer this question accurately, sub questions will be defined. It\u2019s important to get a \n\ngood understanding of what Topic analysis is, how it works and how it should be applied in the \n\ncontext.  \n\n- What is Topic Analysis? \n\n- How does Topic Analysis work? \n\n- Is Topic Analysis the right fit for the context? \n\n \n\nResearch-methods \n De onderzoeksmethoden wordt beschreven met de terminologie van het Dot framework. Om de \n\neerste twee deelvragen te beantwoorden moet er veel nieuwe kennis opgedaan worden. Voor een \n\ngroot gedeelte gaat dit onderzoek dus gebruik maken van literatuuronderzoek, wat valt binnen het \n\nbieb domein.  \n\nOm de derde deelvraag te beantwoorden moet er gebruik gemaakt worden van prototyping en \n\nvergelijkingen met andere oplossing. Ook moet er geverifieerd worden met de stakeholder (Eric \n\nSlaats) dat de oplossing past in de context. Hiermee raken we dus de werkplaats en veld domeinen. \n\nDoor deze combinatie van strategie\u00ebn wordt gegarandeerd dat er een goed overzicht van de kennis \n\nbereikt wordt en dat er met zekerheid een correct antwoord gegeven kan worden op de hoofdvraag. \n\n  \n\n\n\n \n4 \n\nWhat is Topic Analysis? \nTopic analysis is a machine learning technique that is used when great quantities of documents need \n\nto be organized based on their subjects. This is done by assigning these documents in \u2018buckets\u2019 \n\nwhere they are grouped with similar documents. \n\nTopic analysis used Natural Language Processing to understand the documents, then it recognizes \n\ncertain patterns and structures. It\u2019s not only used on a document level, but also for sentences or \n\neven parts of sentences. \n\n \n\nApplications for Topic Analysis \nIt can be applied immensely broadly, and even more so when combined with other techniques like \n\nsentiment analysis. First you could split up a document based upon topics, then you could apply \n\nsentiment analysis on singular topics. \n\nExamples of use cases like this are: analyzing product reviews, customer service complaints, SEO or \n\nsocial media. Basically, if a great quantity of documents need to be grouped together based on \n\nsubjects, but the set is to huge to be sorted through manually, then Topic Analysis is a fitting \n\nsolution.  \n\n \n\nAdvantages and disadvantages  \nAfter a model is trained, it will assign documents to categories consistently in the same way, you will \n\nalways get the same results. In contrary to assigning documents with humans, who will always judge \n\ndocuments in a different way. Each person has a slightly different way of classifying documents. \n\nMoreover Topic analysis is easily scaled horizontally, and can be applied in real-time. \n\nHowever there are also several disadvantages, the training of a model can be very hard, and Topic \n\nAnalysis is inherently prone to overfitting. This means that it performs very well on the training data, \n\nbut not so much on real, unknown for the model, data. On top of that is that the model does not jibe \n\nwell with documents that do not fit in the previously (during training) established categories. \n\n \n\nTopic Modeling vs Topic Classification \nThere are two most used ways to approach Topic Analysis, these are Topic Modeling and Topic \n\nClassification. Topic modeling uses a unsupervised way of machine learning, this means that the \n\ndataset does not have to be labeled beforehand, the model will define categories on its own. \n\nTopic Analysis is trained with prelabeled data, where all the documents that are used for training are \n\nalready assigned to a category. \n\nWhen comparing these two methods with each other it becomes clear that Topic Modeling is more \n\nflexible and can be set up quickly, while Topic Analysis is harder to set up. This is because there are \n\nmore requirements for the data to meet and the categories need to be defined from the beginning. \n\nHowever, when Topic Analysis is set up, it is more accurate than Topic Modeling \n\n \n\n \n\n\n\n \n5 \n\nHow does Topic Analysis work? \nThe two most common approaches for topic analysis with machine learning are NLP topic modeling \n\nand NLP topic classification. \n\nThe topic modeling variant is an unsupervised machine learning technique while topic classification is \n\nsupervised.  \n\nTopic modeling \nTopic analysis models can detect topics in a text with advanced machine learning algorithms that \n\ncount words and find and group similar word patterns. \n\nTo better understand the ideas behind topic modeling, we must understand the two most popular \n\nalgorithms: LSA and LDA. \n\nLatent Semantic Analysis (LSA) \nLatent Semantic Analysis is seen as the \u2018standard\u2019 method for topic modeling. This method is based \n\non the distributional hypothesis principle. Basically, this means that words and expressions that \n\noccur in similar pieces of text will have similar meanings. \n\nIt is based on the word frequencies of the dataset. The general idea is that for every word in each \n\ndocument, you can count the frequency of that word and group together the documents that have \n\nhigh frequencies of the same word. \n\nAlthough, this approach is limited, so many times tf-idf is used here. In our TF-IDF research we look \n\ninto the application of tf-idf to see if it's applicable for our use cases. After doing the word frequency \n\ncalculation we are left with a matrix that has a row for every word and a column for every document. \n\nEach cell is the calculated frequency for that word in that document. This is the document-term \n\nmatrix.  \n\nIn this matrix we must extract a document-topic matrix and a term-topic matrix which relate to the \n\ntopics and term topics. This can be done using singular value decomposition (SVD).  \n\n(Wikipedia contributors, 2021) \n\n \n\n  \n\n\n\n \n6 \n\nLatent Dirichlet Allocation (LDA) \nImagine a fixed set of topics. Each topic is represented by an unknown set of words. These are the \n\ntopics that our documents cover, but we do not know these topics are.  \n\nLDA tries to map all the documents to the topics in such a way that the words in each document are \n\nmostly covered by these topics. \n\nLDA and LSA both use the same fundamental assumption: \u2018Documents with the same topic will use \n\nsimilar words.  \n\nLDA assumes documents are generated in a mixture of topics and then picks words that belong to \n\nthose topics. These words are picked randomly according to how likely they\u2019re to appear in a \n\ndocument. \n\nSo, LDA sees a document and then works backwards from the words that make up the document and \n\ntries to guess the mixture of topics that resulted in that order of words. \n\nThe implementation has two parameters for training (alpha and beta). Alpha is used to control the \n\nsimilarity of documents. So, this means that when alpha is low, this represents documents as a \n\nmixture of few topics and a high alpha more topics. Beta is the same but used for topic similarity. A \n\nlow value will represent topics as distinct. A high value results in topics containing more words. \n\nThese values must be specified before the training starts.  \n\n(Wikipedia contributors, 2021b, Dwivedi, 2019) \n\n  \n\n\n\n \n7 \n\nTopic classification \nIn contrast to top modeling, with topic classification you already know what the topics are. Unlike the \n\nalgorithms for topic modeling, the machine learning algorithms are supervised. This means you need \n\nto feed them documents already labeled by topic to learn the algorithm how to label new unseen \n\ndocuments with these topics.  \n\nBut how would label topics for documents with a different issue? Well, if you\u2019re looking to automate \n\nsome already existing tasks, then you probably have a good idea about the topics of the text. \n\nOtherwise, you could use the topic modeling methods to better understand the content beforehand. \n\nSince automated classification always involves a first step of manually analyzing and tagging texts, \n\nyou usually end up updating your topics.  \n\nRule-based system \nIt\u2019s important to note that it\u2019s possible to build a topic classifier entirely without machine learning. \n\nYou do this by programming a set of rules based on the content of the documents that a human \n\nexpert read. The idea is that the rules represent the knowledge of the expert. Each one of these rules \n\nconsists of a pattern and a prediction.  \n\nRule-based systems are comprehensible for humans. Humans can read these rules, update existing \n\nrules and add new ones.  \n\nThis sounds great right? But there are some disadvantages. Remember that I used the word \u2018human \n\nexpert'? This is necessary because the system requires deep knowledge of the topic. This can take a \n\nlot of time and can be expensive. Besides that, although I said human can update it, it\u2019s hard to do so. \n\nRule based systems are hard to maintain and don\u2019t scale easily. Adding new rules will affect the \n\nperformance that were already created.  \n\nMachine learning system \n\nSo, let\u2019s continue to machine learning classification. With machine learning classification examples of \n\ntext and expected categories are used to train the model. This model learns to recognize patterns an \n\ncategorizes the text in categories. To train a model, you must create data that the model can \n\ncomprehend, in other words, you must convert the data to vectors. At this moment they are fed to \n\nthe algorithm which uses these data to create a model that can classify text.  \n\nTo categorize new pieces of text, the model will convert this new text to vectors, extracts important \n\nfeatures and makes the prediction.  \n\nThe model can be improved by keep training it with more and more data. Another thing that can be \n\nused to improve the outcome of the model is changing the training parameters.  \n\n(Topic Analysis: a comprehensive guide to detecting topics from text, z.d.) \n\n\n\n \n8 \n\nIs Topic Analysis the right fit for the context? \nTo ascertain that Topic analysis is the right fit for the context, a POC will be set up. Since for the use \n\ncase the categories will not be known before hand, Topic Modeling is the method that is going to be \n\nused. The POC needs to prove that it is indeed feasible to train a model that can accurately \n\ndetermine topics based on a collection of documents. \n\nThe data that is used to train the model is a collection of news articles. First text preproccesing will \n\nbe done, namely removing whitespace, punctuation and stop words. Then TF-IDF is used to \n\ndetermine which words are important in a document, these words will be given extra weight. \n\nAlso, to check if the results are indeed correct, two word clouds will be made, one based on the titles \n\nand one based on the content of the articles. \n\nThere are several libraries that will be used, the most important ones are Pandas, Gensim, nltk, dfITF \n\nand pyLDAvis. Pandas is used for easily organize our data in dataframes. Gensim is used for creating \n\nthe Topic Modeling model, in combination with pyLDAvis which visualizes this model for us. Nltk and \n\ndfITF are both used for text preprocessing purposes. \n\n \n\nFrist we define some methods mostly for text preprocessing purposes, and also one for creating the \n\nword cloud. \n\n\n\n \n9 \n\n \n\nIn the main program we  read out the articles from a CSV file and store them in a dataframe. Then we \n\nuse the aforementioned text preprocessing methods to clean up  text.  Before we use TF-IDF to \n\ntransform the text, we first make word clouds of both the titles and contents of the articles, this \n\nallows us to get a feel of the subjects of the articles. \n\nAfterwards we remove stop words from the text, and give more weight to important words in a \n\ndocument with TF-IDF. Then we create a kind of library where we give each unique word an ID, which \n\nis used to create corpuses for the documents.  \n\nThen to create the model we pass in the corpus, the library of words with ID\u2019s, and the amount of \n\ntopics we want to extract.  \n\nLastly we use pyLDAvis  to visualize our model,  this is a interactive html file where the topic are \n\nclickable and are described by their most defining terms.\n\n\n\n \n10 \n\n \n\n\n\n \n11 \n\nDiscussion \nThis research could be improved by using a more fitting dataset for the POC. Now a dataset of \n\nprofessionally written news articles was used, however for the use case the documents will be \n\nwritten by students with less experience in writing. This could impact the performance of the model. \n\n \n\nConclusion \nTo conclude this research it can be stated with reasonable certainty that Topic Analysis would indeed \n\nbe a good fit for connecting students based on their assignments. Namely Topic Modeling, since it \n\nwould not be known beforehand to what category the assignment would belong to.  \n\nThen with Topic Modeling a model could be trained with a collection of assignments, then when a \n\nstudent creates a new assignment the model can be used to assign this new assignment to a topic. \n\nThe last step would be to then extract some of these similar other assignments with more details, \n\ne.g. student name and score.  \n\nWith this information the student with their freshly made assignment can now contact other \n\nstudents that already have made a similar assignment. \n\n\n\n \n12 \n\nSources \n \n\nTopic Analysis: a comprehensive guide to detecting topics from text. (z.d.). MonkeyLearn. \n\nhttps://monkeylearn.com/topic-\n\nanalysis/#:%7E:text=Topic%20analysis%20is%20a%20Natural,of%20unstructured%20\n\ntext%20every%20day. \n\nWikipedia contributors. (2021, 31 januari). Latent semantic analysis. Wikipedia. \n\nhttps://en.wikipedia.org/wiki/Latent_semantic_analysis \n\nWikipedia contributors. (2021b, maart 13). Latent Dirichlet allocation. Wikipedia. \n\nhttps://en.wikipedia.org/wiki/Latent_Dirichlet_allocation \n\nDwivedi, P. (2019, 27 maart). NLP: Extracting the main topics from your dataset using LDA in \n\nminutes. Medium. https://towardsdatascience.com/nlp-extracting-the-main-topics-\n\nfrom-your-dataset-using-lda-in-minutes-21486f5aa925 \n\n \n\nhttps://monkeylearn.com/topic-analysis/#:%7E:text=Topic%20analysis%20is%20a%20Natural,of%20unstructured%20text%20every%20day\nhttps://monkeylearn.com/topic-analysis/#:%7E:text=Topic%20analysis%20is%20a%20Natural,of%20unstructured%20text%20every%20day\nhttps://monkeylearn.com/topic-analysis/#:%7E:text=Topic%20analysis%20is%20a%20Natural,of%20unstructured%20text%20every%20day\nhttps://en.wikipedia.org/wiki/Latent_semantic_analysis\nhttps://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\nhttps://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925\nhttps://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925\n\n\tDependency matrix\n\tIntroduction\n\tResearch question\n\tResearch-methods\n\tWhat is Topic Analysis?\n\tApplications for Topic Analysis\n\tAdvantages and disadvantages\n\tTopic Modeling vs Topic Classification\n\n\tHow does Topic Analysis work?\n\tTopic modeling\n\tLatent Semantic Analysis (LSA)\n\tLatent Dirichlet Allocation (LDA)\n\n\tTopic classification\n\tRule-based system\n\tMachine learning system\n\n\n\tIs Topic Analysis the right fit for the context?\n\tDiscussion\n\tConclusion\n\tSources\n\n"
            }]
        },
        "CVStageMaxvanHattum": {
            "hand-ins": [{
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCV - Max van Hattum \n \n\nPersonalia \nIk ben Max van Hattum, vierentwintig jaar oud en studeer HBO ICT te \n\nFontys. Ik ben geboren en getogen in Tilburg, waar ik ook nog steeds \n\nwoon in een studiootje. Voordat ik aan het Fontys ging studeren, heb ik \n\nhet VWO afgerond en heb ik eerst drie jaar biomedische technologie \n\ngestudeerd aan het TU/e. \n\nNaast mijn studies ben ik een sociale kerel die graag met zijn vrienden \n\nafspreekt of uitjes onderneemt. Verder hou ik enorm van lezen, hier en \n\ndaar wat krachttraining en gamen (met vrienden).  \n\nIn professioneel verband zou ik mij willen beschrijven als iemand die \n\neffici\u00ebnt en doelgericht naar oplossingen zoekt voor een gegeven \n\nprobleem. Ik ben niet vies van een uitdaging, dit blijkt ook wel uit het feit \n\ndat ik deelneem aan het FHICT excellentietraject; Delta. Bij meerdere \n\nprojecten in dit programma ben ik ook projectleider.  \n\n \n\nWerk- en projectervaring \nHier en daar heb ik al ervaring op verscheidene vlakken opgedaan. In \n\nmijn jeugd heb ik als trainer gewerkt bij HC Tilburg, verder heb ik toen ook stage gelopen bij De \n\nBibliotheek Midden-Brabant. \n\nTijdens mij studie Biomedische technologie heb ik op vrijwilligers basis de website van \n\nVrouwenkamerkoor Cantilare opgezet en beheerd.  \n\nNu ben ik ICT & Open Learning aan het studeren met de focus op Software engineering. Ik heb de \n\nmogelijkheid gekregen om aan veel interessante projecten te werken, met name door mijn deelname \n\naan het excellentietraject Delta. \n\n \n\nHotspot App Strijp-t \nVoor het Delta traject ben ik projectleider bij het Hotspot app project voor Strijp-T. Deze applicatie \n\nmoet op een kaart verscheidene Hotspot weergeven, die informatie bevat over bijvoorbeeld \n\naanwezige bedrijven, interviews die hier gedaan zijn en andere gelegenheden. Bij dit project ben ik \n\nvanaf het begin betrokken geweest, de concepting fase tot nu de realisatie waar we iteratief mee \n\nbezig zijn.  \n\nBinnen dit project ben ik bezig geweest met een REST API in ASP.NET Core die migrations gebruikt \n\nom de database up to date te houden, het opzetten van de Vue.js front-end, het realiseren van het \n\ngoogle maps component, het opzetten van de CI/CD en het deployen van de applicatie met behulp \n\nvan Docker.  \n\n  \n\n\n\nQuantified Student \nQuantified student is een project wat door het verzamelen en analyseren van data studenten in staat \n\nwil stellen inzicht te krijgen in hun leerproces.  \n\nOp het moment ben ik projectleider van een groep die binnen dit mandaat onderzoek doet naar \n\nNatural Language Processing en Machine Learning en hoe dit toegepast kan worden voor Quantified \n\nstudent. Onderwerpen zoals Topic Analyse, Sentiment Analyse en het analyseren van \n\ntaalvaardigheidsniveaus komen hier bij kijken. \n\n \n\nDigital Excellence \nBinnen het Delta traject werk ik ook aan een project genaamd Digital Excellence. Dit is een platform \n\ndat mensen en projecten met elkaar moet verbinden. Dit project is opgezet in C# met ASP.NET Core.  \n\nBinnen dit project heb ik een message broker (RabbitMQ) opgezet om communicatie tussen \n\nmicroservices te faciliteren, verder heb ik een ElasticSearch stack opgezet die gebruikt wordt voor \n\neen project-recommendation system en voor het verbeteren van de zoekfunctionaliteiten.  \n\nOok heb ik binnen dit project te maken met de software architectuur en het deployen door gebruik \n\nvan Docker.  \n\n \n\nPSV Zwemclub Applicatie \nDe PSV Zwemclub had het probleem dat het een erg ingewikkeld proces was voor zwemmers om zich \n\nte kunnen registreren voor wedstrijden. Dit werd allemaal met de hand gedaan en bijgehouden door \n\nde administratie. De opdracht was om dit te vergemakkelijken, maar wel binnen vooraf \n\ngedefinieerde beperkingen. Er is namelijk al een internationaal data model genaamd Lenex wat \n\ngebruikt moest worden. \n\nVoor dit project heb ik een REST API met Java geschreven met behulp van Spring boot. Gerealiseerde \n\nfunctionaliteiten waren het inschrijven op wedstrijden, het beheren van deze inschrijvingen door \n\ncoaches en management en het importeren van wedstrijden vanuit Lenex files. \n\n \n\nMagazijn applicatie DualInventive \nVoor het bedrijf DualInventive heb ik in groepsverband een applicatie gerealiseerd die het reserveren \n\nvan, uitgifte van en teruggave van producten deed ondersteunen. Functionaliteiten zoals het \n\nbijhouden van logs van deze handelingen, exporteren van logs naar pdf formaat en het \n\nterplekke/online zetten van handtekeningen waren gerealiseerd. \n\n\n"
            }]
        },
        "Deployingtheapplication": {
            "hand-ins": [{
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStrijp-T  \u2013  Deployment \nMax van Hattum \u2013 Delta \u2013 23-3-2021 \n\nV0.2 \n\n \n\n  \n\n\n\nVersion control \nVersion Author Date Adjustments \nV0.1 Max van Hattum  Initial document setup \nV0.2 Max van Hattum  Introductie \n\ntoegevoegd het \nbedoelde publiek van \nhet document en doel \nvan het document. \nVerduidelijking bij \nCloud Service Provider \nwat de eisen precies \nzijn. \nUitbreiding \nDomeinnaam. \nOnder SSL het proces \nvan een certificaat \nbemachtigen \nbeschreven. \nInfrastructuur design \nnaar voren gehaald \nvoor beter beeld lezer. \nDocker CI/CD \nopmerking toegelicht. \nOnder Docker \ncredentials toegelicht. \n \n\n \n\n  \n\n\n\nInhoud \nVersion control ........................................................................................................................................ 2 \n\nIntroductie ............................................................................................................................................... 4 \n\nCloud Service Provider ............................................................................................................................ 4 \n\nAWS ..................................................................................................................................................... 4 \n\nHeroku ................................................................................................................................................. 4 \n\nGoogle Cloud ....................................................................................................................................... 5 \n\nDigitalocean ......................................................................................................................................... 5 \n\nKeuzes productie en test server .............................................................................................................. 5 \n\nDomeinnaam ........................................................................................................................................... 5 \n\nSSL............................................................................................................................................................ 6 \n\nInfrastructuur .......................................................................................................................................... 7 \n\nDocker ..................................................................................................................................................... 8 \n\n.Env ...................................................................................................................................................... 8 \n\nDocker-compose.yml ........................................................................................................................... 9 \n\nApache ................................................................................................................................................... 11 \n\nGoogle Maps Pricing .............................................................................................................................. 12 \n\nConclusie ............................................................................................................................................... 13 \n\n \n\n \n\n  \n\n\n\nIntroductie \nVoor Strijp-T was de opdracht om met een concept te komen die de kernwaarden van Strijp-T zou \n\ntentoonstellen. Na verscheidene concepten gepitcht te hebben werd het Hotspot App idee \n\nuitgekozen om te gaan realiseren. \n\nDeze app laat verscheidene interessante bezienswaardigheden, foto\u2019s van vroeger, interviews en \n\nbedrijven weergeven op een kaart, samengebundeld in Hotspots.  \n\nOm de app te realiseren is ervoor gekozen om de PWA richtlijnen te volgen en de user interface te \n\nbouwen met behulp van het Vue.js framework. Deze is dan gekoppeld aan een REST API, gebouwd \n\nmet ASP.NET Core en een MySql database. \n\nDit document is bedoelt voor het weergeven van mijn werkzaamheden, als bewijs voor mijn \n\ncompetenties. Daarnaast kan het gebruikt worden door andere studenten om meer inzicht te krijgen \n\nin het proces achter het deployen van de Strijp-T applicatie.  \n\nIn dit document worden de stappen beschreven die gezet moeten worden om de app te deployen \n\nnaar een test omgeving en naar een productie omgeving. Verder wordt er onderbouwt waarom \n\nverscheidene keuzes gemaakt zijn.  \n\nHet release proces wordt nog apart gepubliceerd in een los document, om op te leveren aan de \n\nstakeholder. \n\n \n\nCloud Service Provider \nVoor de test omgeving moet er mogelijkheid tot schaling zijn, de belangrijkste factor is dan de \n\nbandbreedte. Dit omdat er relatief weinig CPU en RAM intensieve processen plaatsvinden, maar er \n\nwel een mogelijkheid bestaat dat er een influx van gebruikers kan komen.  \n\nDe test server moet daarnaast gratis zijn, of gratis door studenten te gebruiken zijn. Tot slot moet er \n\nsprake zijn van gebruiksvriendelijkheid, zoals goede documentatie en eventueel handleidingen voor \n\nuse cases.  Voor de release server heeft performance een hogere prioriteit, verder is schaalbaarheid \n\nook van belang, ook met name weer bandbreedte. \n\nEr zijn verscheidene Cloud serviceproviders die gebruikt kunnen worden om apps te deployen. AWS, \n\nGoogle Cloud, Heroku, Azure en Digitalocean worden het meest genoemd.  \n\nAWS \nAWS wordt genoemd als de premium oplossing voor hosting van een enorm aanbod aan \n\nverschillende services, performance en schaalbaarheid. Dit is wel terug te zien in de prijs, ook \n\nworden als nadelen genoemd de User Interface en slechte gebruiksvriendelijkheid. Een kanttekening \n\nis wel dat het eerste jaar van AWS gratis is, plus dat er gebruik gemaakt wordt van een Pay-as-you-go \n\nmodel, waarbij je alleen betaald voor wat je gebruikt en er geen vast abonnementen zijn. \n\nHeroku \nHeroku is een service die gehost is op AWS, wat betekent dat je dezelfde stabiliteit en veiligheid kan \n\nverwachten. Het verschil is dat alles al voor je geconfigureerd is. Voor kleinschaligere apps is Heroku \n\neen goede keuze, de kosten van Heroku beginnen op 0 euro voor hobby projecten, waar een \n\nabonnement geschikt voor productie op 25 dollar per maand staat. \n\n\n\nGoogle Cloud \nGoogle Cloud is op dit moment minder uitgebreid qua aanbod van services dan AWS, maar is goed \n\naan het opkomen. Je begint met 300 dollar aan gratis credit, en veel services zijn gratis binnen een \n\nbepaald limiet op maand basis. Je gebruikt dezelfde infrastructure als Google zelf voor populaire \n\ndiensten gebruikt. \n\nDigitalocean \nDigitalocean is een provider die veel al geconfigureerde services aanbiedt. Ze focussen erg op \n\ngebruiksvriendelijkheid, zijn goedkoop en schaling is makkelijk toe te passen.  \n\nHier tegenover staat wel dat de configuratie minder flexibel is, je kan bijvoorbeeld alleen Linux based \n\nsystemen opzetten. Daarnaast profileren zij zich als een Infrastructure as a Service en niet een \n\nPlatform as a Service. Dit houdt in principe in dat zij verwachten dat de gebruiker alles zelf beheert. \n\nEen Platform as a Service zorgt ervoor dat de gebruiker alleen zich zorgen hoeft te maken over de \n\napplicatie en de data. \n\n \n\nKeuzes productie en test server \nAangezien de Strijp-t Hotspot app kleinschalig is, is de prioriteit voor zowel de test- als \n\nproductieserver dat het goedkoop is. Verder is de mogelijkheid om de brandbreedte omhoog te \n\nschalen een must voor de release server.  \n\nAangezien ik persoonlijk Infrastructuur KPI\u2019s wil aantonen, is het ook een vereiste dat de service een \n\nzogenaamde: \u201cInfrastructure as a Service\u201d is.  \n\nMet deze factors is de keuze voor Digitalocean een duidelijke voor in ieder geval de testserver, hier \n\nkomt boven op dat er een studenten aanbod is voor 100 euro aan gratis credit.  \n\nAangezien Delta niet aan het beheer van applicatie doet wordt er geen keuze gemaakt voor een \n\nproductieserver die langere tijd in de lucht zal zijn. Het advies is om de code van applicatie over te \n\ndragen aan het bedrijf dat ook de website van Strijp-T beheert. \n\n \n\nDomeinnaam \nVoor de testserver wordt een persoonlijk domeinnaam gebruikt, om kosten en registratielasten te \n\nbesparen.  \n\nOns advies voor de productieserver is om deze te koppelen aan een sub domein van het huidige \n\nStrijp-T.nl domeinnaam; e.g. app.Strijp-T.nl. Zo valt er mee te liften op de naamsbekendheid die Strijp \n\nal heeft, en kan het al bestaande SSL certificaat worden, mits deze voor sub-domeinen werkt.  \n\nAls alternatief zou het ook mogelijk zijn om een nieuw domein aan te schaffen, het nadeel hiervan is \n\ndat je dan dus niet mee kan liften op de naamsbekendheid van Strijp-T en er altijd een nieuw SSL \n\ncertificaat aangeschaft moet worden.. \n\n \n\n  \n\n\n\nSSL \nEen SSL-certificaat kan ook niet meer weg in de huidige tijd, daarom is het advies om de webserver \n\ndie de applicatie ter beschikking gaat stellen ook te configureren met een SSL-certificaat.  \n\nOm een SSL certificaat te bemachtigen is er bij Namecheap, met een studenten account, een gratis \n\naangeschaft. Om een certificaat request met private key en certificaat te genereren was de volgende \n\ntool gebruikt: https://decoder.link/csr_generator , na invullen van de domeinnaam en locatie \n\ngenereert deze alles voor je. \n\n \n\nLet op, noteer de private key en sla deze op in onderstaand formaat met de \u201c.key\u201d extensie , deze is \n\nnodig om het certificaat op de server te installeren. \n\n \n\n  --- Rest van de key --- \n\n \n\nVervolgens kon dit request gebruikt worden om het certificaat te activeren bij de leverancier waar \n\nhet SSL certificaat is aangeschaft. Na activeren leverde de leverancier de volgende drie files. \n\nhttps://decoder.link/csr_generator\n\n\n \n\nLees verder op in dit document onder kopje: Apache voor een voorbeeld van hoe dit certificaat \n\nvervolgens te installeren. \n\nInfrastructuur \nOnderstaand het design van de infrastructuur zoals hij opgezet gaat worden. Met Docker zijn twee \n\nnetwerken ge\u00efnstantieerd om de front-end en back-end gescheiden te houden. Verder is het front-\n\nend naar de localhost blootgesteld op poort 8080, en de REST API op poort 5000, de database wordt \n\nniet blootgesteld aan de localhost, deze mag alleen via de API benaderd worden. \n\nVervolgens route de reverse proxy requests op port 443 naar de desbetreffende services, als de URL \n\nde prefix \u201c/backend\u201d bevat naar de API, anders naar de front-end app. Bij deze reverse proxy is het \n\nSSL certificaat ge\u00efnstalleerd, die gebruikt wordt bij port 443 (https) \n\n \n\n \n\n\n\nDocker \nVoor development werd er al een docker-compose.yml bestand gebruikt om de back-end op te \n\nstarten, de api en database.  \n\nVoor productie wordt weer opnieuw gekozen om Docker te gebruiken. De voordelen hiervan zijn dat \n\nna eenmalige configuratie, de applicatie gemakkelijk en snel gedeployed kan worden als Docker \n\nge\u00efnstalleerd is. Verder is het ook makkelijker om later de applicatie op te schalen, bijvoorbeeld door \n\ngebruik van Docker Swarm.  \n\nDaarnaast wordt bij de back-end repository Docker al gebruikt bij de CI/CD pipeline, maar nu zou het \n\nook simpel zijn om dit op te zetten voor de front-end. Daarnaast kan in een later stadia de pipeline \n\naangepast worden om de gebouwde images te publiceren naar een private image repository en om \n\ndan direct de server met deze nieuwe versie up te daten. \n\nNadelen zijn dat er een stuk overhead is, Docker moet ge\u00efnstalleerd zijn en de containers moeten \n\nopgezet worden. \n\nDe configuratie van de applicatie wordt in een docker-compose.yml file gedaan, met behulp van \n\nenvironment variables die in een .env bestand staan. De code van de front- en back-end is zo \n\naangepast dat op basis van deze waardes de applicaties goed geconfigureerd worden. \n\n.Env \n\n \n\n\n\nDocker-compose.yml \n\n  \n\nZoals te zien schakelen we SSL uit in de REST API, dit omdat we SSL gaan configureren vanuit de \n\nwebserver die deze dockerapplicatie blootstelt naar de buitenwereld. Ook wordt er aangegeven dat \n\nCORS geconfigureerd moet worden, zodat het alleen werkt vanuit de front-end url. We stellen de API \n\nbeschikbaar lokaal op port 5000.  \n\nDe database is geconfigureerd om bij het voor het eerst op starten een nieuwe gebruiker aan te \n\nmaken speciaal voor de REST API. De username en password voor deze user worden gehaald uit \n\ngeheime environment variabelen. Deze environment variabelen worden ook gebruikt bij de API om \n\nde connectie met de database te maken.  \n\nOok binden we database data aan een lokaal volume, zodat deze behouden wordt ook als de docker \n\ncontainer offline gehaald wordt. Deze stellen we niet open om toegankelijk te zijn buiten het docker \n\nmysql-network dat geconfigureerd is. Zo kan de data alleen opgehaald worden door de REST API.  \n\nTot slot wordt de front-end opgestart en exposen we deze service op port 8080. \n\nOm dit draaiende te krijgen op een VPS moeten de code en Docker files als volgt op de server staan: \n\n\n\n \n\nWaarbij respectievelijk in de back-end en front-end de code van de master branches staat. \n\nHierna moeten  de commands \u201cdocker-compose build . \u201d en vervolgens \u201cdocker-compose up\u201d \n\nuitgevoerd worden. \n\nDit proces valt te verbeteren door het bouwen op een dedicated service te doen, waarna de images \n\ngepubliceerd worden naar een Container Registry. Deze zouden dan gespecificeerd kunnen worden \n\nin de docker-compose file, in plaats van ook het bouwen te doen in de docker-compose file. \n\nNu zijn de services lokaal draaiende op de VPS, de vervolg stap is om dit nu beschikbaar te stellen via \n\nhet gekozen domeinnaam. \n\n \n\n \n\n\n\nApache \nOm de services beschikbaar te stellen moet er \n\neen webserver gebruikt worden om een \n\nreverse proxy op te stellen.  \n\nApache is een veelgebruikt webserver en hier \n\nis veel documentatie van beschikbaar, vandaar \n\ndat hier de keuze op valt. \n\nVoor de huidige context gaat Apache gebruikt \n\nworden om de verbinding te encrypten met \n\nSSL en om requests op port 80 vanuit het \n\ndomeinnaam \u201cwww.maxvanhattum.me\u201d, \n\n\u201cmaxvanhattum.me\u201d en op port 443 vanuit \n\n\u201cmaxvanhattum.me\u201d te redirecten naar \n\n\u201chttps://www.maxvanhattum.me\u201d.  \n\nVervolgens worden requests op port 443 naar \n\n\u201cwww.maxvanhattum.me\u201d op basis van de \n\nURL geroute naar de bijbehorende service. Op \n\nde root wordt de front-end aangeboden, \n\nverder is de swagger documentatie te vinden \n\nop \n\n\u201cwww.maxvanhattum.me/backend/swagger/\u201d \n\nen de api aanspreekbaar op \n\n\u201cwww.maxvanhattum.me/api/\u201d . \n\n \n\n \n\n  \n\n\n\nGoogle Maps Pricing \nDe Google Maps gerelateerde services vallen onder een regeling waarbij een account begint met \n\n300$ aan credit uit te geven over de eerste 90 dagen en daarna maandelijks 200$ credit \n\n(https://developers.google.com/maps/billing-credits). \n\nDe services die gebruikt worden (of gaan gebruikt worden) zijn de Google Maps API, Geolocation, \n\nGeocoding en de Directions API. \n\nGoogle Maps API \n\n \n\n \n\nGeocoding \n\n \n\n \n\nDirections \n\n \n\n \n\n  \n\nhttps://developers.google.com/maps/billing-credits\n\n\nConclusie \nEr is gedemonstreerd dat de applicatie prima naar een test server gedeployed kan worden. Voor \n\nproductie kunnen dezelfde stappen genomen worden, alleen wel meer met het oog op prestatie en \n\nschaling. Het probleem is wel dat Delta niet aan beheer van applicaties doet, dus deployen naar \n\nproductie is lastig. Een oplossing zou zijn om het over te dragen aan het bedrijf dat ook de site van \n\nStrijp-T beheert. Een andere optie is om het te deployen met een account op naam van Strijp-T en de \n\nkosten terug te vragen bij Strijp-T, het nadeel hiervan is, is dat er geen langdurige ondersteuning zal \n\nzijn. \n\nVerder is er nog ruimte voor verbetering van dit proces, met name door CI/CD te implementeren en \n\ndoor een technologie te gebruiken die downtime vermijdt (Docker Swarm/Kubernetes).  \n\nVerder zouden de front-end en back-end ook op aparte server de deployed kunnen worden, waarbij \n\ner meer wordt gelet op wat de specifieke applicatie nodig heeft. De front-end zou bijvoorbeeld meer \n\nbandwidth nodig hebben, terwijl de api eerder betere processorkracht kan gebruiken. \n\nDe applicaties zijn al wel gemaakt met oog op deze uitbreidingen, doordat de applicaties gescheiden \n\nzijn en geconfigureerd kunnen worden doormiddel van environment variabelen. \n\n\n"
            }]
        },
        "Deployment": {
            "hand-ins": [{
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeploying with Docker \nMax van Hattum \n\n \n\nIntroduction \nFor Open Learning we get the opportunity to work for an external stakeholder, giving context to our \n\nlearning process. I have chosen to work on an application for PSV Swimming. The main features this \n\napp needs to offer are enabling swimmers to manage their event registrations, coaches to review \n\nthis registrations and administrators to see an overview of these registrations so they can process \n\nthem. \n\nNow the REST API must be deployed, research has been done for this, the advice is to use Docker. \n\n \n\nConfiguring Docker \nSince we need a database for the application to function, we need to bundle the API with a database. \n\nFirst, we create a Dockerfile to build a Docker image for the REST API. \n\n \n\nBasically, we build further upon the open source jdk11 image, copy the jar we built with maven of \n\nour API and then configure the starting point of the image. \n\nNow to  bundle it with the database, we create a docker-compose.yml file. Detailing how to \n\ncontainers should be bundled and how they should connect. \n\n \n\n\n\nNow this file might be a bit more complicated. \n\nWe start with determining which version we \n\nuse, 3 is the most recent. Then we define which \n\nservices need to run. \n\nWe define a mysql-database service, detailing \n\nwhich image it needs to use. \n\nThen, to configure the database we pass in \n\nenvironment variables. \n\nThen we configure a volume, where it should \n\nsave its data persistent (outside of the docker \n\ncontainer), so that we can keep the data \n\nthrough restarts.  \n\nLastly, we configure a network in the container \n\nit should use. \n\nNow we can go on to define the rest-api service.  \n\nWe let it restart on failure, since it depends on \n\nthe database service, but if the API start before \n\nthe database it will crash. \n\nThen we define the build conditions, setting the \n\ncontext to the current folder and pointing to \n\nthe Dockerfile we create earlier.  \n\nHere also we have defined environment \n\nvariables, so that it sets these values correctly \n\nfor use within Docker. These are used in the \n\napplication-docker.properties file in the API. \n\nThen we connect this service to the same network as the database, so they can communicate.  \n\nLastly, we expose the port so that the service can ben accessed from outside of the Docker container. \n\nNow that the services are defined, we are left with configuring the network and volume we use. This \n\nis quite simple as you can see. \n\n \n\nDynamic configuration \nSpring conveniently reads out an environment variable called: SPRING_PROFILES_ACTIVE. We can set \n\nthis and then it will use the set profile. See the naming of the files: \n\nNow in the regular development properties we hardcode \n\nthe values, but in the docker properties we retrieve the \n\nvalues from the environment variables.  \n\n  \n\n\n\nStarting the application \nNow starting the application is quite easy. Docker needs to be installed but then it is just a matter of \n\nrunning the following command in the root folder of the repository: \n\nDocker-compose up \n\nNow it will build the services and get them up and running, as seen below, the service is exposed on \n\nthe 5000 port. \n\n \n\n\n"
            }]
        },
        "DeploymentResearchandAdvice": {
            "hand-ins": [{
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\nPSV Swimming App Deployment \nFontys Open Learning Semester 3 \n\nMax van Hattum\n\n\n\n \n2 \n\nInhoud \nIntroduction ............................................................................................................................................. 3 \n\nDOT Framework approach ...................................................................................................................... 3 \n\nWhat .................................................................................................................................................... 3 \n\nHow ..................................................................................................................................................... 3 \n\nWhy ..................................................................................................................................................... 3 \n\nResearch Question .................................................................................................................................. 3 \n\nMain Question ..................................................................................................................................... 3 \n\nSub questions ...................................................................................................................................... 3 \n\nWhat are the details of the stakeholder\u2019s server? .................................................................................. 4 \n\nWhat services are already running on the server? ................................................................................. 4 \n\nHow are these services configured? ....................................................................................................... 4 \n\nWhat are ways of deploying the application? ......................................................................................... 4 \n\nConclusion ............................................................................................................................................... 5 \n\n \n\n  \n\n\n\n \n3 \n\nIntroduction \nFor Open Learning we get the opportunity to work for an external stakeholder, giving context to our \n\nlearning process. I have chosen to work on an application for PSV Swimming. The main features this \n\napp needs to offer are enabling swimmers to manage their event registrations, coaches to review \n\nthis registrations and administrators to see an overview of these registrations so they can process \n\nthem. \n\nAt the PSV Swimming club, they manage their own server and domain name. The task is to research \n\nhow the application could be best deployed. This contains the REST API, MySQL Database and the \n\nPWA. \n\n \n\nDOT Framework approach \n\nWhat \nThis research will mostly take place in the application domain. The current system needs to be \n\nresearched and understood. Afterwards techniques and methods on how to deploy an application \n\ncan be researched, touching on the Available Work domain. \n\nHow \nField research will be done to investigate the current situation. Key factors are where the server is \n\nhosted, what OS is running on it, what else is already running on it and how these services are \n\nconfigured. Afterwards some Library research will be done to investigate what methods on deploying \n\nare available and which are applicable for the current context. \n\nWhy \nWe focus heavily on the Field approach because we want to achieve the best fit possible for the \n\nsituation. However, since research will be done in the available work domain, it will also touch on \n\nExpertise. These two will provide the balance that is needed for a good and complete result.  \n\n \n\nResearch Question \n\nMain Question \nTo give guidance to this research we formulate a main question: \n\n\u201cHow should the application be deployed on the stakeholder their server?\u201d \n\nFor this question to be answered, multiple facets need to be researched. First, we need to investigate \n\nthe stakeholder\u2019s server, what services are already running on this server and how these are \n\nconfigured. Afterwards we need to research ways of deploying the application. Then we can answer \n\nthe main question. \n\nSub questions \n- What are the details of the stakeholder\u2019s server? \n\n- What services are already running on the server? \n\n- How are these services configured? \n\n- What are ways of deploying the application? \n\n\n\n \n4 \n\n \n\nWhat are the details of the stakeholder\u2019s server? \nThe server is a Virtual Private Server, hosted externally. This server is running a Linux distro: CentOS. \n\nThe address of the server is: 185.107.213.193 and there is an account available for Fontys Students. \n\nNow for security purposes the details will not be shared in this document.  \n\nThe server can be approach by SSH, the stakeholder was using PuTTY.  \n\n \n\nWhat services are already running on the server? \nCurrently the server hosts their website and a filesharing/cloud service. There is an Apache \n\nWebserver present, and a MariaDB service which contains a database for the filesharing/cloud \n\nservice. There are also remnants available of ghost projects, which are not exposed to the public. \n\n \n\nHow are these services configured? \nThe stakeholder has a domain name, and this is configured to point to the IP with certain \n\nports/prefixes/suffixes. Then it seemed to me that the Apache Webserver is configured with \n\nVirtualHosts to point different requests to the linked services. \n\n \n\nWhat are ways of deploying the application? \nThere are several ways on how to deploy the whole application. Lately Docker has risen in popularity \n\nsince it requires almost no configuration after the initial configuration has been done. \n\nTwo Docker files could be written, one for the PWA and one for the API. The API could then be \n\nbundled with a pre-configured MySQL database, with the help of a docker-compose.yml file.  \n\nThen these could be deployed together, again with a docker-compose.yml file or apart from each \n\nother.  \n\nThe other option is to build both separately, the API can be built with Maven, producing a JAR file. \n\nThen to setup a local database and make sure the configurations are the same as in the API. \n\nThe PWA can be built with NPM and then served with a Webserver, note that the webserver needs \n\nto have a fallback URL configured. This because of the way the routing works in Vue.js, if not you will \n\njust get 404 errors. \n\nBoth options have their merits, using Docker allows for very easy deployment and reduces the \n\ndifficulty of deploying plus the time that needs to be invested. \n\nNot using Docker and setting up everything separately means less bloat, but more time is needed for \n\ndeploying. Moreover, this is more difficult to achieve. \n\n  \n\n\n\n \n5 \n\nConclusion \nMultiple facets regarding the deployment of the software system have been touched upon now. The \n\nserver has been investigated, turning out to be an externally hosted VPS, running CentOS.  \n\nOn this server their website and a cloud service are running, these are facilitated by Apache \n\nWebserver and MariaDB.  \n\nThe domain name is configured to point to the server IP, where the Apache Webserver utilizes \n\nVirtualHosts to direct the request to the connected service. \n\nThe application could be deployed with Docker or separately. Separately would mean that everything \n\nmust be configured on its own, every time that the system needs to be deployed. \n\nUsing Docker would mean that the configuration is done once, by the software developer. \n\nAfterwards it is easier to deploy since it will just need one command to get everything up and \n\nrunning. \n\nTaking all this in mind, the advice will be to deploy the system with Docker. This will simplify the \n\ndeployment and save a lot of time. There will be a small bit of bloat, since Docker needs to be \n\ninstalled, but this is worth the save in time and the lesser difficulty of deployment. \n\n\n"
            }]
        },
        "ElasticSearchPerformanceResearch": {
            "hand-ins": [{
                "text": "ElasticSearch Performance research\n\nBy: Max van Hattum & Niray Mak\n\n\nVersion\n\n\nDistribution\n\n\n\n\n\nIntroduction\n\nAfter implementing ElasticSearch into our system and creating a real time automatic migration system we found out that the performance dropped drastically. To find out what exactly caused this performance drop we started a research.\n\nArchitecture\n\nTo get an idea of the made implementation we added a C3 model of the created changes.\n\nEvery time a create, update, or delete is executed we use a so-called TaskPublisher to publish a task which is the synchronization to the ElasticSearch dataset. After the task is published the API can continue without waiting for the task. The actual synchronizing is done by a different application which is called ElasticSynchronizer.\n\n\n\n\n\nPerformance measurements\n\nPostman integration tests time\n\nTo find the issue we tracked the difference between the integration tests on the develop branch and our branch.\n\nResults Postman tests:\nResults of 3 runs on the develop branch\nTwo runs on branch 63-recommendationsystem\n\n\nIn 63-recommendationsystem we manually checked the time in ms for each request. We often see 700-900ms.\nIn the develop branch we also manually checked the time in ms for each request. We often see 10-100ms.\n\nThis concluded that the performance definitely dropped drastically since our changes.\n\nApproach\n\nTo find what causes this enormous difference we started looking into several things:\nCPU load\nAmount of queries to database made with their performance\nRAM usage\n\nVisual Studio Performance Profiler\n\nWe used Visual Studio\u2019s Performance Profiler to track what the issue was.\nThe Performance Profiler can track CPU load, Queries that the DBContext makes and RAM usage. Beneath are the results.\n\nCPU\nFirst, we checked for CPU load. A performance analysis session from the build-in Visual studio tool was started, after start-up three iterations of the integration tests were run. From the results it can be concluded that the problem lies not with a lack of CPU power, no huge spikes were found.\n\n \nQueries\nThe performance profiler also has an option to record all queries done by the DBCONTEXT, it shows the timestamp of when the query was executed, plus the execution time.\n Some showed [Unknown] in the Duration column, however quite some did show the execution time in ms. Many of them were showing 1-7ms, only a very few were showing more than 50ms. From this it can be concluded that this is not the origin for the response delay.\n\n \n\nRAM usage\nThen the RAM usage was checked. The performance profiler shows per timestamp how much in megabytes is stored.\nIn the results below (branch 63-recommendationsystem) you can see that it starts stable on +/- 135MB. Then after one iteration of Postman integration tests it climbs up to +/- 485MB. It stays stable till the next iteration was started, then it climbs up to 730MB.\n\n\nTo make sure this is the problem the same RAM usage test was run for the development branch.\nThree iterations of integration tests we\u2019re done, as shown, here the memory does stay stable. \nIn the beginning it started stable again around 135MB. Then after the first run it increases to 245MB. During the next runs the garbage collection reduces the amount MB\u2019s used and after the 3rd run, which returns the value back to 179MB. Below are the results of three iterations of integrations tests in the develop branch.\n\n\n\nWhen comparing these values, it can be concluded that the problem lies somewhere with the RAM usage.\nThe performance profiler for the RAM usage gives additional insight in the objects that are in the memory. The biggest size is used in BufferedStreams, which are used by RabbitMQ. \nTherefore it can be concluded that RabbitMQ does something that results in the creation of, or usage of, big objects.\n\n\nTroubleshooting\nKeeping in mind that the problem originates from RabbitMQ using a large amount of RAM, changes were made in the code. \nFirst the TaskPublisher related methods were commented out, then another iteration of integration tests was done. Total time was: 295077ms. It can be concluded that thus the problem does not lie with the TaskPublisher methods..\nThen the properties TaskPublisher and ElasticSearchRestClient were removed, also the dependency injection in the constructor was removed.\nThis resulted in a better performance: Total time was: 60299ms. Average time per request went up to 100-200ms\nTo then determine if the ElasticSearchRestClient or the TaskPublisher was at fault, the ElasticSearchRestClient was added again. Results after one iteration of integration tests: 64949ms.\nFrom this it can be concluded that the dependency injection for the Taskpublisher is at fault for the slow performance.\n\n\nThe dependency injection for the TaskPublisher was checked, it was created as a scoped service. The Taskpublisher also used a RabbitMQConnectionFactory to create the connection with the RabbitMQ service. This RabbitMQConnectionFactory also was a scoped service. \nBoth were changed into a singleton. This resulted in the performance improving. One iteration of integration tests had a total time of 16055ms.\nWe checked memory again for 3 iterations of integration tests. The results are shown in the image below.\n\nConclusion\n\nWe found out that for every request a new instance of both the RabbitMQConnectionFactory and TaskPublishers were created which caused the used memory to go up. Specifically the RabbitMQConnectionFactory slowed everything down substantially.  When thinking about this afterwards this makes sense, since every request, a new connection to the RabbitMQ service was created. This was not the desired result, since the connection only has to be established once, and then can be reused. This caused the memory usage to spike, since it saved these connections and it caused the response time to go up, because it reestablished a connection every request.\n\nWe refactored both to dependencies to be a singleton. Now these are created once on start up and then reused for every request and dependency injection.See images below for the lines that were changed.\n\n\n\n\n \n"
            }]
        },
        "ImplementatieRecommendationSystem": {
            "hand-ins": [null]
        },
        "ImplementatieRESTAPI": {
            "hand-ins": [null]
        },
        "ImplementationNotificationSystem": {
            "hand-ins": [{
                "text": ["implementatie\n", "https://github.com/DigitalExcellence/dex-backend/pull/322\n", "\n", "testing:\n", "https://github.com/DigitalExcellence/dex-backend/pull/344"]
            }]
        },
        "Messagebrokerguide": {
            "hand-ins": [{
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n1 \n\nDeX  - Message Broker Guides \nMax van Hattum \u2013 11-3-2020  \n\n\n\n \n2 \n\nTable of Contents \nMessage broker RabbitMQ ..................................................................................................................... 3 \n\nRegistering a \u201cSend email\u201d task .............................................................................................................. 4 \n\nCreating new channels and services ....................................................................................................... 5 \n\nUse Case .............................................................................................................................................. 5 \n\nPublishing the task to a new channel .................................................................................................. 5 \n\nConsuming packages from a channel .................................................................................................. 7 \n\nCreating the Worker Service ............................................................................................................... 7 \n\nConfig data .......................................................................................................................................... 8 \n\nDocker ............................................................................................................................................... 12 \n\nCode to execute on receiving a package ........................................................................................... 13 \n\n \n\n  \n\n\n\n \n3 \n\nMessage broker RabbitMQ \nThe message brokers use is extremely versatile, we use it to connect our different services with each \n\nother. Mainly the API registers tasks that needs to be done, for example sending an email because a \n\nuser needs to convert their account, or for syncing our databases.  \n\nThe message broker allows us to scale services horizontally, since it can distribute tasks to multiple \n\ninstances of a service at the same time. \n\nFor new use cases it\u2019s easy to implement extra services listening to the message broker.  \n\nThe workflow is typical like so: \n\n1. An endpoint in the API is called \n\n2. One or multiple tasks which are blocking need to be executed \n\n3. The API registers tasks to a specific channel (e.g., EMAIL) \n\n4. One or more services are listening to channels  \n\na. For example, three emailSender applications \n\n5. When a channel has packages, the message broker handles distribution to connected \n\nservices \n\na. The message broker is in charge of confirming tasks are executed, and evenly \n\ndistributing these to connected services. \n\n6. A service (e.g., a ASP.NET Core Worker Service) is subscribed and listening to a channel and \n\nreceives a package \n\n7. Code is executed based on the package \n\n8. Depending on if this code succeeds or fails, it acknowledges the processing of the package \n\n \n\n  \n\n\n\n \n4 \n\nRegistering a \u201cSend email\u201d task \nWithin DeX we use a message broker, RabbitMQ, to register tasks for other services to execute. This \n\nenables the API to focus on facilitating endpoint requests, while the tasks get executed in a non-\n\nblocking way.  \n\nTo send an email the RegisterTask method from the TaskPublisher.cs class can be used. This is class is \n\npresent in the IOC, facilitating dependency injection like so: \n\n \n\nIn the constructor of the class in which you want to publish a task, set the parameter ITaskPublisher \n\ntaskPublisher and assign this to a private property, so you can use it. \n\n \n\nNow you need to make an instance of the EmailNotificationRegister.cs, passing in the recipientEmail, \n\nthe textContent. \n\nBefore you can publish this data to the message broker, you need to serialize it into JSON. Then you \n\nspecify to which channel of the message broker the task should be published to, in this case it is \n\nSubject.EMAIL. \n\n \n\nWithin the DeX architecture, another service is listening to this channel, it now gets this data and is \n\ngoing to send an email with it. \n\n \n\n  \n\n\n\n \n5 \n\nCreating new channels and services \nSay you want to execute a certain action, but either this will be blocking the API, just takes a lot of \n\nprocessing power or the action is just huge. Then it might be a good idea to separate this service \n\nfrom the API, so that the API just handles the data processing. This will also allow for horizontal \n\nscaling, just boot up some extra services listening to the same channel, the message broker will \n\nhandle the efficient distribution part. \n\n \n\nUse Case \nWe\u2019ll walk you through the steps on how to achieve this based on a simple use case. Say you want to \n\nimplement the following feature: \u201cA user can request an evaluation of all their projects using \n\nMachine Learning.\u201d  \n\nYou can imagine that processing these projects can take a lot of time, and we don\u2019t want to block the \n\nAPI with this. \n\nIf you read further down the steps to achieve this will be explained, but the general gist of how we \n\nare going to achieve this is creating two endpoints, one for the Front-End which receives the request, \n\nand one for an Analyzer Service, receiving the results. \n\nBecause this guide is focused on approaching the message broker and creating and subscribing to \n\nmessage broker channels, I\u2019m not going into depth on how to create endpoints/services withing the \n\nAPI. \n\nSo first you need to make two endpoints, the request for analysis, which is going to send the projects \n\nto analyze to a Worker Service (which we will create later), and an endpoint which the Worker \n\nService will approach to report back the results. \n\nPublishing the task to a new channel \nIn the code flow of the request endpoint, you will be going to send data to the message broker. To do \n\nthis you will first need to make a Model which is going to be data that you want to send. We will \n\nplace this class in the 09_MessageBrokerPublisher library class, in the Models folder. \n\n \n\n\n\n \n6 \n\nFor this example, we will keep it simple, do note that we will send the projects separately. Instead of \n\nsending all the projects in one data chunk, we will allow the message broker to distribute the \n\nseparate projects to services listening to the channel we will soon make. \n\nNow in the repository layer you will need to get an instance of the TaskPublisher class, we will get \n\nthis with Dependency Injection. Simple add it to the constructor of the repository. \n\nNow you are almost ready to start publishing projects to the message broker, we are however still \n\nmissing the channel to which we want to publish! This is however quite easy, just add a new ENUM \n\nto the Subject.cs Class in the MessagebrokerPublisher.cs library class. \n\n \n\nNow back to the repository layer, in the method you\u2019ve created you will need to make an instance of \n\nthe Model populated with data. This will probably be done within a loop which enumerates through \n\na list of projects to analyze. Then per project, the task needs to be sent to the channel we\u2019ve just \n\ncreated. \n\n \n\nThat\u2019s it, the first part is done! You\u2019ve just published projects to channel PROJECT_ANALYSE. Now the \n\nmessage broker is managing a queue where all the data chunks are lined up in. \n\n \n\n  \n\n\n\n \n7 \n\nConsuming packages from a channel \nNow we get to a slightly more complicated part. All the code for the configuration of consuming from \n\nthe message broker is already written, however you still need to write a new service that will parse \n\nthe payload, validate it and then executing a certain task (analyzing those projects!). Then when \n\ndone and no exception is thrown, the preconfigured code will send an acknowledgement to the \n\nmessage broker.  \n\nFor this example, we will be making a new project, a ASP.NET Core Worker Service, it will need a \n\nproject dependency on the 11_NotificationSystem and a package dependency on \n\nNetEscapades.Configuration.Validation . Moreover, this Worker Service is going to utilize \n\nenvironment variables to get credentials so that it can connect to the message broker.  We will use a \n\nDockerfile to build the application, and add it to the docker-compose.yml, where the environment \n\nvariables are going to be passed in. \n\nLet\u2019s hop in the process of creating the Worker Service and do all the tedious configuration with \n\nDocker. \n\n \n\nCreating the Worker Service \nCreate a new project in the solution and chose the Worker Service template. \n\n \n\nChoose the following settings, don\u2019t enable docker support, we\u2019ll be writing this ourselves. \n\n  \n\nYou\u2019ll end up with the following preset. \n\n\n\n \n8 \n\n \n\n \n\nConfig data \nWe\u2019ll start by creating a configuration folder with a config class and adjusting the appsettings.json \n\nfiles accordingly. This will allow the Worker Service to receive the credentials needed for connecting \n\nto the message broker, and for accessing them easily. \n\nIn the Config folder we create the following classes, we also use NuGet to install \n\nNetEscapades.Configuration.Validation. \n\n \n\n  \n\n\n\n \n9 \n\n \n\nWe use the validatable interface so that we confirm that the instance populated from either json or \n\nenvironment variables is correct, don\u2019t worry we\u2019ll get to it in time. \n\nYour folder should look like this now. \n\n \n\n  \n\n\n\n \n10 \n\nNext up is adding the values to appsettings.json and appsettings.development.json. Press the little \n\narrow next to appsettings.json to access the development variant. \n\n \n\n \n\n  \n\n\n\n \n11 \n\nGreat, now we\u2019ll do the configuration in the Program.cs! \n\n \n\nAlright a lot of extra code, basically we will be getting the environment from an environment \n\nvariable, this will make sure that if we are in development, the right appsettings.development.json is \n\nused. \n\nThen we use the Microsoft.Extension.Configuration to create an instance of the config file. It will \n\npopulate the instance with data from the appsettings or override them with data from environment \n\nvariables if these are present.  \n\nThen we add this config object scoped, so we can used dependency injection to access the data. \n\n  \n\n\n\n \n12 \n\nDocker \nNow we will make a Dockerfile so that the project can be build from the main docker-compose.yml \n\nfile, which we will adjust to add our service with configuration (including the environment variables). \n\nAlright create a Dockerfile in the root directory of our Worker Service and put in the following code. \n\n \n\nWe first specify where docker should work in, build the application in a building environment, then \n\ncopy only the necessary files for running the application and specify what the entry point should be. \n\nNow for the docker-compose.yml, we should add a new service, based on this Dockerfile, adding it to \n\nthe network of the message broker and providing the connection details with environment variables. \n\n \n\nTo quickly walk you through this, we first name our service and specify that the service should restart \n\nif it fails. This is important since if this service starts earlier than the RabbitMQ message broker, it will \n\ncrash, basically it now retries connecting until it does. Then we define how the service needs to be \n\nbuild and specify the environment variables. Lastly, we add it to the same network on which the \n\nRabbitMQ message broker runs.  \n\n  \n\n\n\n \n13 \n\nCode to execute on receiving a package \nNow we will be writing the code that will be executed when the message broker sends us a package. \n\nWe will be making a data model, which is the same as in the API. Why don\u2019t we just add a \n\ndependency on the API you ask? Well, the idea is that this service should be able to work entirely on \n\nitself, so that in the future we could maybe migrate it to another server, better tailored to the needs \n\nof this service. Then we only have to change hostname variable and now we can deploy it anywhere. \n\nMoreover, we need to make a service that implements the Interface specified in the \n\nNotificationSystem \n\n \n\nThe service needs to parse the json string to our Model class, validate if the data is correct and \n\nultimately execute the task. If anywhere in this flow an exception is thrown by our service, then now \n\nacknowledgement of completion will be sent to the message broker. This should only be done if the \n\ntask couldn\u2019t be executed because of a program specific problem, so that another instance of the \n\nservice can execute it.  \n\nFirst for the model, create a new folder Models and create the following class. \n\n \n\n\n\n \n14 \n\nNow for the service, create a folder Executors and create the following class: \n\n \n\n  \n\n\n\n \n15 \n\nWhen a package has arrived an instance of this class is going to used to call the three methods as \n\nspecified by the interface. We\u2019ll show you how this works next. When we created this Worker Service \n\nproject, the template made a Worker.cs class. We are going to adjust it like so: \n\n \n\nNotice the RabbitMQ and NotificationSystem usings. We utilize the premade methods in the \n\nNotificationSystem to quickly set up a listener that will execute the ParsePayload, ValidatePayload \n\nand ExecuteTask methods from our own ProjectAnalyzer class. \n\nFirst, we create a RabbitMQSubscriber, which we use to subscribe to our subject, remember, it\u2019s the \n\nsame one we used to publish the task. This allows us to create a listener to this channel. \n\nNow we need to configure the listener so that it uses our ProjectAnalyzer class when a package is \n\nreceived. We do this by first creating an instance of the ProjectAnalyzer class and then creating a \n\nConsumer that can be used by the listener. \n\n\n\n \n16 \n\nLastly, we invoke the StartConsumer method on the listener object, here we pass in the consumer \n\nand the subject. \n\nThat\u2019s it! Now every time a package is in the PROJECT_ANALYSE queue, our worker service gets \n\nnotified by the message broker and will process it. \n\n \n\n \n\n \n\n \n\n \n\n  \n\n\n\n \n17 \n\n \n\n \n\n\n"
            }]
        },
        "Notificationsystem": {
            "hand-ins": [{
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis notification system DEX \n\nContext \nThe Digital excellence platform needs a way to alert and notify users. Currently there is no \n\nimplementation for this feature. The system needs to be SOLID so that it would be possible to be \n\neasily reused and extended. Currently the platforms architecture is of a monolithic nature, however \n\nthere are plans to change this to a more microservice-like architecture. This needs to be kept in \n\nmind. \n\n \n\nFunctional Requirements \n- Authorized clients can create a subject which can be subscribed to \n\n- Authorized clients can create or use templates for notifications to be send \n\n- Authorized  clients can register notifications for specific subjects, sending them immediately \n\nor schedule them \n\n- Authorized clients can specify by which method the notification should be send \n\n- Users can be subscribed to subjects \n\n- Users can be unsubscribed for subjects \n\nNon-functional Requirements \n- The system should be a stand-alone service \n\n- The system needs to be able to utilize existing services like Sendgrid and Firebase Cloud \n\nMessaging \n\n \n\nProposal \nI did a bit more research in to this. I suggest just making a stand-alone notification handler, which can \n\nreceive contents and the notification type, handling the data and passing it along to another class \n\nthat handles the method of sending. \n\nIf designed correctly it would easily allow other systems to utilize the notification system. \n\nMoreover this way we could not only use it for sending emails, but maybe at a later point of time \n\nalso for real time notifications with web sockets or for push notifications. \n\nFor specific technologies, the best options in my opinion are to make it a REST API so that it can be \n\napproached from the web, or make it a service to make it more contained.  \n\nAs for the messaging by email right now, I looked into SendGrid and they allow easy dynamic \n\ntemplating. (SendGrid, z.d.) (z.d.) \n\nDedase, (2020) has created a resource that looks really extensive touching on how the architecture \n\nof such a service could look like. It's focussed on web shop notifications based on products etc but a \n\nlot of the core concepts can be used. \n\nFrom what I remember of design patterns and from research done I think we could utilize the \n\nobserver pattern in our design. Seniuk, (z.d.) has an extensive article which uses notifications as a \n\nmeans to explain this pattern. \n\nWhen searching online for existing implementations most try to indeed separate the data handler \n\nand the sending methods for obvious reasons, some examples: Codeproject, (2009) and   Josue S \n\n(z.d.). \n\n\n\nI'd like input as to what you all prefer or if there are better ways that I missed. My preference is as \n\nsummarized: a separate handler which can receive dynamic data and registers notifications based on \n\nthe observer pattern, when fired passing it along to one of the available actual notification methods \n\navailable. \n\n \n\nReferences \ncodeproject. (2009, 5 oktober). Email Notification Framework. \n\nhttps://www.codeproject.com/Articles/42843/Email-Notification-Framework \n\nDedase, A. (2020, 9 februari). Architecting a Scalable Notification Service - The Startup. Medium. \n\nhttps://medium.com/swlh/architecting-a-scalable-notification-service-778c6fb3ac28 \n\nJosue S. (z.d.). Sending Emails Automatically and periodically Using C#. Ipointsystems. Geraadpleegd \n\n1 oktober 2020, van https://www.ipointsystems.com/blog/2018/july/sending-mails-with-c \n\nS. (z.d.). sendgrid/email-templates. GitHub. Geraadpleegd 1 oktober 2020, van \n\nhttps://github.com/sendgrid/email-templates/tree/master/dynamic-templates \n\nSendGrid. (z.d.). How to send an email with Dynamic Transactional Templates. SendGrid \n\nDocumentation. Geraadpleegd 1 oktober 2020, van https://sendgrid.com/docs/ui/sending-\n\nemail/how-to-send-an-email-with-dynamic-transactional-templates/#design-a-dynamic-\n\ntransactional-template \n\nSeniuk, V. (z.d.). The Observer Design Pattern in Java. Stack Abuse. Geraadpleegd 1 oktober 2020, van \n\nhttps://stackabuse.com/the-observer-design-pattern-in-java/ \n\n \n\n\n"
            }]
        },
        "OnderzoekenadviesRealtimeSentimentAnalyse": {
            "hand-ins": [null, {
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantified Student \u2013 Real-time Sentiment \nAnalyse \nMax van Hattum\u2013 Delta \u2013 18-3-2021 \n\n\n\n \n1 \n\nTable of Contents \nIntroduction ............................................................................................................................................. 2 \n\nResearch question ................................................................................................................................... 2 \n\nResearch methods ................................................................................................................................... 2 \n\nWhat is Sentiment Analysis? ................................................................................................................... 3 \n\nTypes of sentiment analysis ................................................................................................................ 3 \n\nFine-grained sentiment analysis ...................................................................................................... 3 \n\nEmotion detection ........................................................................................................................... 3 \n\nAspect-based sentiment analysis .................................................................................................... 3 \n\nReal-time sentiment analysis. ......................................................................................................... 3 \n\nHow does Sentiment Analysis work? ...................................................................................................... 4 \n\nDrawbacks ........................................................................................................................................... 4 \n\nImplementations ................................................................................................................................. 4 \n\nIs Sentiment Analysis de best solution for the context? ......................................................................... 5 \n\nProof of Concept ................................................................................................................................. 5 \n\nConclusion and Advise ............................................................................................................................. 6 \n\nGlossary ................................................................................................................................................... 6 \n\n \n\n  \n\n\n\n \n2 \n\nIntroduction \nQuantified Student wants to enable students to get more insight in their learning process. We want \n\nto achieve this by gathering and processing all kinds of data available through the digital learning \n\nenvironment. \n\nThis should allow students to understand their learning process and adjust where deemed necessary.  \n\nOne of the concepts is to nudge a student if the text they are typing is predominantly negative or \n\npositive. Mainly while they are utilizing the Feedpulse module, which allows students to reflect on \n\ntheir day. \n\nAn example of such an implementation would be that when the student has typed multiple negative \n\nsentences in a short time, that they will get a pop-up saying: \u201cDid you really have such a bad day?\u201d. \n\nTo achieve this automatic Sentiment Analysis in real-time should be researched. \n\n \n\nResearch question \nThe main question is as follows: \n\nHow can Sentiment Analysis be used to instantly make a student aware of the \n\ntone of their text? \n\nTo be able to answer this question accurately, the question needs to be divided in several sub-\n\nquestions. It\u2019s important to first determine wat sentiment analysis is, how it works and if it fits the \n\ncontext. \n\n- What is Sentiment Analysis? \n\n- How does Sentiment Analysis work? \n\n- Is Sentiment Analysis the best fit for the context? \n\n \n\nResearch methods \nWe will be using literature-based research and the creation of a proof of concept. This will also be \n\nvalidated with the stakeholder to guarantee a right fit for the problem. Combining the three methods \n\nwill result in certainty and fit. \n\n  \n\n\n\n \n3 \n\nWhat is Sentiment Analysis? \nSentiment analysis is used for  discovering the polarity of a text, which can be defined as negative, \n\nneutral, or positive.  This could be done with a rule-based or machine learning approach (or a hybrid). \n\nThis could for example be used for quickly gaining insight into the sentiment of people towards a \n\ntweet or scanning product reviews in bulk.  \n\n \n\nTypes of sentiment analysis  \n\nFine-grained sentiment analysis  \nThis type of analysis focusses on a better, more defined assessment of the sentiment of the text in \n\nquestion. It will divide it in more categories e.g., very negative, negative, neutral, positive, and very \n\npositive. This is perfect for generating a star-based evaluation of a review.   \n\n \n\nEmotion detection  \nIt\u2019s also possible to focus on detecting emotion within a text. It might be more suitable for a problem \n\nto be quantified in types of emotion. While \u2018thrilling\u2019 and \u2018interesting\u2019 could both be construed as \n\npositive, they convene a different sentiment.   \n\nThis type of sentiment can be tricky though, since people express their emotions differently, e.g. \u2018I \n\nwould kill for that\u2019.   \n\n \n\nAspect-based sentiment analysis  \nA piece of text might be a reaction to an issue with multiple facets. You might be interested in \n\ndetermining which aspects are received negatively, and which are received positively. For this you \n\ncan use aspect-based sentiment analysis, which tries to determine multiple facets in the text and \n\nassign the sentiment accordingly  \n\n \n\nReal-time sentiment analysis.  \nIt could also be possible to monitor for example tweets or posts on a forum in real-time. You could \n\nset a threshold for when you should be notified and discover issues or trends quickly. (Monkeylearn, \n\nz.d.)  \n\nIn general rule-based solutions are faster in this than implementations based on Machine Learning: \n\nHowever, when compared to sophisticated machine learning techniques, the \n\nsimplicity of VADER carries several advantages. First, it is both quick and \n\ncomputationally economical without sacrificing accuracy. (Hutto & Gilbert, 2014, \n\np. 10) \n\n  \n\n\n\n \n4 \n\nHow does Sentiment Analysis work? \nRule-based programs generally have a lexicon with words that have a score and then use an \n\nalgorithm to determine the sentiment of a piece of text. Most of the time they don\u2019t take position or \n\ndouble negatives into account. Which can result in less accurate determinations. \n\nMost solution use machine-learning or a combination of rules and machine-learning. Most \n\nimplementations use a bag of words or bag-of-ngrams to vectorize the text.   \n\nA bag of words uses a list of predetermined words to search and quantify them from the text. A bag \n\nof n-grams search for a combination of words. Both methods spit out a vector representation of the \n\npiece of text, the vector represents which words or n-grams are present in the text. Then these \n\nvectors can be scored. (Brownlee, 2019)  \n\nTo score the vectors the vocabulary must be classified e.g., positive, neutral, negative.  \n\n \n\nDrawbacks  \nIt\u2019s hard to detect irony or sarcasm, furthermore sentiment must be placed in context. Because \n\nsentiment is subjective, it\u2019s also difficult to score words. This leads to needing to define the terms \n\nwith which the text is going to be scored.   \n\n \n\nImplementations   \nThere are multiple resources available for scoring text based on sentiment. There is Microsoft Azure \n\nText analytics, IBM Watson natural language detection, Amazon Comprehend and VaderSentiment.  \n\nInterestingly VaderSentiment is the only rule-based implementation between these noteworthy \n\nservices. However, it scores well in comparison to the above services: \n\nVADER performed as well as (and in most cases, better than) eleven other highly \n\nregarded sentiment analysis tools. (Hutto & Gilbert, 2014, p. 11) \n\nMost are extensive and complicated. There are also explanations available online on how to train \n\nyour own model (Jain, 2020)  \n\n \n\n  \n\n\n\n \n5 \n\nIs Sentiment Analysis de best solution for the context? \nFor the context, real-time sentiment analysis can be used. A rule-based implementation would be \n\npreferable since it is very fast and light weight. Feedpulse can be in different languages, however \n\nVaderSentiment also works with multiple languages, it states that it is accurate in determining \n\nsentiment even when a sentence is first translated to English and then analyzed. (C, z.d.). \n\nTaking these points in account VaderSentiment is a good fit for the context. It must be noted \n\nhowever that it is made for analyzing small pieces of text, namely focused on social media, and is \n\nprobably less accurate on larger texts.  \n\n \n\nProof of Concept \nAs a proof of concept, I\u2019ve created a simple JavaScript application that reads out a textbox. To extract \n\nsentences, it monitors usages of points (\u2018.\u2019). After every \u2018.\u2019 entry, it takes the text from the previous \n\npoint, up until the latest point. It uses Google\u2019s Compact Language detector to then determine the \n\nlanguage, if it\u2019s not English it first translates the sentence to English. \n\nThen it analyses this piece of text with the VaderSentiment JavaScript library. (V, z.d.-b). This results \n\nin four scores, the positivity, naturality and negativity.  \n\nThe final score is the compound score, it uses rules to give more weight to certain situations. For \n\nexample, in the following sentence: \u201cThe idea was great, however the execution was horrible.\u201d It \n\nregards the part after however as more important. \n\nhttps://github.com/Quantified-Student/POCs/tree/main/poc-sentiment-textbox  \n\n \n\n \n\nhttps://github.com/Quantified-Student/POCs/tree/main/poc-sentiment-textbox\n\n\n \n6 \n\nConclusion and Advise \nTo conclude this research, I can state with certainty that Sentiment Analysis could indeed be used to \n\nmake students aware of the tone of their text in real-time. Sentiment analysis is accurate and can be \n\ndone sufficiently quick, so that real-time analysis is possible. \n\nMy advice is to use a rule-based lexicon implementation, since it is light-weight giving fast results. \n\nThe most used and citated implementation is VaderSentiment, and it proves to fit for the context. \n\nUsing one of the VaderSentiment libraries, passing in a sentence, and then using the result to nudge \n\nthe student would be the desired course of action. \n\n \n\n \n\nGlossary \nBrownlee, J. (2019, 7 August). A Gentle Introduction to the Bag-of-Words Model. Machine Learning \n\nMastery. https://machinelearningmastery.com/gentle-introduction-bag-words-model/  \n\nMonkeylearn. (z.d.). Everything There Is to Know about Sentiment Analysis., van \n\nhttps://monkeylearn.com/sentiment-analysis/  \n\nJain, S. (2020, 5 June). Ultimate guide to deal with Text Data (using Python) \u2013 for Data Scientists and \n\nEngineers. Analytics Vidhya. https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-\n\ndeal-text-data-predictive-python/ \n\nHutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of \n\nSocial Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann \n\nArbor, MI, June 2014. \n\nC. (z.d.). cjhutto/vaderSentiment. GitHub. https://github.com/cjhutto/vaderSentiment#demo-\n\nincluding-example-of-non-english-text-translations \n\nV. (z.d.-b). vaderSentiment/vaderSentiment-js. GitHub. Geraadpleegd op 18 maart 2021, van \n\nhttps://github.com/vaderSentiment/vaderSentiment-js \n\n \n\n \n\nhttps://github.com/cjhutto/vaderSentiment#demo-including-example-of-non-english-text-translations\nhttps://github.com/cjhutto/vaderSentiment#demo-including-example-of-non-english-text-translations\nhttps://github.com/vaderSentiment/vaderSentiment-js\n\n"
            }]
        },
        "OnderzoekenPOCsSentimentAnalyse": {
            "hand-ins": [{
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode and results aspect discovering in longer text \nRandom review from amazon: \"For some reason these controllers from Amazon \n\njust aren't the same as the ones from wal-mart. They're lighter, the analog \n\nsticks are so loose, no stiffness at all, they just flop around, and they \n\nall die on me within a few months. There must be different manufacturers \n\ninvolved here.\" \n\n \n\nSee last result for filtered most important aspects. \n\n \n\n  \n\n\n\n \n\n \n\n  \n\n\n\n \n\n \n\n \n\n\n\n \n\n \n\n \n\n \n\n\n"
            }, {
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProofs of Concepts: Sentiment Analysis \nMax van Hattum \u2013 10-9-2020 \n\n  \n\n\n\nInhoudsopgave \nIntroduction ............................................................................................................................................. 1 \n\nAvailable Methods ................................................................................................................................... 1 \n\nGetting ready to code .............................................................................................................................. 1 \n\nCode! ....................................................................................................................................................... 2 \n\nConclusion ............................................................................................................................................... 4 \n\nGlossary ................................................................................................................................................... 4 \n\n \n\n\n\n \n1 \n\nIntroduction \nKnowing what the sentiment of a sentence or piece of text is could be of enormous value. Imagine \n\nanalysing thousands of reviews or tweets with manpower, or with an AI. Therefor I\u2019m going to \n\nexplore what the options already available for sentiment analysis are and how to utilize them. \n\n \n\nAvailable Methods \nOn a first glance when searching for natural language processing in combination with sentiment \n\nanalyses again Python shows up as most used language. Articles going in-depth about sentiment \n\nanalysis mostly show of its power by using it on Tweets. The library \u2013 trained AI \u2013 most mentioned \n\nseems to be VADER ( Valence Aware Dictionary and sEntiment Reasoner) (Brownlee, 2019). Since \n\nVADER performed especially well on tweets, it logical that this one is mentioned frequently. \n\nHowever there are more options available: NLTK (TutorialsPoint, z.d.) and SPACY both provide low \n\nlevel text analytics and provide options to custom train models or use pre-trained models. They are \n\nboth perfect for preparing text for use with for example deeplearn frameworks like: TensorFlow, \n\nPyTorch and scikit-learn (Spacy, z.d.). \n\n \n\nGetting ready to code \nTo begin with, I\u2019m going to use VaderSentiment to analyse a sentence inputted by an user. The \n\nlibrary is available with PIP and nothing else is required. \n\n \n\n\n\n \n2 \n\nCode! \n\nEnglish-only POC \n\n \n\nIt\u2019s honestly after reading the vader docs pretty straightforward. You import the sentiment analyzer \n\nand easily get sentences scored from -1 as most negative to 1 as most positive with a compound \n\nscore taking not only the word scores into account, but also grammar and punctuation.  See below \n\nfor output. \n\n \n\nAs you can see the compound score is higher, because it weighs the exclamation mark and the part \n\nafter \u2018but\u2019 heavier and multiplies the sentiment of the part of the sentence that connects to that. \n\n \n\n  \n\n\n\n \n3 \n\nAll languages POC \nTo me it feels kind of hacky, but the makers of VADER straight up recommend to just sent a sentence \n\nto a translator and then pass it to VADER. In their experience this gave accurate results. (Hutto, 2014) \n\n \n\n \n\nSee below output: \n\n \n\n \n\n \n\n \n\n \n\n\n\n \n4 \n\nConclusion \nGetting the sentiment polarity of a sentence is easily done and produces good results. For longer \n\npieces of text it would be possible to split up the text to sentence level and analyse these, however \n\nthere are more, maybe better fit, solutions for analysing bigger pieces of text. I\u2019ve found a guide \n\nexplaining how to do aspect-based sentiment analysis to get a polarity score based on \n\ncategories/subjects mentioned in the text. This use NLTK and StanfordNLP. This will be the next step \n\nin my journey to becoming a NLP master. \n\nGlossary \nBrownlee, J. (2019, 7 augustus). A Gentle Introduction to the Bag-of-Words Model. Machine Learning \n\nMastery. https://machinelearningmastery.com/gentle-introduction-bag-words-model/ \n\nTutorialsPoint. (z.d.). AI with Python \u00c3\u00a2\u00c2\u20ac\u00c2\u201c NLTK Package - Tutorialspoint. Geraadpleegd 10 \n\nseptember 2020, van \n\nhttps://www.tutorialspoint.com/artificial_intelligence_with_python/artificial_intelligence_with_pyth\n\non_nltk_package.htm \n\nSpacy. (z.d.). Models \u00b7 spaCy Models Documentation. Models. Geraadpleegd 10 september 2020, van \n\nhttps://spacy.io/models/ \n\nHutto, C. J. (2014, 17 november). cjhutto/vaderSentiment. GitHub - VaderSentiment. \n\nhttps://github.com/cjhutto/vaderSentiment#demo-including-example-of-non-english-text-\n\ntranslations \n\n \n\n \n\n \n\n \n\nhttps://www.tutorialspoint.com/artificial_intelligence_with_python/artificial_intelligence_with_python_nltk_package.htm\nhttps://www.tutorialspoint.com/artificial_intelligence_with_python/artificial_intelligence_with_python_nltk_package.htm\n\n"
            }, {
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentiment analysis\n\n\n \n\n  \n\nSENTIMENT ANALYSIS \nWHAT WHY HOW \n\nHattum,Max M.B. van \n3928780 \n\n3-9-2020 \n\nAbstract \nIn dit document geef ik een samenvatting van wat sentiment analyse is, waarom het nuttig is \n\nen hoe het gebruikt kan worden. Zowel daadwerkelijke technieken, maar ook in \nhypothetische situaties. \n\n\n\nInhoud \nWhat is sentiment analysis ...................................................................................................................... 1 \n\nTypes of sentiment analysis .................................................................................................................... 1 \n\nFine-grained sentiment analysis .......................................................................................................... 1 \n\nEmotion detection ............................................................................................................................... 1 \n\nAspect-based sentiment analysis ........................................................................................................ 1 \n\nReal-time sentiment analysis. ............................................................................................................. 1 \n\nHow to implement................................................................................................................................... 1 \n\nDrawbacks ............................................................................................................................................... 2 \n\nImplementations ..................................................................................................................................... 2 \n\nGlossary ................................................................................................................................................... 3 \n\n \n\n \n\n\n\n \n1 \n\nWhat is sentiment analysis \nSentiment analysis is used for  discovering the polarity of a text, which can be defined as negative, \n\nneutral or positive.  This could be done with a rule-based or machine learning approach (or a hybrid). \n\nThis could for example be used for quickly gaining insight into the sentiment of people towards a \n\ntweet or scanning product reviews in bulk. \n\n \n\nTypes of sentiment analysis \n\nFine-grained sentiment analysis \nThis type of analysis is focus an a better, more defined assessment of the sentiment of the text in \n\nquestion. It will divide it in more categories e.g. very negative, negative, neutral, positive and very \n\npositive. This is perfect for generating a star based evaluation of a review.  \n\n \n\nEmotion detection \nIt\u2019s also possible to focus on detecting emotion within a text. It might be more suitable for a problem \n\nto be quantified in types of emotion. While \u2018thrilling\u2019 and \u2018interesting\u2019 could both be construed as \n\npositive, they convene a different sentiment.  \n\nThis type of sentiment can be tricky though, since people express their emotions differently, e.g. \u2018I \n\nwould kill for that\u2019. \n\n \n\nAspect-based sentiment analysis \nA piece of text might be a reaction to an issue with multiple facets. You might be interested in \ndetermining which aspects are received negatively, and which are received positively. For this you \ncan use aspect-based sentiment analysis, which tries to determine multiple facets in the text and \nassign the sentiment accoridingly \n \n\nReal-time sentiment analysis. \nIt could also be possible to monitor for example tweets or posts on a forum in real-time. You could \n\nset a threshold for when you should be notified and discover issues or trends quickly. (Monkeylearn, \n\nz.d.) \n\n \n\nHow to implement \nRule-based programs could be used, but most of the time they don\u2019t take position or double \n\nnegatives into account. Most solution use machine-learning or a combination of rules and machine-\n\nlearning. Most implementations use a bag of words or bag-of-ngrams to vectorize the text.  \n\nA bag of words uses a list of predetermined words to search and quantify them from the text. A bag \n\nof n-grams search for a combination of words. Both methods spit out a vector representation of the \n\npiece of text, the vector represents which words or n-grams are present in the text. Then these \n\nvectors can be scored. (Brownlee, 2019) \n\nTo score the vectors the vocabulary must be classified e.g. positive, neutral, negative. \n\n\n\n \n2 \n\nDrawbacks \nIt\u2019s hard to detect irony or sarcasm, furthermore sentiment must be placed in context. Because \nsentiment is fairly subjective, it\u2019s also difficult to score words. This leads to needing to define the \nterms with which the text is going to be scored.  \n \n\nImplementations  \nThere are multiple resources available for scoring text based on sentiment. There is Microsoft Azure \n\nText analytics, IBM Watson natural language detection, Amazon Comprehend and VaderSentiment. \n\nMost are fairly extensive and complicated. There are also explanations available online on how to \n\ntrain your own model (Jain, 2020) \n\nThere are also pre-trained open source models available, earlier I mentioned VaderSentiment. This \n\nimplementation is surprisingly accurate for social media and smaller pieces of texts. (Hutto & Gilbert, \n\n2014, p. 1) \n\n \n\n \n\n  \n\n\n\n \n3 \n\nGlossary \n \n\nBrownlee, J. (2019, 7 augustus). A Gentle Introduction to the Bag-of-Words Model. Machine Learning \n\nMastery. https://machinelearningmastery.com/gentle-introduction-bag-words-model/ \n\nMonkeylearn. (z.d.). Everything There Is to Know about Sentiment Analysis. Geraadpleegd 3 \n\nseptember 2020, van https://monkeylearn.com/sentiment-analysis/ \n\nHutto, C. J., & Gilbert, E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of \n\nSocial Media Text. Association for the Advancement of Artificial Intelligence, 1\u201310. \n\nhttp://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf \n\nJain, S. (2020, 5 juni). Ultimate guide to deal with Text Data (using Python) \u2013 for Data Scientists and \n\nEngineers. Analytics Vidhya. https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-\n\ndeal-text-data-predictive-python/ \n\n \n\n \n\nhttp://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf\n\n"
            }]
        },
        "POCElasticSearch": {
            "hand-ins": [{
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProof of concept\u00a0\n\nElasticSearch\u00a0\n\n\u00a0 \u00a0\n\n\n\nVersion history\u00a0\n \n\n \n\nDistribution\u00a0\n \n \n\n\u00a0 \u00a0\n\nVersion Date Author(s) Changes State \n\n0.1 02-12-2020 Max van \nHattum & \nNiray Mak \n\nProof of concept for \nrecommendations \n\nIncomplete \n\n1.0 09-12-2020 Max van \nHattum & \nNiray Mak \n\nProof of concept for search \nfunction ElasticSearch \n\nComplete \n\nVersion Date To \n\n   \n\n\n\nIntroduction\u00a0\n \nMax van Hattum and Niray Mak worked together on making  proof of concepts for using \nelasticsearch with the DeX platform. We want to prove that it can be utilized for efficiently \nmaking project recommendations to users, and for improving search results.  \nTo achieve this goal we followed several steps: \n \nSteps to create the proof of concept: \n \n\n1. Setup ELK (Elastic, Logstash, Kibana) stack.  \n2. Create mock data \n3. Post mock data in ElasticSearch database \n4. Retrieve similar user \n5. Retrieve projects liked by similar user. \n6. Searching project with ElasticSearch \n\n \nAfter creating the proof of concept we also looked into how we could use the search function \nof ElasticSearch. This research can be found and the end of the document. \n\n\u00a0 \u00a0\n\n\n\n1. Setup ELK stack: \n \nTutorial on how our ELK stack was setup on our local machine can be found on the link \nbeneath: \n \nhttps://github.com/DigitalExcellence/dex-backend/wiki/ELK-stack-installation-guide \n \n\n2. Create mock data \nWe used the website \u200bhttps://www.mockaroo.com/\u200b to make mock data. We generated 500 \nrecords in the format beneath.  \n \n{\"Created\":\"03/19/2020\",\"Id\":1,\"ProjectName\":\"Zoolab\",\"Description\":\"Lorem ipsum dolor sit \namet, consectetuer adipiscing elit. Proin interdum mauris non ligula pellentesque \nultrices.\",\"Likes\":[21,45,18,34,24,28,36,45,45,47]} \n\n3. Post mock data \nTo get the data in the ElasticSearch we created a C# application which reads the .json file \nand casts this into a list of \u201cProjectResource\u201d objects which is the same format as mentioned \nin step 2. Thereafter the application posts every object into the ElasticSearch non relational \ndatabase. \n \n\n \nWe can keep in mind that we can reuse this code to synchronize our actual database with \nthe ElasticSearch database. \n\n4. Retrieve similar user \nWe wrote a query which retrieves a list of users who are similar to the user entered in the \nparameters. In our case this is hardcoded in the query and is user 10. As a top result we get \n\nhttps://github.com/DigitalExcellence/dex-backend/wiki/ELK-stack-installation-guide\nhttps://www.mockaroo.com/\n\n\nthe user itself who apparently has liked 90 projects in total. As a second result we get user \n44 who has liked 25 projects which are also liked by user 10. We can assume user 10 and \nuser 44 are \u201csimilar\u201d to each other and we will make recommendations out of that. \n \n\n \n  \n\n\n\n5. Retrieve project recommendations \nWe wrote a query which retrieves a list of projects liked by similar user 44 from step 4 but \nnot by user 10. In practice our system can make an assumption that user 10 could also be \ninterested in these projects and it will recommend these projects. \n \n\n  \n\n\n\n6.  Searching project with ElasticSearch \nIn elasticsearch the index can be configured to use multiple analyzers that process the data \nthat is being saved. It\u2019s also possible to analyse the search query.  \nThis results in more relevant search results when querying against the data. \nSpecifically we\u2019ve configured an analyzer that uses a n-gram model, which turns the query in \nbasically a sequence of characters as shown below. This analyzer is assigned to the \nprojectname field. \n \n\n \n(ElasticSearch, z.d.) \n \nWe also configured a separate analyzer for the Project Description that filters the text \nand allows for matching with synonyms. We\u2019ve chosen English as the main language \nfor the language processing, since most projects are in English. \nWe process the description on stopwords (\u201cA\u201d, \u201cBut\u201d, \u201cThe\u201d, etc.) and stem the \nwords, transforming conjugations of verbs into their base (\u201cran => run\u201d).  \n \nThe query underneath can be used to set the mapping for the document with analyzers.  \n \n{ \n\n    \u200b\"settings\"\u200b:\u200b \u200b{ \n\n        \u200b\"analysis\"\u200b:\u200b \u200b{ \n\n            \u200b\"analyzer\"\u200b:\u200b \u200b{ \n\n                \u200b\"autocomplete\"\u200b:\u200b \u200b{ \n\n                    \u200b\"tokenizer\"\u200b:\u200b \u200b\"autocomplete\"\u200b, \n\n                    \u200b\"filter\"\u200b:\u200b \u200b[ \n\n                        \u200b\"lowercase\" \n\n                    \u200b] \n\n                \u200b}, \n\n                \u200b\"autocomplete_search\"\u200b:\u200b \u200b{ \n\n                    \u200b\"tokenizer\"\u200b:\u200b \u200b\"lowercase\" \n\n                \u200b}, \n\n\n\n                \u200b\"description_index\"\u200b:\u200b \u200b{ \n\n                    \u200b\"tokenizer\"\u200b:\u200b \u200b\"lowercase\"\u200b, \n\n                    \u200b\"filter\"\u200b:\u200b \u200b[ \n\n                        \u200b\"synonym\"\u200b, \n\n                        \u200b\"english_stop\"\u200b, \n\n                        \u200b\"english_stemmer\" \n\n                    \u200b] \n\n                \u200b}, \n\n                \u200b\"description_search\"\u200b:\u200b \u200b{ \n\n                    \u200b\"tokenizer\"\u200b:\u200b \u200b\"lowercase\" \n\n                \u200b} \n\n            \u200b}, \n\n            \u200b\"filter\"\u200b:\u200b \u200b{ \n\n                \u200b\"synonym\"\u200b:\u200b \u200b{ \n\n                    \u200b\"type\"\u200b:\u200b \u200b\"synonym\"\u200b, \n\n                    \u200b\"format\"\u200b:\u200b \u200b\"wordnet\"\u200b, \n\n                    \u200b\"lenient\"\u200b:\u200b \u200btrue\u200b, \n\n                    \u200b\"synonyms_path\"\u200b:\u200b \u200b\"analysis/wn_s.txt\" \n\n                \u200b}, \n\n                \u200b\"english_stop\"\u200b:\u200b \u200b{ \n\n                    \u200b\"type\"\u200b:\u200b \u200b\"stop\"\u200b, \n\n                    \u200b\"stopwords\"\u200b:\u200b \u200b\"_english_\" \n\n                \u200b}, \n\n                \u200b\"english_stemmer\"\u200b:\u200b \u200b{ \n\n                    \u200b\"type\"\u200b:\u200b \u200b\"stemmer\"\u200b, \n\n                    \u200b\"language\"\u200b:\u200b \u200b\"english\" \n\n                \u200b} \n\n            \u200b}, \n\n            \u200b\"tokenizer\"\u200b:\u200b \u200b{ \n\n                \u200b\"autocomplete\"\u200b:\u200b \u200b{ \n\n                    \u200b\"type\"\u200b:\u200b \u200b\"edge_ngram\"\u200b, \n\n                    \u200b\"min_gram\"\u200b:\u200b \u200b2\u200b, \n\n                    \u200b\"max_gram\"\u200b:\u200b \u200b10\u200b, \n\n                    \u200b\"token_chars\"\u200b:\u200b \u200b[ \n\n                        \u200b\"letter\" \n\n                    \u200b] \n\n                \u200b} \n\n            \u200b} \n\n        \u200b} \n\n    \u200b}, \n\n    \u200b\"mappings\"\u200b:\u200b \u200b{ \n\n        \u200b\"properties\"\u200b:\u200b \u200b{ \n\n            \u200b\"Created\"\u200b:\u200b \u200b{ \n\n                \u200b\"type\"\u200b:\u200b \u200b\"date\" \n\n            \u200b}, \n\n\n\n            \u200b\"Id\"\u200b:\u200b \u200b{ \n\n                \u200b\"type\"\u200b:\u200b \u200b\"integer\" \n\n            \u200b}, \n\n            \u200b\"ProjectName\"\u200b:\u200b \u200b{ \n\n                \u200b\"type\"\u200b:\u200b \u200b\"text\"\u200b, \n\n                \u200b\"analyzer\"\u200b:\u200b \u200b\"autocomplete\"\u200b, \n\n                \u200b\"search_analyzer\"\u200b:\u200b \u200b\"autocomplete_search\" \n\n            \u200b}, \n\n            \u200b\"Description\"\u200b:\u200b \u200b{ \n\n                \u200b\"type\"\u200b:\u200b \u200b\"text\"\u200b, \n\n                \u200b\"analyzer\"\u200b:\u200b \u200b\"description_index\"\u200b, \n\n                \u200b\"search_analyzer\"\u200b:\u200b \u200b\"description_search\" \n\n            \u200b}, \n\n            \u200b\"Likes\"\u200b:\u200b \u200b{ \n\n                \u200b\"type\"\u200b:\u200b \u200b\"integer\" \n\n            \u200b} \n\n        \u200b} \n\n    \u200b} \n\n} \n\n  \n\n\n\nSynonyms \n \nThe query makes use of a file called wn_s.txt. This file is acquired from WordNet. (The \nTrustees of Princeton University, 2020) \n \nThe file exists out of 220k lines with references to synonyms. \n\n \n \n \n \n \n \n \n \n  \n\n\n\nGlossary\u00a0\n \n\nElasticSearch. (z.d.). \u200bN-Gram explained\u200b [Illustration]. Elastic. \n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/analysi\n\ns-ngram-tokenizer.html \n\nThe Trustees of Princeton University. (2020). \u200bWordNet\u200b. WordNet. \n\nhttps://wordnet.princeton.edu/ \n\n \n\n \n\n \n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-ngram-tokenizer.html\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-ngram-tokenizer.html\nhttps://wordnet.princeton.edu/\n\n"
            }]
        },
        "ResearchtranslatingdemotofunctionalApp": {
            "hand-ins": [{
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearch translating REST API demo to functional app\n\n\n \n\n  \n\nRESEARCH TRANSLATING \n\nREST API DEMO TO \n\nFUNCTIONAL APP \n      \n\nMax \n3928780 \n\n13-11-2020 \n\nOL \u2013 PSV Swimming \n\nCees van Tilborg \n\nV1 \n\nAbstract \nIn this document the current back-end is going to be analysed. Moreover this document is \n\ngoing to discuss the steps that need to be taken to translate this into a functioning back-end \nwith persistent data. Taking in mind security, scalability and maintainability.    \n\n\n\nInhoud \nIntroduction ............................................................................................................................................. 1 \n\nDOT Framework approach ...................................................................................................................... 1 \n\nWhat .................................................................................................................................................... 1 \n\nHow ..................................................................................................................................................... 1 \n\nWhy ..................................................................................................................................................... 1 \n\nResearch question ................................................................................................................................... 2 \n\nMain question ..................................................................................................................................... 2 \n\nSub questions ...................................................................................................................................... 2 \n\nWhat can they current demo already do? .............................................................................................. 2 \n\nWhat technologies does the current demo use? .................................................................................... 2 \n\nWhat needs to be added and/or change for the demo to reach functionality? ..................................... 3 \n\nHow to best implement the aforementioned additions? ....................................................................... 3 \n\nConclusion ............................................................................................................................................... 4 \n\nGlossary ................................................................................................................................................... 5 \n\n \n\n \n\n\n\n \n1 \n\nIntroduction \nFor Open Learning we get the opportunity to work for an external stakeholder, giving context to our \n\nlearning process. I\u2019ve chosen to work on an application for PSV Swimming. The main features this app \n\nneeds to offer are enabling swimmers to manage their event registrations, coaches to review this \n\nregistrations and administrators to see an overview of these registrations so they can process them. \n\nA previous Fontys group already worked on this project, leaving behind a demo existing of a PWA \n\nand REST API. This demo uses mock data, which is non persistent, to showcase how such an \n\napplication would look like. \n\nMy task is to analyse the REST API and identify what is needed to translate this to a fully functional \n\nAPI taking into account security, scalability and maintainability. \n\n \n\nDOT Framework approach \n\nWhat \nThis research will mostly take place in the application domain, since the current application needs to \n\nbe understood before it can be translated into a functional app. However, some research into the \n\navailable work domain will also be done. We take this approach because a lot of similar apps are \n\nalready made, so logically certain techniques will already be developed to solve the issues at hand. \n\n \n\nHow \nSome Field research will be done to get the know the context in which the application is going to be \n\nused. Then we will use the Library method to obtain knowledge about how to best approach the \n\nissue. Not only by doing literature research, but also by talking to experts. After this is done we move \n\non to the Lab approach, where the most work will be taking place. The reason for this is that a \n\nprevious group already did a lot of good research and documentation, however this leaves a huge \n\namount of implementation to be done. By doing presentations we will validate the application, so \n\nthe Showroom approach will also be used. \n\n \n\nWhy \nWe focus heavily on the Lab approach because we want to achieve the best fit possible for the \n\nsituation. The stakeholder expressed his wishes that he just wants something functional, even if it is \n\nbareboned. However we want the application to be secure and scalable, therefor we also do \n\nresearch to gain an overview and show our product to gain expertise.  \n\n  \n\n\n\n \n2 \n\nResearch question \n\nMain question \nTo give guidance to this research we formulate a main question: \n\n\u201cWhat work needs to be done to transform the existing demo into a functional \n\napplication?\u201d \n\nFor this question to be answered, multiple facets need to be researched. First it needs to be clear \n\nwhat the current demo can do and what technologies this uses. Then we need to do research about \n\nwhat is needed to make the demo functional. Afterwards knowledge needs to be attained about how \n\nto best implement the work that is needed for this transformation. \n\nSub questions \n- What can they current demo already do? \n\n- What technologies does the current demo use? \n\n- What needs to be added and/or changed for the demo to reach functionality? \n\n- How to best implement the aforementioned additions? \n\n \n\nWhat can they current demo already do? \nThe current demo is a REST API, where authentication is implemented using JWT. There are multiple \n\nendpoints which pertain to logic for creating users with roles, sending account creation mails, \n\nmanaging registration to events, managing trainings and creating news notifications.  \n\nThe demo does not utilize a way to persist data, meaning that the application only functions for the \n\nlifetime of the service. No database is connected, instead there are classes returning static data, \n\nwhich are retrieved and then worked with. \n\n \n\nWhat technologies does the current demo use? \nThe demo is written in Java, using the Spring Boot framework. Documentation is facilitated by \n\nSwagger. The authentication is done with Spring Boot Security using JWT. To build the application \n\nDocker is used, a Dockerfile with building instructions is present. For the account creation affirmation \n\nthe SendGrid API is used to send mails, the passwords were sent this way, however they were not \n\nencrypted. \n\nThere were a lot of unused dependencies installed, for example Liquibase, while there was no \n\ndatabase present.  \n\nAfter reading about the Spring Boot Framework I approve of the use of this framework. It is one of \n\nthe most used frameworks for building web applications and compares well to alternatives \n\n(Kudryashov, 2020). Moreover it has excellent documentation which facilitates ease of use (Webb, \n\nz.d.). \n\nSwagger is also a good choice for generating documentation for the available endpoints. It provides \n\nan interactive user interface for experimenting with the REST API and has good integration with \n\nSpring Boot. \n\n\n\n \n3 \n\nFor deployment Docker was configured. However it was only a Dockerfile building the \n\nwebapplication, not a docker-compose. If you go the route of using Docker to facilitate easy \n\ndeployment, and you have dependencies such as a database, you should definitely include and \n\nconfigure these services. This was not done, so it actually does not provide much extra value for \n\neasier deployment. Moreover the stakeholder mentioned that they have limited resources for \n\ndeployment, so in my opinion it would be even better to forgo Docker and just manually deploy the \n\napplication and database, resulting in less bloat. \n\nThe SendGrid API is a fine choice for sending e-mails. It is becoming the standard to use dedicated \n\nservices for sending mails, instead of directly accessing and maintaining your own SMTP-server. It has \n\nnative implementation for Java and makes sending emails easy after creating an account. You can \n\njust send a API call with your token and data to be send. There are alternatives available, however \n\nSendGrid is free for low usage and has excellent documentation, making it a good choice for this \n\nproject. \n\nRegarding the unused dependencies, libraries such as Liquibase,  Jakarta xml, fasterxml Jackson, \n\nglassfish, these are straight up not used and therefor strange to include and load. Liquibase seems \n\nstrange to already have configured, this is a service that facilitates creating schemas and logging \n\nthese changes. However the demo does not use a database. Moreover the XML libraries are unused \n\nand, in my opinion, unnecessary since the native XML parser is sufficient.  \n\n \n\nWhat needs to be added and/or change for the demo to reach \n\nfunctionality? \nFor the demo to reach functionality a database needs to be setup and a way to approach this \n\ndatabase. Moreover a password encoder will be needed to guarantee security. The security for \n\nauthorization and authentication should be better configured and should use more of the standard \n\navailable methods that Spring Boot Security provides. \n\nAlso the data models will need to be refactored, because these are not created with the use of a \n\ndatabase in mind. In the domain logic, the entities are still referenced by Id, instead of just including \n\nthe entity as a child. Moreover these entity reference where sometimes \u201cdynamic\u201d foreign keys, the \n\nID could be an ID for a coach or athlete entity, this is not possible in a relational database. \n\n \n\nHow to best implement the aforementioned additions? \nFor the database the only major choice to make is a relational or non-relational database. After \n\nresearching and keeping in mind the relative small scale of data that will be handled, the rigid, \n\npredefined data that needs to be saved and taking my previous expertise in mind, a relational \n\ndatabase will be set up (Smallcombe, 2018). MySQL is opensource, widely used and I have experience \n\nwith this, therefor the choice will be a MySQL database. \n\nTo approach this database multiple options are available. We could write custom plain SQL Queries \n\nor use an ORM. After discussing this with experts (Wilrik de Loose and Cees van Tilborg) an ORM is \n\npreferred, because this is a more scalable and maintainable solution. It takes the relative complex \n\nand long native queries, and simplifies them, making it easy to use for programmers so they can \n\nfocus on coding and implementing features. (Alvarez-Eraso, Danny & Arango-Isaza, Fernando 2016) \n\n\n\n \n4 \n\nWilrik suggested to look into Spring Data JPA, which I did. It Is the native implementation from the \n\nSpring Boot Framework itself. The documentation is once again superb and integration is flawless \n\nbecause of it being from the same makers. \n\nThis in combination with Hibernate, the advised ORM by the makers of Spring Data JPA, allows for on \n\nthe fly generation of DAO (Data Access Objects). After annotating the entities with Hibernate, Spring \n\nJPA can be used to dynamically create queries from method names, which are instantiated on \n\nruntime.  \n\nMoreover the security should be refactored and reconfigured, since with the current implementation \n\nusers could access each other\u2019s information, moreover after sending out the passwords, they were \n\nnot saved in an encoded and secure fashion.  \n\nUtilizing Spring Boot Security our own UserDetails implementation should be made, to access the \n\ncurrent users data easier and more securely. Also the passwords should be saved encoded, not in \n\nplaintext. The most mentioned and used tool for this, also default by Spring Boot, is BCrypt. \n\n \n\nConclusion \nSummarizing, the demo could adequately show what features a functional app would contain, but is \n\nnot ready for use because of non-persistent data and week security. \n\nThe demo used a number of technologies, the most important ones are that the code was written in \n\nJava, using the Spring Boot Framework to setup a web application. The main form of documentation \n\nwas made with Swagger, showing and demonstrating the endpoints. User account creation utilized \n\nSendGrid to notify the user of their credentials. For deployment Docker was used to create a Docker \n\nimage which could be deployed with Docker, this however did not simplify the deployment process a \n\nlot since it did not account for dependencies like a database. There were also unused dependencies \n\nlike Liquibase, which were unnecessary for what the demo was doing.  \n\nTo transform the demo into a functional application, a database should be added, security should be \n\nimproved on and the data models should be changed so that they are compatible with the database. \n\nWhen taking all the research in mind, the easiest way to achieve a functional application is to setup a \n\nMySQL Database and create a new project, using the same technologies but with better \n\nconfiguration and data model setup.  \n\nThis will make for easier work in the future, resulting in less code debt than if we would continue in \n\nthe existing project. In this new project the security should be configured maximising Spring Boot \n\nSecurity methods. Entities should be annotated with Hibernate and the DAO should be made with \n\nSpring Data JPA, using Hibernate as the ORM.  \n\nThe current demo can be used as guidance as to what logic needs to be implemented, after the initial \n\nsetup and configuration is done correctly.  \n\n  \n\n\n\n \n5 \n\n \n\nGlossary \nAlvarez-Eraso, Danny & Arango-Isaza, Fernando. (2016). Hibernate and spring - An analysis of \n\nmaintainability against performance. Revista Facultad de Ingenier\u00eda Universidad de Antioquia. 2016. \n\n10.17533/udea.redin.n80a11. \n\nKudryashov, R. (2020, 9 januari). Not only Spring Boot: a review of alternatives. Roman Kudryashov\u2019s \n\ntech blog. https://romankudryashov.com/blog/2020/01/heterogeneous-microservices/ \n\nSmallcombe, M. (2018, 29 november). SQL vs NoSQL: 5 Critical Differences. Xplenty. \n\nhttps://www.xplenty.com/blog/the-sql-vs-nosql-difference/ \n\nWebb, P. D. S. (z.d.). Spring Boot Reference Documentation. Spring Boot. Geraadpleegd op 26 \n\nnovember 2020, van https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/ \n\n \n\n\n"
            }]
        },
        "RESTAPIAnalyseDesign": {
            "hand-ins": [{
                "text": "\n\nIntroductie\nBinnen het Delta programma van Fontys kregen wij de optie om een project voor Strijp-T te kiezen. Het project was extreem breed, het uitgangspunt was om concepten te ontwikkelen die kenmerken zijn voor de kernwaarden van Strijp-T. Deze zouden als visitekaartje moeten dienen voor het Strijp-T terrein.\nNa het bedenken van verscheidene concepten is er een conceptpresentatie gegeven waarna Boudie Hoogedeure, de stakeholder, de keuze maakte voor de app die het huidige Strijp-T meer in contact met zijn rijke verleden zou brengen. Het idee van de app is om onder andere doormiddel van AR beelden van vroeger, van Phillips, over de huidige werkelijkheid te weergeven. Daarnaast gaf hij aan dat zij veel video materiaal hebben waarin oud Phillips medewerkers op Strijp-T ge\u00efnterviewd worden.\nHet idee is om hotspots te defini\u00ebren op terrein Strijp-T, waarna als de gebruiker in de buurt is deze informatie krijgt over de huidige locatie.\n\nRequirements\nFunctional Requirements\nFR01 - Als gebruiker kan ik een overzicht zien van alle hotspots.\nK01.1 De hotspots zijn op een kaart als markers te zien.\nK02.2 De hotspots kunnen gesorteerd worden op basis van afstand ten opzichte van de gebruiker.\nFR02 \u2013 Als gebruiker kan ik de informatie van een specifieke hotspot zien.\nK02.1 De informatie is overzichtelijk opgedeeld op basis van het onderwerp of mediatype.\nFR03 \u2013 Als gebruiker kan ik zien op een kaart waar ik ben in relatie tot de hotspots.\nB03.1 De gebruiker moet toestaan dat hun locatie gebruikt wordt.\nFR04 \u2013 Als gebruiker kan ik wanneer ik bij een hotspot ben doormiddel van AR technologie op een interactieve manier informatie zien.\nB04.1 De gebruiker moet toestaan dat hun camera gebruikt wordt.\nFR05 \u2013 Als medewerker van Strijp-T kan ik mij authenticeren.\nB05.1 De medewerker moet een account hebben.\nFR06 \u2013 Als medewerker van Strijp-T kan ik nieuwe hotspots toevoegen.\nK06.1 De medewerker kan op een kaart de locatie van de nieuwe hotspot aangeven.\nB06.1 De medewerker moet geautoriseerd zijn.\nFR07 \u2013 Als medewerker van Strijp-T kan ik bestaande hotspots verwijderen.\nK07.1 De medewerker kan een overzicht zien van hotspots en deze terplekke verwijderen\nB07.1 De medewerker moet geautoriseerd zijn.\nFR08 \u2013 Als medewerker van Strijp-T kan ik de informatie van bestaande hotspots wijzigen.\nK08.1 De medewerker kan uit een overzicht kiezen welke hotspot te willen wijzigen.\nB08.1 De medewerker moet geautoriseerd zijn.\nNon-Functional Requirements\nNFR01 \u2013 Het al opgenomen beeldmaterial dat Strijp-T beschikbaar stelt wordt gebruikt\nNFR02 \u2013 De app is in de vorm van een PWA\n\nContext model\n \nUse Cases\n\n\n\n\n\n\n\n\n\nUse Case Diagram\n\n"
            }, {
                "text": "\n\nIntroductie\nIn Delta we got the option to work on a project for the Strijp-T organization. This organization stands for Make/Create/Innovate. Our assignment was to come up with concepts which express these values and can be used as a calling card for the organization. \nAfter concepting the stakeholder chose the concept of a Hotspot App, this app would show users markers on a map of Strijp-T, tracking their geolocation in relation to the Hotspots. Each Hotspots will show details of the place, showing an interview at the location with an old employee of Phillips and showing through AR how it looked like in the past.\nContextmodel\n\n\n\nContainermodel\n\nComponentmodel\n\n\n\nCodemodel\nIk heb besloten om deze uiteindelijk niet toe te voegen aan het document, ik vind dat het niet duidelijk is en juist eerder voor meer verwarring zorgt. Dit omdat het diagram al gauw enorm kan worden en het makkelijk is om daadwerkelijk naar de code te gaan die eventueel ge\u00efnspecteerd moet worden.  "
            }]
        },
        "RestAPIRealisatie": {
            "hand-ins": [{
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStrijp-T Hotspot  - Rest API \nFontys Delta Semester 4 \n\nMax van Hattum \n\n25-2-2021\n\n\n\n \n1 \n\nInhoudsopgave \nIntroductie ............................................................................................................................................... 2 \n\nEerste stappen ......................................................................................................................................... 2 \n\nDatabase .................................................................................................................................................. 3 \n\nAuthenticatie, Autorisatie en veiligheid .................................................................................................. 4 \n\nHotspot Feature ...................................................................................................................................... 4 \n\nInteractieve documentatie ...................................................................................................................... 5 \n\nDocker ..................................................................................................................................................... 5 \n\n \n\n \n\n  \n\n\n\n \n2 \n\nIntroductie \nBinnen het Delta programma van Fontys kregen wij de optie om een project voor Strijp-T te kiezen. \n\nHet project was extreem breed, het uitgangspunt was om concepten te ontwikkelen die kenmerken \n\nzijn voor de kernwaarden van Strijp-T. Deze zouden als visitekaartje moeten dienen voor het Strijp-T \n\nterrein. \n\nNa het bedenken van verscheidene concepten is er een conceptpresentatie gegeven waarna Boudie \n\nHoogedeure, de stakeholder, de keuze maakte voor de app die het huidige Strijp-T meer in contact \n\nmet zijn rijke verleden zou brengen. Het idee van de app is om onder andere doormiddel van AR \n\nbeelden van vroeger, van Phillips, over de huidige werkelijkheid te weergeven. Daarnaast gaf hij aan \n\ndat zij veel video materiaal hebben waarin oud Phillips medewerkers op Strijp-T ge\u00efnterviewd \n\nworden. \n\nHet idee is om hotspots te defini\u00ebren op terrein Strijp-T, waarna als de gebruiker in de buurt is deze \n\ninformatie krijgt over de huidige locatie. \n\nVoor het systeem moet een API gerealiseerd worden, deze moet een gebruiker kunnen \n\nauthentiseren, hotspots kunnen weergeven/maken/aanpassen/verwijden. \n\nDe API moet geconfigureerd worden met Swagger, zodat een interactieve documentatie aanwezig is. \n\nVerder moet het EF Migrations gebruiken om de database up-to-date te houden. \n\n \n\nEerste stappen \nHet systeem is opgezet in ASP.NET Core, bestaande uit de volgende lagen: \n\nAPI, Viewmodels, Services, Models, Repositories en Data. Waar nodig zijn \n\nook test lagen gemaakt. \n\nIn de API laag is de configuratie te vinden, ook staan hierin de endpoints en \n\nhelperclasses.  \n\nDocumentatie van de architectuur is terug te vinden in het Design \n\ndocument. \n\n \n\n  \n\n\n\n \n3 \n\nDatabase \nDe API verbindt met een MySQL database en gebruikt de ORM EntityFramework om deze aan te \n\nspreken. Verder wordt Migrations gebruikt om de database te initialiseren en up-to-date te houden \n\nmet de modellen, hiervoor wordt een code-first approach gebruikt. \n\n \n\n \n\n \n\nDoormiddel van de EF Migrations CLI kunnen we files genereren vanuit de models. Deze worden bij \n\nstart-up gecontroleerd en waar nodig wordt een database gemaakt of geupdatet.  \n\nDoor het uitvoeren van onderstaande command kan dit gedaan worden: \n\n \n\nDeze moet uitgevoerd worden in de Data folder, aangezien we deze data gescheiden houden in een \n\nandere laag dan de API, moet het start-up project gedefinieerd worden. \n\nOnderstaand de gegenereerde files die Migrations vervolgens gebruikt: \n\n \n\n \n\n  \n\n\n\n \n4 \n\nAuthenticatie, Autorisatie en veiligheid \nAuthenticatie wordt gedaan doormiddel van JWT\u2019s, om bepaalde endpoints te mogen aanspreken, \n\nzoals updaten of maken van hotspots, moet er een valide JWT bij de request zitten.  \n\nDe AuthenticateHandler.cs service faciliteert het cre\u00ebren en inloggen van accounts. Vooralsnog \n\nwordt er een standaard administrator account gemaakt bij het opstarten van de API, het inloggen \n\nkan door het aanspreken van de login endpoint in de Authcontroller.cs.  \n\nVerder worden de ingebouwde Authenticatie en autorisatie modules van ASP.NET core gebruikt om \n\nendpoints daadwerklijk te beveiligen. \n\nVooralsnog bestaat de autorisatie maar uit een rol, namelijk administrator die alle rechten heeft. \n\nOm de veiligheid te garanderen wordt SSL enforced door de client te redirecten naar de https variant \n\nvan de API. Verder wordt het wachtwoord encrypted doormiddel van de Bcrypt Library, deze \n\ngebruikt het BLOWFISH protocol. \n\n \n\nHotspot Feature \nDe Hotspot feature is ge\u00efmplementeerd, dit houdt in dat er endpoints zijn voor het verkrijgen van \n\neen lijst van hotspoten individuele hotspots. Deze returnen Viewmodels met de benodigde data voor \n\nde client.  \n\nVerder wanneer geautoriseerd, is het ook mogelijk om hotspots te verwijderen, up te daten of aan te \n\nmaken.  \n\n \n\n  \n\n\n\n \n5 \n\nInteractieve documentatie \nDoormiddel van de tool Swagger worden de endpoints en viewmodels gemapped en zijn deze \n\nbeschikbaar om uit te testen via een UI. Via Swagger is het ook mogelijk jezelf te autoriseren met \n\nbehulp van een token die je verkrijgt via de auth endpoint. \n\n \n\n  \n\n\n\n \n6 \n\nDocker \nOm het systeem samen te bundelen en makkelijke te deployen wordt Docker gebruikt. Na het \n\nbouwen van de images kan gemakkelijk via de command; docker-compose up, het systeem gestart \n\nworden.  \n\n \n\nOm dit te bewerkstelligen heb ik eerst een Dockerfile geschreven die de benodigde lagen bouwt. \n\nHieruit worden alleen de benodigde files voor het runnen van de API gehaald en er wordt een SSL \n\ncertificaat voor development gebruik aangemaakt.   \n\n \n\n  \n\n\n\n \n7 \n\nHierna heb ik een Docker-compose.yml geschreven die de image gemaakt vanuit de dockerfile en \n\neen MySQL database bundelt. Door middel van environment variables te specificeren worden deze \n\nservice verder geconfigureerd. \n\n \n\n\n"
            }]
        },
        "Sprintopleveringen": {
            "hand-ins": [{
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeedback \n\nMartin \nHartstikke mooi ziet er goed uit, vooral het stukje beveiliging is belangrijk goed te krijgen. Zorg dit \n\nstukje front-end,  hier had de vorige groep hele demo van wat je nu laat zien, maar andere look en \n\nfeel. Hoeverre ben je het wiel opnieuw aan het uitvinden? Het is een groter project en niet fijn dat ik \n\nhet gevoel heb dat er eerst een stap terug dan stap weer naar voren gedaan wordt. \n\nGoed om contact te leggen met de gebruiker basis van PSV zelf. Puntje: zelfde kleurstelling als die \n\nvan de website te gebruiken. Zelfde kleuren vooral. \n\nDeadline voor het registreren van event/meet. Deadline voor zwemmers, deadline voor coaches voor \n\ngoedkeuring, tweede deadline voor zwemmers tot afmelden. \n\n \n\nBritt \nOp basis waarvandaan komen de nieuwe brandguide vandaan etc. Meer contact leggen met de \n\ngebruikers groep. \n\n \n\nCees \nVeel werk gedaan, goed aan de slag. Misschien een idee om oplevering ook aan de achterban van de \n\nzwemclub te doen. Focus leggen op combineren van front-end met back-end. We kunnen werken \n\nnaar testen met de gebruikersbasis.  \n\n\n"
            }, {
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSprint III Oplevering \u2013 REST API \nMax van Hattum \u2013 Open Learning PSV Zwemmen \u2013 17-11-20 \n\n \n\nIntroductie \nIn dit document beschrijf ik voor mijn docenten en groepsgenoot welk werk ik gedaan heb en mijn \n\nverantwoording hiervan. Dit is betreft de REST API, oftewel de back-end.  \n\n \n\nTechnische tegenslag \nHelaas was het niet mogelijk om de demo direct om te zetten naar een functionele applicatie. Het \n\ndesign van de demo was niet met het oog op veiligheid en het verwerken van grote hoeveelheden \n\ndata ontworpen. Daarom ben ik een nieuw project begonnen, die wel veel van de zelfde technieken \n\ngebruikt, maar met een sterkere basis.  \n\n \n\nVeiligheid, Optimalisatie en Database \nEr zijn een aantal toevoegingen gemaakt zodat er op een veiligere manier met data wordt omgegaan. \n\nVoorbeelden hier van zijn het encoden van wachtwoorden en het checken of de huidige gebruiker de \n\nrechten heeft om bepaalde data op te vragen. Hierbij moet je denken aan bijvoorbeeld het feit dat \n\nniet Gebruiker A de gegevens van Gebruiker B kan opvragen.  \n\nDaarnaast aangezien het project helemaal opnieuw opgezet is heeft het een betere basis met oog op \n\nmakkelijkere beheerbaarheid, uitbreidbaarheid en snelheid. \n\nVerder is de applicatie nu zo opgezet dat deze automatisch vanuit de code in de database de tabellen \n\ngenereert als deze nog niet bestaan, en vult deze met drie standaard gebruikers: \n\n \n\n\n\nHuidige functionele logica \n\nAccount maken \nEen administrator kan een nieuw account voor iemand aan maken. Hier wordt aangegeven wat voor \n\nrol deze nieuwe gebruiker heeft; SWIMMER, COACH, ADMIN. Hierna krijgt deze gebruiker een mail \n\nmet zijn of haar account gegevens. \n\n \n\n \n\n  \n\n\n\nOphalen van Meets met inschrijvingen en goedgekeurde inschrijvingen \nDe meets kunnen allemaal tegelijkertijd opgehaald worden. Deze data bevat naast de Meet info ook \n\neen lijst met \u2018PendingRequests\u2019 en \u2018ParticipatingAthletes\u2019. Daarnaast kunnen ook alle Meets horende \n\nbij een zwemmer opgehaald worden.  \n\n \n\n \n\nBeheren van inschrijvingen door Zwemmer en goedkeuringen door Coach \nEen zwemmer kan zich inschrijven op een Event, ook kan deze weer teruggetrokken worden. Daarna \n\nkan de coach de inschrijving goedkeuren of afkeuren. Hierna kan een zwemmer zich alsnog \n\nterugtrekken. \n\n \n\n  \n\n\n\nDeployment \nDe back-end kan heel gemakkelijk op praktisch elk soort OS (onder andere CentOs) gehost worden. \n\nDe applicatie kan zo gebuild worden dat er een bestand is, een zogenaamd .war bestand, die meteen \n\nopgestart kan worden zonder verdere installatie. Wel moet er een MySQL database beschikbaar zijn, \n\nof opgezet worden, met een schema genaamd \u2018psvzwemmen\u2019 en een gebruiker met credentials root \n\n\u2013 root. \n\nDeze credentials kunnen echter gewoon aangepast worden, net als de connectie met de database. \n\nDe database hoeft daardoor niet perse lokaal gehost te worden, al raad ik dit wel aan. \n\n \n\nDatabase ontwerp \n\n \n\n \n\nSuggesties verdere stappen \n\nMail \nHet account waarmee de automatische mails gemaakt worden moet over gezet worden naar een \n\naccount van de stakeholder. Hiervoor moet een account gemaakt worden op SendGrid, zodat dit in \n\nde applicatie omgezet kan worden. \n\n\n\n \n\nAanmaken/Importeren van Meets \nEen administrator moet Meets kunnen aanmaken door een lenex file te importeren. \n\n \n\nKoppelen zwemmer aan coach \nEen zwemmer zou uit een lijst een coach kunnen uitkiezen om zich bij aan te melden, waarna de \n\ncoach dit kan accepteren en/of vice versa. Hierna kan de logica zou aangepast worden dat alleen de \n\nzwemmer zijn of haar eigen coach de goedkeuringen van inschrijvingen beheert. \n\n \n\nTrainingen \nEen coach zou trainingen moeten kunnen aanmaken en hier zwemmers aan kunnen toevoegen, \n\nverder zouden zwemmers zich moeten kunnen inschrijven op trainingen en de coach ze kunnen \n\ntoelaten. Ook moet een coach op de dag van, of na, de training de aanwezigheid van zwemmers \n\nkunnen noteren. \n\nZwemmers moeten een overzicht van beschikbare trainingen kunnen zien en een overzicht van \n\ningeschreven trainingen. \n\n \n\nNotificaties \nEen administrator moet een bericht kunnen publiceren die zichtbaar is voor alle gebruikers of \n\ngeselecteerde gebruikers. Gebruikers moeten aan hun geadresseerde berichten kunnen inzien en \n\nverwijderen. \n\n \n\n\n"
            }, {
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPSV Zwemmen applicatie\n\n\nPSV Zwemmen applicatie\nDoor Max van Hattum & Stephanie Bolder\n\n\nProbleemstelling\n\nIngewikkeld proces registratie wedstrijden\n\nVeel menselijke components\n\nOnvriendelijke gebruikerservaring\n\nPSV Zwemmen applicatie\n\n\n\n\n\n\n\nFront-end\nBrandguide\n\tDesign standaarden, zoals fonts & kleurenpalet\n\nClickable prototype - XD\n\tGetest bij de doelgroep met UX & UI tests\n\t\tAlleen zwemmers getest\n\nClickable prototype \u2013 code\n\tOnboarding\n\tZwemmers\u2019 profiel\n\tRegistratie voor evenement\n\tNotificaties\nInterface designs\n\n\n\nBack-end\n\nNiet mogelijk om demo direct om te zetten naar functionele applicatie\n\nDesign niet met het oog op veiligheid en het verwerken van grote hoeveelheden data\n\n\nNieuw project begonnen met een sterkere basis\n\tVeiligheid & Optimalisatie\n\tAccount maken\n\tOphalen van Meets\n\tInschrijvingen & goedkeuringen\n\tDeployment\n\nTechnische tegenslag\n\n\nBack-end\n\nEncoden van wachtwoorden\nChecken of de gebruiker rechten heeft om data op te vragen\nGebruiker A mag niet de gegevens van Gebruiker B opvragen\n\nTabellen worden gegenereerd als deze nog niet bestaan, en vult deze aan met drie standaard gebruikers\n\tAdmin, Coach & Swimmer\n\n\nVeiligheid, Optimalisatie en Database\n\n\n\nBack-end\n\nDe administrator kan een nieuw account aanmaken\nHierbij wordt aangegeven wat voor rol de nieuwe gebruiker heeft\nAdmin, Coach of Swimmer\n\nDe gebruiker krijgt hierna een mail met zijn / haar account gegevens\n\nAccount maken\n\n\n\n\nBack-end\n\nAlle meets kunnen tegelijkertijd opgehaald worden\nOok een lijst met PendingRequests en ParticipatingAthletes\n\tPendingRequests = lopende registraties van de atleten\n\tParticipatingAthletes = atleten die mee doen aan de meet\nOok alle meets horende bij zwemmers\n\n\nOphalen van Meets\n\n\n\nBack-end\n\nBeheren van inschrijvingen van zwemmer\nZwemmer kan zich inschrijven & uitschrijven voor een event\n\nBeheren van goedkeuringen van coach\nCoach kan inschrijvingen van zwemmers goedkeuren \nen afwijzen\n\n\nInschrijvingen & goedkeuring\n\n\n\nBack-end\n\nBack-end kan gehost worden op elk soort OS\nOnder andere op CentOS\n\nDe database hoeft niet perse lokaal gehost worden, maar wordt wel aangeraden\n\nDeployment\n\n\nVolgende stappen\n\nFront-end\n\tCoach & Admin interfaces testen\n\tCoach & Admin interfaces implementeren\n\tFront-end koppelen aan back-end\n\nBack-end\n\tAanmaken / importeren van Meets\n\tKoppelen zwemmers aan coach\n\tTrainingen aanmaken & zwemmers toevoegen\n\tNotificaties publiceren, inzien & verwijderen\n\n\nSprint IIII\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/docProps/thumbnail.jpeg\n\n"
            }]
        },
        "TranslerendemonaarfunctioneleAppobsolete": {
            "hand-ins": [{
                "text": "Feedback\nMartin\nHartstikke mooi ziet er goed uit, vooral het stukje beveiliging is belangrijk goed te krijgen. Zorg dit stukje front-end,  hier had de vorige groep hele demo van wat je nu laat zien, maar andere look en feel. Hoeverre ben je het wiel opnieuw aan het uitvinden? Het is een groter project en niet fijn dat ik het gevoel heb dat er eerst een stap terug dan stap weer naar voren gedaan wordt.\nGoed om contact te leggen met de gebruiker basis van PSV zelf. Puntje: zelfde kleurstelling als die van de website te gebruiken. Zelfde kleuren vooral.\nDeadline voor het registreren van event/meet. Deadline voor zwemmers, deadline voor coaches voor goedkeuring, tweede deadline voor zwemmers tot afmelden.\n\nBritt\nOp basis waarvandaan komen de nieuwe brandguide vandaan etc. Meer contact leggen met de gebruikers groep.\n\nCees\nVeel werk gedaan, goed aan de slag. Misschien een idee om oplevering ook aan de achterban van de zwemclub te doen. Focus leggen op combineren van front-end met back-end. We kunnen werken naar testen met de gebruikersbasis. "
            }, {
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSprint III Oplevering \u2013 REST API \nMax van Hattum \u2013 Open Learning PSV Zwemmen \u2013 17-11-20 \n\nTechnische tegenslag \nHelaas was het niet mogelijk om de demo direct om te zetten naar een functionele applicatie. Het \n\ndesign van de demo was niet met het oog op veiligheid en het verwerken van grote hoeveelheden \n\ndata ontworpen. Daarom ben ik een nieuw project begonnen, die wel veel van de zelfde technieken \n\ngebruikt, maar met een sterkere basis.  \n\n \n\nVeiligheid, Optimalisatie en Database \nEr zijn een aantal toevoegingen gemaakt zodat er op een veiligere manier met data wordt omgegaan. \n\nVoorbeelden hier van zijn het encoden van wachtwoorden en het checken of de huidige gebruiker de \n\nrechten heeft om bepaalde data op te vragen. Hierbij moet je denken aan bijvoorbeeld het feit dat \n\nniet Gebruiker A de gegevens van Gebruiker B kan opvragen.  \n\nDaarnaast aangezien het project helemaal opnieuw opgezet is heeft het een betere basis met oog op \n\nmakkelijkere beheerbaarheid, uitbreidbaarheid en snelheid. \n\nVerder is de applicatie nu zo opgezet dat deze automatisch vanuit de code in de database de tabellen \n\ngenereert als deze nog niet bestaan, en vult deze met drie standaard gebruikers: \n\n \n\n  \n\n\n\n \n\n \n\nHuidige functionele logica \n\nAccount maken \nEen administrator kan een nieuw account voor iemand aan maken. Hier wordt aangegeven wat voor \n\nrol deze nieuwe gebruiker heeft; SWIMMER, COACH, ADMIN. Hierna krijgt deze gebruiker een mail \n\nmet zijn of haar account gegevens. \n\n \n\n \n\n  \n\n\n\nOphalen van Meets met inschrijvingen en goedgekeurde inschrijvingen \nDe meets kunnen allemaal tegelijkertijd opgehaald worden. Deze data bevat naast de Meet info ook \n\neen lijst met \u2018PendingRequests\u2019 en \u2018ParticipatingAthletes\u2019. Daarnaast kunnen ook alle Meets horende \n\nbij een zwemmer opgehaald worden.  \n\n \n\n \n\nBeheren van inschrijvingen door Zwemmer en goedkeuringen door Coach \nEen zwemmer kan zich inschrijven op een Event, ook kan deze weer teruggetrokken worden. Daarna \n\nkan de coach de inschrijving goedkeuren of afkeuren. Hierna kan een zwemmer zich alsnog \n\nterugtrekken. \n\n \n\n  \n\n\n\nDeployment \nDe back-end kan heel gemakkelijk op praktisch elk soort OS (onder andere CentOs) gehost worden. \n\nDe applicatie kan zo gebuild worden dat er een bestand is, een zogenaamd .war bestand, die meteen \n\nopgestart kan worden zonder verdere installatie. Wel moet er een MySQL database beschikbaar zijn, \n\nof opgezet worden, met een schema genaamd \u2018psvzwemmen\u2019 en een gebruiker met credentials root \n\n\u2013 root. \n\nDeze credentials kunnen echter gewoon aangepast worden, net als de connectie met de database. \n\nDe database hoeft daardoor niet perse lokaal gehost te worden, al raad ik dit wel aan. \n\n \n\nSuggesties verdere stappen \n\nMail \nHet account waarmee de automatische mails gemaakt worden moet over gezet worden naar een \n\naccount van de stakeholder. Hiervoor moet een account gemaakt worden op SendGrid, zodat dit in \n\nde applicatie omgezet kan worden. \n\n \n\nAanmaken/Importeren van Meets \nEen administrator moet Meets kunnen aanmaken door een lenex file te importeren. \n\n \n\nKoppelen zwemmer aan coach \nEen zwemmer zou uit een lijst een coach kunnen uitkiezen om zich bij aan te melden, waarna de \n\ncoach dit kan accepteren en/of vice versa. Hierna kan de logica zou aangepast worden dat alleen de \n\nzwemmer zijn of haar eigen coach de goedkeuringen van inschrijvingen beheert. \n\n \n\nTrainingen \nEen coach zou trainingen moeten kunnen aanmaken en hier zwemmers aan kunnen toevoegen, \n\nverder zouden zwemmers zich moeten kunnen inschrijven op trainingen en de coach ze kunnen \n\ntoelaten. Ook moet een coach op de dag van, of na, de training de aanwezigheid van zwemmers \n\nkunnen noteren. \n\nZwemmers moeten een overzicht van beschikbare trainingen kunnen zien en een overzicht van \n\ningeschreven trainingen. \n\n \n\nNotificaties \nEen administrator moet een bericht kunnen publiceren die zichtbaar is voor alle gebruikers of \n\ngeselecteerde gebruikers. Gebruikers moeten aan hun geadresseerde berichten kunnen inzien en \n\nverwijderen. \n\n \n\n\n"
            }, {
                "text": "\n\nIntroduction\nFor Open Learning we get the opportunity to work for an external stakeholder, giving context to our learning process. I\u2019ve chosen to work on an application for PSV Swimming. The main features this app needs to offer are enabling swimmers to manage their event registrations, coaches to review this registrations and administrators to see an overview of these registrations so they can process them.\nA previous Fontys group already worked on this project, leaving behind a demo existing of a PWA and REST API. This demo uses mock data, which is non persistent, to showcase how such an application would look like.\nMy task is to analyse the REST API and identify what is needed to translate this to a fully functional API taking into account security, scalability and maintainability.\n\nResearch method \n\n\n\n\nConclusie\n\n\nGlossary\n\n"
            }, {
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPSV Zwemmen applicatie\n\n\nPSV Zwemmen applicatie\nDoor Max van Hattum & Stephanie Bolder\n\n\nProbleemstelling\n\nIngewikkeld proces registratie wedstrijden\n\nVeel menselijke components\n\nOnvriendelijke gebruikerservaring\n\nPSV Zwemmen applicatie\n\n\n\n\n\n\n\nFront-end\nBrandguide\n\tDesign standaarden, zoals fonts & kleurenpalet\n\nClickable prototype - XD\n\tGetest bij de doelgroep met UX & UI tests\n\t\tAlleen zwemmers getest\n\nClickable prototype \u2013 code\n\tOnboarding\n\tZwemmers\u2019 profiel\n\tRegistratie voor evenement\n\tNotificaties\nInterface designs\n\n\n\nBack-end\n\nNiet mogelijk om demo direct om te zetten naar functionele applicatie\n\nDesign niet met het oog op veiligheid en het verwerken van grote hoeveelheden data\n\n\nNieuw project begonnen met een sterkere basis\n\tVeiligheid & Optimalisatie\n\tAccount maken\n\tOphalen van Meets\n\tInschrijvingen & goedkeuringen\n\tDeployment\n\nTechnische tegenslag\n\n\nBack-end\n\nEncoden van wachtwoorden\nChecken of de gebruiker rechten heeft om data op te vragen\nGebruiker A mag niet de gegevens van Gebruiker B opvragen\n\nTabellen worden gegenereerd als deze nog niet bestaan, en vult deze aan met drie standaard gebruikers\n\tAdmin, Coach & Swimmer\n\n\nVeiligheid, Optimalisatie en Database\n\n\n\nBack-end\n\nDe administrator kan een nieuw account aanmaken\nHierbij wordt aangegeven wat voor rol de nieuwe gebruiker heeft\nAdmin, Coach of Swimmer\n\nDe gebruiker krijgt hierna een mail met zijn / haar account gegevens\n\nAccount maken\n\n\n\n\nBack-end\n\nAlle meets kunnen tegelijkertijd opgehaald worden\nOok een lijst met PendingRequests en ParticipatingAthletes\n\tPendingRequests = lopende registraties van de atleten\n\tParticipatingAthletes = atleten die mee doen aan de meet\nOok alle meets horende bij zwemmers\n\n\nOphalen van Meets\n\n\n\nBack-end\n\nBeheren van inschrijvingen van zwemmer\nZwemmer kan zich inschrijven & uitschrijven voor een event\n\nBeheren van goedkeuringen van coach\nCoach kan inschrijvingen van zwemmers goedkeuren \nen afwijzen\n\n\nInschrijvingen & goedkeuring\n\n\n\nBack-end\n\nBack-end kan gehost worden op elk soort OS\nOnder andere op CentOS\n\nDe database hoeft niet perse lokaal gehost worden, maar wordt wel aangeraden\n\nDeployment\n\n\nVolgende stappen\n\nFront-end\n\tCoach & Admin interfaces testen\n\tCoach & Admin interfaces implementeren\n\tFront-end koppelen aan back-end\n\nBack-end\n\tAanmaken / importeren van Meets\n\tKoppelen zwemmers aan coach\n\tTrainingen aanmaken & zwemmers toevoegen\n\tNotificaties publiceren, inzien & verwijderen\n\n\nSprint IIII\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/docProps/thumbnail.jpeg\n\n"
            }]
        },
        "VueAppGoogleMaps": {
            "hand-ins": [{
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\nStrijp-T Hotspot  - Vue Map \nFontys Delta Semester 4 \n\nMax van Hattum \n\n13-4-2021\n\n\n\n \n1 \n\nVersion control \nVersion Author Date Adjustments \nV0.1 Max van Hattum  Initial document \n\nsetup, and description \nmaps \n\nV0.2 Max van Hattum  Testing description \nand outcomes \n\n \n\nInhoudsopgave \nVersion control ........................................................................................................................................ 1 \n\nIntroductie ............................................................................................................................................... 2 \n\nMaps ........................................................................................................................................................ 3 \n\nTesten ...................................................................................................................................................... 3 \n\n \n\n \n\n  \n\n\n\n \n2 \n\nIntroductie \nBinnen het Delta programma van Fontys kregen wij de optie om een project voor Strijp-T te kiezen. \n\nHet project was extreem breed, het uitgangspunt was om concepten te ontwikkelen die kenmerken \n\nzijn voor de kernwaarden van Strijp-T. Deze zouden als visitekaartje moeten dienen voor het Strijp-T \n\nterrein. \n\nNa het bedenken van verscheidene concepten is er een conceptpresentatie gegeven waarna Boudie \n\nHoogedeure, de stakeholder, de keuze maakte voor de app die het huidige Strijp-T meer in contact \n\nmet zijn rijke verleden zou brengen. Het idee van de app is om onder andere doormiddel van AR \n\nbeelden van vroeger, van Phillips, over de huidige werkelijkheid te weergeven. Daarnaast gaf hij aan \n\ndat zij veel video materiaal hebben waarin oud Phillips medewerkers op Strijp-T ge\u00efnterviewd \n\nworden. \n\nHet idee is om hotspots te defini\u00ebren op terrein Strijp-T, waarna als de gebruiker in de buurt is deze \n\ninformatie krijgt over de huidige locatie. \n\nHiervoor is het nodig om in de applicatie een kaart te hebben waarop de hotspots en gebruikers als \n\nmarkers te zien zijn. \n\n  \n\n\n\n \n3 \n\nMaps \nEr waren verscheidene opties zoals Google Maps API, OpenLayers en TomTom. Aangezien de pricing \n\npraktisch hetzelfde was en ze allemaal gratis varianten aan bieden was dit geen belangrijke factor. Ik \n\nhad wel al eerder met Google Maps API gewerkt, en hier was ook goede documentatie voor \n\nbeschikbaar, vandaar dat ik hier uiteindelijk voor gekozen heb. \n\n \n\nMarkers worden gegenereerd vanuit een prop die de MapComponent krijgt van de Home pagina. Er \n\nis ook een livetracking feature, wanneer de persoon in de buurt is van Strijp-T, focust het scherm op \n\nwaar de gebruiker is. Op het moment zie je mij in Tilburg. \n\nTesten \nEr is specifiek voor gekozen om niet automatische testen te schrijven, dit omdat het niet mogelijk \n\nleek te zijn om programmatisch deze geocoordinaten aan te passen. Het is dan wel mogelijk om een \n\nMock object in te voeren, maar dan begin je de gegeven API en functionaliteit te testen wat niet \n\nwenselijk is en weinig toegevoegde waarde heeft. \n\nOm de feature toch te testen zijn de Chrome Dev Tools gebruikt, in specifiek om met de Sensor tool \n\nde locatie aan te passen. De locatie van de gebruiker wordt real-time ge\u00fcpdatet als de waardes met \n\nde pijltjes ernaast worden verandert, zie onderstaand twee verschillende locaties van de gebruiker. \n\n \n\n\n\n \n4 \n\n \n\n \n\n \n\n\n"
            }]
        },
        "VueAppkoppelingbackend": {
            "hand-ins": [{
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\nStrijp-T Hotspot  - Vue Koppeling Back-end \nFontys Delta Semester 4 \n\nMax van Hattum \n\n26-2-2021\n\n\n\n  \n\nInhoudsopgave \nIntroductie ............................................................................................................................................... 3 \n\nKoppeling Back-end ................................................................................................................................. 4 \n\n \n\n \n\n  \n\n\n\n  \n\nIntroductie \nBinnen het Delta programma van Fontys kregen wij de optie om een project voor Strijp-T te kiezen. \n\nHet project was extreem breed, het uitgangspunt was om concepten te ontwikkelen die kenmerken \n\nzijn voor de kernwaarden van Strijp-T. Deze zouden als visitekaartje moeten dienen voor het Strijp-T \n\nterrein. \n\nNa het bedenken van verscheidene concepten is er een conceptpresentatie gegeven waarna Boudie \n\nHoogedeure, de stakeholder, de keuze maakte voor de app die het huidige Strijp-T meer in contact \n\nmet zijn rijke verleden zou brengen. Het idee van de app is om onder andere doormiddel van AR \n\nbeelden van vroeger, van Phillips, over de huidige werkelijkheid te weergeven. Daarnaast gaf hij aan \n\ndat zij veel video materiaal hebben waarin oud Phillips medewerkers op Strijp-T ge\u00efnterviewd \n\nworden. \n\nHet idee is om hotspots te defini\u00ebren op terrein Strijp-T, waarna als de gebruiker in de buurt is deze \n\ninformatie krijgt over de huidige locatie. \n\nOm de data te managen is er een Rest API, hieraan moet de Vue app dus gekoppeld worden. \n\n \n\n  \n\n\n\n  \n\nKoppeling Back-end \nOm de hotspots op te vragen bij de back-end moeten er API calls gedaan worden via HTTPS, deze \n\nrequest zouden gedaan kunnen worden met de standaard Fetch API. Er zijn echter ook libraries \n\naanwezig die het proces versimpelen. Denk hierbij aan makkelijker gebruik en ingebouwde error \n\nhandling. Axios is een erg populaire library, deze gebruik ik dan ook om requests te maken. \n\n \n\n \n\nIn een aparte file instantieer ik de methodes die in de rest van de applicaties gebruikt kunnen \n\nworden. Ik cre\u00eber een Axios instantie die een base URL gebruikt waarnaar de requests verzonden \n\nworden. Verder waar nodig wordt een Authorization header toegevoegd. \n\n  \n\n\n\n  \n\nDan wanneer een component gemaakt wordt, roept deze de bijbehorende methode aan om de data \n\nte krijgen. \n\n \n\n \n\n\n"
            }]
        }
    }
}