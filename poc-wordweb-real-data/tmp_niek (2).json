{
    "17661": {
        "AdviceNLPsolutions": {
            "hand-ins": [null]
        },
        "BeehiveBeeresearch": {
            "hand-ins": [{
                "text": "Requirements\nAuthor: Sabrina Hooijmans\nDate: 26/10/20\n\n(M)\tTrack Health of the beehive\n(M) \tRecognise/track where the bees are coming from\n(S)\tNice representation on the window\n\nResearch Questions\nWhat are the key indicators of a healthy beehive?\nHow can we recognise/track where the bees are coming from?\nHow can we get the measured data to a remote location?\nHow can we make the beehive look representable on the window without effecting the health of the beehive?\n\n\n\nIntroduction\n\n\nVersion control\n\n\nResearch method\nThe research methods that I applied for this document are community research and literature studies. In order to get started with the key bee indicators, I went on several forums to garner information from beekeepers. The most frequent forum posts I visited were about how to keep bees alive, and how to see whether or not your bees are healthy. A list with the community forums I went on can be found at the bottom of this page. \n\nFor literature studies, I searched on ResearchGate and google scholar to find applicable research that had been done before. Sadly enough, a lot of the documents I found were not concretely helpful, but they did give me a better foundational understanding of the entire situation. The documents talked about the theory behind the bee dance and how bees kept their population standing. I was hoping to find more \u2018concrete\u2019 applications on how tracking of bees has been done, sadly enough most of the concrete examples were unavailable to us or with cutting-edge technology. \n\nWhat are the key indicators of a healthy bee hive?\nTraffic / population\nThe most obvious indicator to use on determining whether or not a beehive is healthy is the population. Strong hives have an abundance in worker bees, which can show itself in forms of traffic at the entrance of the hive. This traffic isn\u2019t an infallible indicator, as there are many outside factors which can reduce the traffic and make it seem less populated than it actually is. These outside factors can be: time of day, weather, wind speed and warmth. Because of all of these factors, getting a good overview of the actual traffic should be done by averaging the traffic, as single measurements will result in a distorted view on the situation.  \nAnother \u2013 but more complex way \u2013 of checking the hive is physically opening it up and checking. By incorporating this technique we remove the beforementioned outside factors, and are no longer dependant on the fluctuating traffic.  Strong beehives will, when opened up, have bees hanging from the top of the frames, combined with activity on the frame itself. Besides checking the population, this is also a good way to physically check the brood pattern, which is another factor to take into account. The brood pattern should be covered with nurse bees, which are using their bodies as a blanket, covering the brood pattern and retaining the warmth. Next to those nurse bees should also be bees stationed in the honey supers, which are working on protecting the honey from moths and beetles. This area is less populated than other locations in the hive, but the more you see, the healthier the colony.  \nBrood pattern\nA healthy colony has a healthy brood pattern. A brood pattern is a straightforward indicator since there is an objective \u2018good\u2019 and \u2018bad\u2019 brood pattern. When the queen starts laying her eggs, she will put the brood in a specific pattern. This pattern is usually close together in a group, in such a shape that it\u00b4s called a brood pattern. It is the easiest to see when the brood are capped with wax, but it can be viewed earlier. \nThe brood should look like a solid concrete \u00b4block\u00b4 of brood. There should be areas of eggs, larvae and the capped brood, grouped together to retain heat. There are a handful of reasons to this pattern being neglected, most of which are prominent indicators of disease. One of those diseases would be the \u2018Spotty\u2019 brood pattern, which is, as the term indicates, full of spots without brood (figure 1) . This is almost the opposite of what a healthy brood pattern (figure 2) should look like. By knowing what a healthy brood pattern looks like, we can easily test the beehive against a set of healthy beehive pictures and check for similarities. \n\n\n\n\n\n\n\nFigure 1: 'Spotty' brood pattern\t\t\tFigure 2: Healthy brood pattern\n\n\nInsects in the brood\nBesides the patterns, there is more to the brood. Insects are constantly looking to annex part of the beehive brood area for themselves. This doesn\u00b4t necessarily pose a concrete threat, as long as the hive has enough guards to support the brood, all is well. However, there is a chance that the amount of brood becomes too large to protect by the guards. There are a heap of insects waiting for this to happen, so that they can move in and grow their population in the under-guarded brood. \nOnce brood starts to get out of the comb and leave their unused comb behind, the insects start using it as free real estate. This can pose a serious threat to the colony, if the guards cannot fend off these insects there is a chance they will be overrun. As long as the guards are able to fend off the insects there is nothing to worry about, this means that there is always a possibility to find small numbers of beetles and moths. Their presence alone does therefore not define a weak hive.  \n\nPollen\nPollen are a major part of a bees\u2019 diet, which creates a strong correlation between a healthy hive and the abundance of pollen stored in the hive. The collected pollen are taken by the bees throughout the hive to the pollen stores, which are commonly found near the entrance of the hive. A good way of checking whether or not the bees have enough food intake would be to scan the entrance for bees with pollen on them (figure 3) and count how many of these we see a day. Based on this we will be able to create an average trend of the amount of food consumed by the hive per day/week. \n\nFigure 3: Bee with sidebags storing pollen \nHeat of the hive\nIf a beehive gets too hot, the bee brood dies and the honey gets dehydrated too quickly. If a beehive gets too cold, broods die off and the nectar cannot dehydrate fast enough to make honey. 35\u00b0C is the exact right temperature for a hive.\nWarmth\nA key outside factor which has to be taken into account, is the warmth of the beehive. According to studies, beehives need to be kept at a consistent 32-35\u00b0C in order to produce honey. If the temperature rises above 35\u00b0C, not only will the honey production stop, but the larvae will die from hyperthermia. In order to prevent causalities, the bees will strategically position themselves throughout the beehive and start flapping their wings, which will act as a \u2018natural air conditioning\u2019. Besides being lethal for brood, a warm hive also dries out the honey, which would result in no food for the young brood. \n\nCold\nBesides the danger of hyperthermia, there is also the lethal risk of hypothermia. If the brood temperature gets below 32\u00b0C, there is a chance they will decease. Luckily bees are skilled insulators, and can manage to keep the brood at a constant 35\u00b0C, even at temperatures of absolute 0 (-40F, -40\u00b0C). At this temperature, the bees will cluster together and use their bodies as insulating blankets to keep the brood warm. The bees on the inside of the cluster will move toward the outside of the cluster and vice versa, creating a constant flow of warm/cold bees. \n\nHow can we check the key indicators of a healthy beehive?\n\nHow can we recognise/track where the bees are coming from?\nBees communicate in two ways: releasing pheromones and dancing. In order to analyse where the bees are coming from, we will be focussing on the latter. \n\nBee dance detection algorithm\nThe waggle dance\nWhen communicating resource stores in the vicinity to its peers, bees incorporate a specific dance to transmit the location. The bees have a very distinct dance which is executed each time one tries to communicate the location of a resource store.  This dance has a few simple parameters which need to be taken into account in order to comprehend the dance. The dance goes in two parts:\n\nThe bee \u2018waggles\u2019 in a straight line for x amount of seconds ,every second the bee moves forward indicates one extra km. This line is angled against the azimuth, and indicates how many degrees the target is, relative to the hive. \nAfter it has indicated the distance, the bee makes a turn and returns back to the original starting position, this is repeated for a supposedly amount of iterations\n\nIdea 1: bee location plotting\n\nOne idea to track the bees is by plotting their locations on a canvas once every 0.1 seconds (10fps) (figure 4). Every frame we also check whether or not any of the bees are plotting something which looks similar to a waggle dance, and compare it to an actual waggle dance (figure 5). If the comparison validates that this is indeed a waggle dance, we can analyse the dance by looking at the previously plotted points and see what location the bee is trying to communicate.  \n\nFigure 6: Bee dances for different locations in the field\n\nT.B.D.\n\nHow can we get the measured data to a remote location?\nThere are many ways to transfer data to a remote location. The main four types that are used in IoT are Cellular, LAN/PAN, LPWAN and mesh networks \nCriteria :\nRange to the gateway (mid range)\npower consumption (low)\nSecurity (encryption/authentication)\nData rate\nQuality of service and reliability\nSimplex or duplex\nSuitable and available spectrum (Licensed or unlicensed)\nHow can we make the beehive look representable on the window?\n\n\n\nSources:\nBeeculture. (23-04-2018). A closer look \u2013 waggle dances. \nWikipedia. (n.d.). Waggle Dance. \nCyborgAntorphology (22-39-12). Bee dance. \nThe Management Agency National American Foulbrood Pest Plan. (n.d.). \u201cSpotty\u201d brood pattern. \nAllison\u2019s Apiaries. (13-03-2018). Can a beehive get too hot or too cold?. \nBackYardHive. (09-10-2017). Honey Comb Identification \u2013 Brood nest \nHoneyBeeSuite. (n.d.). What is a brood pattern?. \n\n\nScienceDirect. (02-06-2014). Dancing bees communicate a foraging preference for rural lands. "
            }]
        },
        "CompetenceDocument": {
            "hand-ins": [{
                "text": "Version control\n\n\nIntroduction\nNiek van Dam\n433010\n\nMy name is Niek van Dam, I am a 19-year-old student from the FHICT Delta course studying the software trajectory.\nBefore coming to Fontys, I studied software engineering on a vocational level for 3 years, where we created responsive web apps. During this study, my teachers let me take a deeper dive into algorithms and design patterns, after which I got interested in software engineering.\nRight now, I am still very interested in algorithms and started working with neural networks, which is my main goal to master right now, combined with mathematics.\u00a0 Besides programming, I am also interested in trading on the stock market and crypto markets. I have been trading on the crypto market for about two years now, and the stock market for a few months. My goal is to create a trading algorithm - if possible, with neural networking - that will automate most of the trades for me.\n\u00a0\nWithin Delta, I am working on the following projects:\nDataWall\nASAM\nOpenRemote\nIntelligent Beehive\nDeX\n\n\n\nProjects\nDataWall\nDatawall is a project that aims to identify and visualize the identify of all FHICT students and employees and improve the social cohesion between all the different study routes. The project will be an \u2018interactive art\u2019 setup, in which every student will be visualized by a \u2018node\u2019. The more nodes, the clearer the pictures become and the more complex pictures can be created. \nWithin DataWall, I am working on the API and Unity back-end, which has been a struggle so far. I am planning on getting some good progress in the coming few weeks, if everything goes according to plan. \n\nASAM\nAsam is a relatively new project, in which we have not done much work yet. The project is about upgrading the internship system for Fontys, by adding new innovative features. \n\nOpenRemote power prediction\nThe OpenRemote project initially began as the \u2018smart streetlights on Strijp-s\u2019 project. After the first meeting it became clear that this project was already taken by another group, which meant that we were free to take another project. In that moment, our team decided to take \u2018wind power prediction\u2019, which is a project where we must create a RNN neural network which will predict wind power soon. The biggest challenge in this project is finding a dataset, needed to create a proof of concept. After that we must start creating a network based on real life data, which is also a challenge. \nWithin this project I tried to take the \u2018lead\u2019 and get the others to work with me. After a few weeks, we also decided to implement scrum into the project. We are working in sprints of two weeks and have a delivery at the end of each one of them. Within this project I have been working on the research document and the proof of concepts for predicting wind power.  \n\nIntelligent Beehive\nIntelligent Beehive is a delta project in which the goal is to create a \u2018smart\u2019 beehive, which is being monitored by an array of sensors. In this project we are building upon an already existing piece of software, which already has some realized features like the BroodPatternDetection. This piece of software analyses the breeding patterns of the honeybees and checks whether those patterns are considered \u2018healthy\u2019. \nThe first concrete piece of software which I want to realize is the tracking of the honeybees\u2019 communicative ways, of which the goal is to map where all the bees have been. \n\n\nDeX\nDeX (Digital Excellence) is a big archive of all projects and ideas throughout Fontys stored in one place. The goal of this project is to increase cooperation and popularity of projects since most remain unseen currently. \nBy the time I joined DeX, it was already a running project with a functioning front- and back-end codebase. Because it was already set up, I was tasked with making the website more \u2018discoverable\u2019 by improving the SEO (Search Engine Optimization). Besides, SEO I am also working as full-stack developer, meaning that I can work on both codebases if I see fit. This gave me the opportunity to also implement other miscellaneous tasks, like CI/CD and pipelining. \n\n\nKPI-table with proof\nProject abbreviations:\nOR: OpenRemote Wind Power Prediction\nIBH: Intelligent Beehive\nASAM: ASAM\nDeX: Digital Excellence\nDW: Data Wall\n\n\nKPI Table\n\n\n\nKPI-Matrix\nEvaluation and Reflection\nEvaluation mid-term\nIn the semester so far, there have been a lot of interesting things happening. In the beginning we started out with little to no structure in the projects, which was mostly due to the fact that almost none of my projects had a dedicated leader. After realising that the projects were not going anywhere, I decided to ask Wilrik for advice on how to structure the projects better. His advice was: implement a type of project management like Scrum and get a project lead. After this meeting I decided to incorporate this into the projects which were progressing a bit \u2018rocky\u2019. A few days after implementing this, I started noticing more structure within the project, goal achieved. \nBy now, most of my projects have improved greatly in comparison to the beginning, which makes working within delta a lot more comfortable and less stress-invoking. \n\nFinal evaluation\nThis semester has turned out to be a lot differently than I initially thought, this is mostly due to the covid situation and the fact that we are unable to physically work together. In the beginning of the semester I was sceptical about Delta in general, as most projects were not working smoothly and I felt like there was little productivity within the entire course. When structure finally appeared in most of the projects I was able to get a lot more joy out of working within delta, as I felt like I was finally doing something productive.\nWorking at a lot of projects at the same time definitely was not the best approach to this semester, as it made things a bit more stressful than they should have been for me. This wasn\u2019t the case at the start of the semester, as in the beginning I was only working on DeX. At the time, I did a lot of research for SEO and trying to implement more SEO features into the project. As soon as the OpenRemote project started rolling, my attention shifted to that project and I had no time left to still work as hard on DeX as I did in the beginning. After all of my other projects starting up as well, it started to become a daily juggle between my projects and how to spend most of my time. \nThe projects DeX and OpenRemote are definitely the ones that I am the most proud of working on, I was really invested in these projects. Sadly enough we were not able to finish the OpenRemote project in one semester, as that was my main goal. I am still really proud of our accomplishments within this project, as we will still get a turbine up and running in the near future, accompanied by the great load of AI skills that I learnt. DeX was all around a good project to be in and I felt that the work I was doing there actually helped the website. This was also something that could concretely be checked with SEO tools, further proving to myself that my actions were actually doing something. \n\n\nReflection  mid-term\nDuring the semester, I have grown mostly in terms of personal leadership and researching. These are two things that I have not done in such a dedicated manner as I have in this semester. In the beginning the projects were not really progressing as much as I would have hoped, so I picked up the initiative to try and \u2018lead\u2019  the projects and get structure going within the team. After a painstaking amount of awkward meetings with no clue what to discuss next, I finally got the hang of it. Now I\u2019m able to more comfortably take the lead in a meeting when I feel that it is going nowhere, which is a huge improvement in comparison to last year.\nBesides leading a team, I have also done a good amount of research on the topics I was about to work on, which helped more than imaginable. I have the feeling that the writing of a research document gives you a better understanding of what you have just researched. I think this because you can immediately try and apply your new knowledge by creating examples for your readers which makes you think about the topics more as well. By writing research documents for the more complicated topics I was about to approach, I felt like I had better preparation for what I was about to get myself into. \n\nFinal reflection\nIn the beginning of the semester I was mostly working on getting the projects to run smoothly and increasing my PO skills in the project. This changed in the second half, when the projects finally started to run smoothly. \nWithin OpenRemote, I was usually presenting the deliveries with Elmira, to the point where I felt like I was gradually improving my way of presenting to OpenRemote. This has been a goal of mine for a while, as I would like to improve my presentation skills. At OpenRemote I have also gained a lot more opportunities to have contact with the stakeholders, which also helped me in figuring out how to properly do so in the future. \nWe also had to incorporate a lot of research in order to buy to buy/install a wind turbine for OpenRemote. Due to concretely researching what the company needed, I gained a lot of research skills, mostly because of the broad array of research questions we had to solve: how to install a wind turbine/how safe is it/is it realistic/ etc. This really made us think about all possible edge cases when installing such a thing. I feel like I still have a lot to gain on this subject, as this didn\u2019t go as smoothly as expected and gave me some stress now and then.  \nThere were times where the teamwork within OpenRemote did not go as expected, to the point where it was difficult to work with my teammates in the project. At first I discussed this with Elmira, who had noticed the same thing. Afterwards we decided to bring it up in a meeting, in which everyone agreed that teamwork wasn\u2019t going well. After this meeting, however, nothing concretely changed. After this going on for another while we decided it was time to undertake action, which is when we kicked Daniel out of the project.\nIn general this semester lead me to learn a lot about presenting to the customers, communicating with them, trying to manage a team and researching \u2018abstract\u2019 questions. On the other hand, it showed me that I need to improve my scheduling skills, as this gave me a lot of stress.  \n\n\nAttachments:\nAttachment  1:\n\n\nAttachment 2:\n\n\nAttachment 3:\n\n\nAttachment 4:\n\nAttachment 5:\n"
            }]
        },
        "DataWallAnalyseexistingcode": {
            "hand-ins": [{
                "text": "\n\n\n\n\n\nTo-Do\n\nGetters and Setters of objects\nFor some reason, the objects which are being referenced in the BackendSpring project do not contain getters and setters. This causes the IDE to fall over and report up to 100+ problems. This is uncalled for and should be able to be fixed by simply creating getters and setters. \nCreate postman tests for the API\nTo test the API, we can use postman, this is a program which executes simple web requests to a specified address. These web requests can be filled up with a json body in order to add a request body. Because of this we are able to use this for testing the API, and checking whether or not everything returns in an appropriate manner. \n\nA small list of things that we can test with Postman are listed below, including further explanations:\nCreate users / roles (possibly not necessary)\nThis could possibly not be necessary, as we are going to use the FHICT API to be logging in. The assumption is that using the FHICT API to log in would automatically create a user/role\nAuthenticate users\nAuthenticating users goes with the FHICT API as said before, this procedure however does need to be researched. \nManage modules\nAdd\nUpdate\nDelete\nCreate visualizations from modules\nEach visualisation consists of a set of modules, we need to find out how these are linked and passed to the API and Unity project\nGet OAuth working\n\nUpdate recipes to match unity\u2019s recipe model\nAccording to last meeting with the original developers, Unity had a different object for recipes in comparison to the API. This needs to be updated from the API side, since the Unity side shouldn\u2019t be tweaked with if possible.  \n\n\n\n\n\n\n"
            }]
        },
        "DeXCICDLighthousecheck": {
            "hand-ins": []
        },
        "DeXSEOIntroduction": {
            "hand-ins": [{
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\nDeX\u00a0\nSEO\u00a0 \u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n05.10.2020\u00a0\n\nVersion 1.0\u00a0\n\n\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\nVersion History\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0 \u00a0\n\n1\u00a0\n\nVersion\u00a0 Date\u00a0 Author\u00a0 Changes\u00a0\n\nV0.1\u00a0 05-10-2020\u00a0 Niek van Dam\u00a0 Initial document structure\u00a0\u00a0\n\nV0.2\u00a0 06-10-2020\u00a0 Niek van Dam\u00a0 Addition of future SEO plans\u00a0\n\nV0.3\u00a0 09-10-2020\u00a0 Niek van Dam\u00a0 Refactor of the document\u00a0\n\nV1.0\u00a0 12-10-2020\u00a0 Niek van Dam\u00a0 Added examples and improved\u00a0\ndocument structure\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\nTable of Content\u00a0\n\n\u00a0\n\nVersion History 1\u00a0\n\nTable of Content 2\u00a0\n\n1. Introduction 3\u00a0\n\n2. SEO Basics 4\u00a0\n\n3. What DeX has done to improve SEO 5\u00a0\n\nTag updates 5\u00a0\n\nFiles to increase SEO 6\u00a0\n\nRobots.txt 6\u00a0\n\nSitemap.xml 8\u00a0\n\nUser-friendly pages 10\u00a0\n\n4. Future SEO plans 11\u00a0\n\nEnhanced google search results 11\u00a0\n\nEnhance URLs 11\u00a0\n\nEnhance tags and websites 12\u00a0\n\nT.B.D 12\u00a0\n\n5. Testing 12\u00a0\n\n\u00a0 \u00a0\n\n2\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\n1. Introduction\u00a0\n\nA lot of employees and students of FHICT create a lot of nice products. As a hobby, for their\u00a0\n\nwork, or because of a school project. These can be software solutions, small scripts, research\u00a0\n\npapers, thesis but possibly also project ideas and proposals for others to pick up. Unfortunately,\u00a0\n\nmost of them remain private and are not known by the majority of the population. The goal of the\u00a0\n\nDeX-platform (short: DeX) is to make all this work more findable and thus improving collaboration.\u00a0\n\n\u00a0\n\nTo get people on the platform, we need to edit the website in such a way that the platform\u00a0\n\nbecomes more discoverable on SERPs (Search Engine Result Pages). A few common examples of\u00a0\n\nSERPs are Google, Bing, and Yahoo. These SERPs get their results from the respective web\u00a0\n\ncrawlers, which are constantly indexing new webpages and ranking these for specific keywords.\u00a0\n\nThe goal with SEO is to reach the highest possible index on these SERPS. This can be reached\u00a0\n\nby modifying your pages, adding metadata, and guiding the crawlers.\u00a0\u00a0\n\n\u00a0\n\nIn this document, we will go more into depth about how we approached SEO in DeX.\u00a0 \u00a0\n\n3\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\n2. SEO Basics\u00a0\n\nDeX is a project that has been launched pretty recently at the time of writing. Because of its\u00a0\n\nnovelty, it does not have a lot of organic visitors yet, which means that we have to garner them\u00a0\n\nourselves. To get these visitors, we need to update the site in such a way that google will relate it\u00a0\n\nto specific search \u2018keywords\u2019.  For example, DeX is a site mainly focussed on\u00a0\n\nsharing/collaborating on ideas and projects, this means that when someone is looking up:\u00a0\n\n\u201csoftware collaboration\u201d, we would want our site popping up in the SERPs. To achieve this, we\u00a0\n\nhave to organize our site in such a way that the crawlers will link our website to keywords as\u00a0\n\n\u2018software\u2019, \u2018collaboration\u2019, \u2018sharing\u2019 etc.\u00a0\u00a0\n\nBy getting our site linked to a few keywords we\u00a0\n\nare far from done, there are a lot of optimizations\u00a0\n\nto be made to get a higher ranking on the SERPs\u00a0\n\n(which is something to always strive for). Although\u00a0\n\nthat might seem obvious, I still want to stress how\u00a0\n\nbig of a role the SERP index plays. The SERP\u00a0\n\nindex is based on how well-designed and \u2018crawler\u00a0\n\nfriendly\u2019 your page is. The more information (good\u00a0\n\nmetadata, updated tags, informative alt tags, etc.)\u00a0\n\na crawler can get from your website, the more\u00a0\n\nlikely you are to get a higher ranking.  Besides\u00a0\n\nthat, the ranking is also based on monthly traffic\u00a0\n\nand engagement. The image below shows the\u00a0\n\nCTR (Click Through Ratio) based on the ranking in\u00a0\n\nthe SERP. This ratio is used in expressing how\u00a0\n\nmany people click the link to the website when\u00a0\n\nthey are posed with it. For example, 43.2% of all\u00a0\n\npeople seeing the red result would have clicked\u00a0\n\nthis. From this picture, we can see that a sub-top 5\u00a0\n\nalready reduces your potential traffic down to 15% of all people looking for \u2018your tag\u2019.\u00a0\u00a0\n\n\u00a0\n\nIn conclusion, to grow in organic clicks we have to create a website that has good and relatable\u00a0\n\ntags so that we appear in the SERPs, besides which we also have to create such an SEO-friendly\u00a0\n\nsite that we get ranked in the top 5 results.\u00a0\n\n4\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\n3. What DeX has done to improve SEO\u00a0\u00a0\n\nThe issues that DeX has worked on in terms of SEO can be found \u200bhere\u00a0\u00a0\n\nTag updates\u00a0\n\nThe first thing we did to improve the SEO is updating \u2018tags\u2019. This is described vaguely on purpose\u00a0\n\nsince there is a broad array of different kinds of tags to be updated.\u00a0\u00a0\n\nWe started by updating the meta description and title tags since these are easily modified and\u00a0\n\nhave a decent impact on the SEO already. The meta description (figure 1) is usually shown by\u00a0\n\nSERPs underneath the search results, which plays a big role in convincing users to click on your\u00a0\n\nwebsite. The best practice is to give each page a good and unique meta description, which will\u00a0\n\nrank you higher in the SEO index.\u00a0\n\n\u00a0\n\n\u00a0\n\nBesides updating the meta description, it is also important to add some \u2018quality of life\u2019 tag\u00a0\n\nupdates to the website. For example the lazy loading of images, alt image tags for text-only\u00a0\n\nbrowsers, and a dynamically updating title tag. All of these small additions will end up boosting\u00a0\n\nyour SEO indexing by a significant amount if applied properly.\u00a0\n\n\u00a0 \u00a0\n\n5\u00a0\n\nhttps://github.com/DigitalExcellence/dex-frontend/issues?q=is%3Aissue+label%3ASEO\n\n\n\u00a0\n\n\u00a0\n\nFiles to increase SEO\u00a0\n\nBesides the updating of web content, there is also the possibility of increasing the SEO by adding\u00a0\n\nSEO-specific files to the root folder of the website. These files are meant for guiding the\u00a0\n\nrespective web crawlers to the right files (or restricts them from entering certain directories)\u00a0\n\nRobots.txt\u00a0\n\nThe robots.txt is an important text file that instructs web crawlers to crawl certain pages on your\u00a0\n\nsite. This file gives site-wide instructions on how to handle links to the web crawler. By guiding\u00a0\n\nthe web crawlers through your website you gain more control over the pages which are being\u00a0\n\nindexed. For example, denying the web crawlers from accessing the staging site, which could be\u00a0\n\nfull of SEO-unfriendly pages.\u00a0\u00a0\n\nThe markup of a robots.txt file is for the most part quite simple and direct, making the document\u00a0\n\nclean and easily understandable. The only part that can get quite complicated is the markup for\u00a0\n\nURLs, but we\u2019ll get into that later.\u00a0\u00a0\n\nFor now, the basic syntax is as follows:\u00a0\n\n\u00a0\n\nIn the robot.txt above we can see most of the syntax, I\u2019ll describe it once again below:\u00a0\n\n- User-agent\u00a0\n\n- The user agent is the entity that is currently crawling the webpage, this could be\u00a0\n\nDiscoBot, GoogleBot, BingBot, etc. You can either specify one of those or use a\u00a0\n\nwildcard (*) to target all of them\u00a0\n\n- Below the User-agent tag, the rules for that agent are listed\u00a0\n\n- A new user agent is defined by a line break underneath the rule list\u00a0\n\n- Disallow\u00a0\n\n- This is a rule for a specific user-agent, which is why it is listed underneath the\u00a0\n\nuser-agent. This rule tells the crawler where to go, or where to stay away from.\u00a0\n\n6\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n- Allow\u00a0\n\n- This is the opposite of disallowing and is mostly used for specific user agents that\u00a0\n\nare allowed to crawl sites that others are not allowed to\u00a0\n\n- Sitemap\u00a0\n\n- This is a generic tag, used to tell the crawlers where the sitemap is. The usage\u00a0\n\nand importance of a sitemap will be covered later.\u00a0\u00a0 \u00a0\n\n7\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\nSitemap.xml\u00a0\n\nThe second file is the sitemap.xml, which is also used to guide the crawlers, but differently. This\u00a0\n\nfile is a blueprint containing each site you have on your website. This is especially useful when\u00a0\n\nthere is a lot of content that may not be well-linked to each other. In this case, the crawler can\u00a0\n\nrequest the sitemap and check which web pages still need to be indexed.\u00a0\n\nEvery sitemap has a few basic required tags. Those are as follows:\u00a0\n\n- A sitemap must begin with an opening <urlset> tag and end with a </urlset>\u00a0\n\n- Include an <url> entry for every child\u00a0\n\n- You must include a <loc> child tag for each <url> parent tag\u00a0\n\nBelow you can see these rules applied in a basic sitemap document.\u00a0\n\n \n\nOptionally, there are two more tags to add, used for manipulating the frequency of your page\u00a0\n\nbeing crawled. These are optional, if none are provided the crawler will implement these values\u00a0\n\nitself. The two tags are as follows:\u00a0\n\n- lastmod\u00a0\n\n- On every update of a webpage, you can update the lastmod. This will let the\u00a0\n\ncrawlers know whether or not the page content is outdated or not. The lastmod\u00a0\n\ntag can go hand in hand with the changefreq tag since both are related to page\u00a0\n\nupdates.\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n8\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\n- Changefreq\u00a0\n\n- The changefreq is a tag used to indicate how many times a period the page is\u00a0\n\n\u2018updated\u2019. Crawlers have the option to memorize this changefreq and will come\u00a0\n\nback once the website has changed based on the changefreq. This is not\u00a0\n\nnecessarily always the case, since a page marked \u200bhourly\u200b could be crawled daily.\u00a0\nThis decision cannot be influenced and comes from the crawler itself\u00a0\n\n- The list of change freqs is as follows:\u00a0\u00a0\n\n- Always\u00a0\n\n- Hourly\u00a0\n\n- Daily\u00a0\n\n- Weekly\u00a0\n\n- Monthly\u00a0\n\n- Yearly\u00a0\n\n- Never\u00a0\n\n- This value should be used for archived URLs, which will no longer\u00a0\n\nbe updated\u00a0\n\n\u00a0 \u00a0\n\n9\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\nUser-friendly pages\u00a0\n\nAnother small issue that has been worked on is the creation of user-friendly page URLs. This\u00a0\n\nmeans that all of the web page URLs need to have human-readable text in them instead of, for\u00a0\n\nexample, ids. We did this by replacing the ID\u2019s with the project name in the router and in certain\u00a0\n\nHREFs, which made it \u2018human-readable\u2019.\u00a0\n\nThis updating of links is not only useful for crawlers but can also be used to improve CTR from\u00a0\n\norganic users. This is because a semantically accurate URL will look a lot more \u2018trustworthy\u2019, in\u00a0\n\ncomparison to a poorly structured URL with almost \u2018random\u2019 numbers in it.\u00a0\n\n\u00a0\n\n\u00a0 \u00a0\n\n10\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n4. Future SEO plans\u00a0\n\nEnhanced google search results\u00a0\n\nWith enhanced google search results we can make the website more appealing to the public,\u00a0\n\nwhich would therefore increase the CTR. There has not yet been a fully-fledged brainstorm\u00a0\n\nsession about this idea, but some inspiration has been drawn from the google carousel.\u00a0\n\n\u00a0\n\nEnhance URLs\u00a0\n\nThere are still plenty of URL enhancements to be\u00a0\n\nmade to increase SEO even more. We can add\u00a0\n\nmore keywords to the URL (but not too many!).\u00a0\n\nAnother way to enhance the URLs is by shortening\u00a0\n\nthem. As shown in the diagram to the left, there is a\u00a0\n\ncorrelation between the average URL length and\u00a0\n\nthe google position.\u00a0\u00a0\n\n\u00a0\n\n11\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\nEnhance tags and websites\u00a0\n\nThere are a handful of additional tags to update and implement to increase SEO. These tags or\u00a0\n\nwebsite updates don\u2019t have a huge impact itself, but 10 of these small updates already have the\u00a0\n\npotential to boost your SERP ranking. I listed the most prominent 5 below:\u00a0\n\n1. Use your keyword in H1, H2, or H3 tags\u00a0\n\n2. Use your keyword once in the first 150 words\u00a0\n\n3. Update your page content with LSI keywords\u00a0\n\n- LSI keywords are a special set of keywords used by Google to determine the\u00a0\n\noverall topic of your webpage. A detailed description of LSI can \u200bbe found here\u00a0\n4. Use trusted external links (5-8 are recommended). This way google sees that the content\u00a0\n\nis well-referenced and trustworthy.\u00a0\u00a0\n\n5. Use internal links with keyword-rich anchor text in these links. This means that you should\u00a0\n\nuse keywords of the referenced page in the \u200banchor text\u200b.\u00a0\n\n\u00a0\n\nT.B.D\u00a0\n\nThere are still loads of things to do to increase SEO. The checklist that I am currently using is the\u00a0\n\nblacklinko SEO checklist\u200b. Blacklinko is a good resource for more SEO inspiration, so I recommend\u00a0\nchecking this out when all other things are finished.\u00a0\u00a0\n\n5. Testing\u00a0\n\nTesting is done by using the Google Search Console. In this tool, we can easily get an overview\u00a0\n\nof all data that Google has gathered about the website.\u00a0\u00a0\n\nThe console has a broad array of SEO tools, which can help you a lot in analyzing whether or not\u00a0\n\nyour website needs to be upgraded. There has been a discussion about applying Google\u00a0\n\nAnalytics, but this idea has been rejected due to the privacy policy that would have to be\u00a0\n\nupdated.\u00a0\u00a0\n\n\u00a0\n\n\u00a0\n\n12\u00a0\n\nhttps://backlinko.com/hub/seo/lsi\nhttps://backlinko.com/seo-checklist\n\n"
            }]
        },
        "DeXSEOResearch": {
            "hand-ins": [{
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearch: SEO \nDigital Excellence \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n\n \n\n\n\nVersion history \n \n\n  \n\n1 \n\nVersion Date Author Changes \n\nV1.0 13-10-2020 Niek van Dam Initial document structure \n\n   -  \n\n\n\nTable of contents \nVersion history 1 \n\nTable of contents 2 \n\nReason for this research 3 \n\nResearch method 3 \n\nResearch questions 3 \nMain questions 3 \nSub questions for each SEO improvement 3 \nSub questions for checking if SEO is working 3 \n\nWhat is SEO? 4 \n\nWhere can I find information on how to improve SEO? 5 \nGoogle SEO Starter guide 5 \nBlacklinko 6 \nNeilpatel blog 7 \n\nHow to test whether your SEO is functional? 8 \nGoogle analytics 8 \nGoogle Search Console 8 \nSEMRush 8 \n\nFinal remarks 9 \nKeywords 9 \n\nConclusion 10 \n \n\n  \n\n2 \n\n\n\nReason for this research \n \nWithin DeX, I took the responsibility to increase the SEO on the website. Before this, I had no real \nexperience with SEO besides some small posts I read about this here and there. In this document \nI will be looking for some good sources I can use in order to start working on SEO, combined with \ntools that can be used in order to test whether or not I have done a good job.  \n \n\nResearch method \nThe research method that I have applied in this research document is community research. I \nstarted off by looking online for any threads about SEO which asked for checklists with SEO \nimprovements. After a while I gathered a big collection of \u2018seo checklists\u2019. Most of these were \nsmall and irrelevant, until I got a collection of extensive SEO improvements which were all unique \nin their own way. The collections with SEO improvements that remain will be discussed in the \ndocument below, as one of the main research questions.  \n\n \n\nResearch questions \nBefore the start of the research, we have to set several questions we would like to answer. These \nquestions are as follows: \n\nMain questions \n- What is SEO? \n- Where can I find information on how to improve SEO? \n- How to test whether or not SEO is working? \n\nSub questions for each SEO improvement \n- Complexity, how difficult is it to implement the SEO improvement? \n- Impact, how impactful is the SEO improvement? \n\n \n\nSub questions for checking if SEO is working \n- Complexity \n- Usability \n- Cost \n\n \n\n3 \n\n\n\nWhat is SEO?\u00a0\nA lot of employees and students of FHICT create a lot of nice products. As a hobby, for their work,\u00a0\n\nor because of a school project. These can be software solutions, small scripts, research papers,\u00a0\n\nthesis but possibly also project ideas and proposals for others to pick up. Unfortunately, most of\u00a0\n\nthem remain private and are not known by the majority of the population. The goal of the\u00a0\n\nDeX-platform (short: DeX) is to make all this work more findable and thus improving collaboration.\u00a0\n\n\u00a0\n\nTo get people on the platform, we need to edit the website in such a way that the platform\u00a0\n\nbecomes more discoverable on SERPs (Search Engine Result Pages). A few common examples of\u00a0\n\nSERPs are Google, Bing, and Yahoo. These SERPs get their results from the respective web\u00a0\n\ncrawlers, which are constantly indexing new webpages and ranking these for specific keywords.\u00a0\n\nThe goal with SEO (Search Engine Optimization) is to reach the highest possible index on these\u00a0\n\nSERPS. This can be reached by modifying your pages, adding metadata, and guiding the crawlers.\u00a0\u00a0\n\n\u00a0 \u00a0\n\n4 \n\n\n\nWhere can I find information on how to improve \nSEO? \n\nGoogle SEO Starter guide \nAnother trusted resource is the \u200bGoogle SEO Starter Guide\u200b. Since this is made by Google, we can \nassure that it covers most of the requirements which the GoogleBot has.  \n \nGoogle offers a good starting guide in order to improve basic SEO. The tutorial is written in an \naccessible way, without any prior knowledge needed to get started. It starts off with basics like \nadding robots.txt, unique title tags, description meta tags, basic markup and url optimizations.  \n \nThe tutorial goes into depth on the content markup on the websites. It emphasises the importance \nof a well structured and user-friendly website. It also slightly references the usage of \u2018trusted \nsources\u2019, which is an additional parameter google uses for ranking.  \n \nOverall, this SEO Starter Guide is a very good foundation to get a basic understanding of SEO. In \nterms of complexity, this guide has a lot of accessible topics and I would recommend it as a \nstarter guide. The impact of applying the SEO tips noted in this guide will be enough for you to get \nmore organic clicks on your website.  \n \n \n\n  \n\n5 \n\nhttps://support.google.com/webmasters/answer/7451184?hl=en\n\n\nBlacklinko \nBlacklinko is a highly praised website for SEO which I saw coming by quite a few times while \nresearching. This website has tons of information about SEO and thanks to the common blog \nposts it has a lot of resources about SEO knowledge.  \n \nBlacklinko offers extensive in-depth tips and tricks about how to improve your SEO. Due to how \nin-depth it is, it can be hard to grasp some concepts which are applied in the blogs. Whereas \nGoogle was mostly practical examples with not a lot of technical jargon, Blacklinko has the \ntendency to go into more technical language pretty quickly.  \n \nThe ways on how to improve SEO are also completely different than what Google offers. \nBlacklinko goes into the ways that crawlers work, and modify their content so that it \u2018optimizes\u2019 for \nthe crawlers (which is \u200bsurprisingly \u200benough what SEO stands for). That doesn\u2019t mean that \nBlacklinko doesn\u2019t go into the basics as well - since it does -, but the majority of the blog posts are \nabout really specialized topics.  \n \nOverall, Blacklinko has a broad array of ticks and trips, but I would not recommend it as a starter\u2019s \nguide because it has too many specialised topics. It would be better to start with basic SEO then \nto immediately jump the gun and do some highly specialised SEO tricks.  \n \n\n  \n\n6 \n\n\n\nNeilpatel blog \nNeil patel\u2019s blog is comparable to blacklinko, but this blog does not only feature SEO. Neil Patel\u2019s \nblog is about a broad array of business and ecommerce related topics, that\u2019s where SEO comes \nin.  \n \nThe blogs which are about SEO are a huge source of information, and are usually more than just \na few lines of information. Most guides I found from Neil concerning SEO have over 300+ pages \nof information, all for free. The information that Neil delivers is comprehensible and easily \nreadable, with a lot of visualisations to explain foreign concepts.  \n \nI do however feel like Neil\u2019s blog is usually more oriented about the business side of SEO as well, \nwhich is certainly an interesting point of view, but not really what I want to achieve with DeX. \nOverall the advice from neil is doable and accessible, but it would not be my go-to choice.  \n \n\n  \n\n7 \n\n\n\nHow to test whether your SEO is functional? \n\nGoogle analytics \n \nThe most prominent one being Google Analytics, which gives you extensive information into your \nvisitors like: bounce percentage, time spent per page, conversions per page etc. In short, this \nlibrary is ideal for analysing user behaviour on your website.  The problem that google analytics \nbrings with it is the fact that we would also have to update our privacy policy, since google garners \nquite some user data which we can\u2019t do without the users\u2019 permissions. \n \nFurthermore, Google analytics is free and easy to implement/use, if privacy was not a problem \nthen this would be a go-to tool. \n \n\nGoogle Search Console \nThis tool is the little brother of Google Analytics, which is more focussed on SEO. Google Search \nConsole shows the sites from which your visitors came and also shows the amount of \nimpressions you get a day. You can also request indexing of your website, and see where you \nrank on google with each Besides analysing where your visitors are coming from, this tool also \noffers website analytics which shows web pages that aren\u2019t optimised for SEO.  \n \nIn order to implement the tool you only need to confirm that the website is yours in the DNS \nrecords, after which you can access all data that google search console has gathered of your \nwebsite, no payment required.  \n\nSEMRush \nThis is a sophisticated SEO tool, which is probably too overkill for a project like DeX. SEMrush \noffers on-demand SEO audits, analytics of your competitor\u2019s SEO strategies, pre generated \nkeywords and new potential keywords. It has a built in dashboard for your website in which you \ncan see all kinds of automated reports. The \u2018crawlability\u2019, \u2018site performance\u2019, \u2018internal linking ratio\u2019, \nand the \u2018keyword ranking\u2019 are just a few examples from the extensive list of power tools that \nSEMrush has to offer.  \n \nThis tool sounds really powerful and could really well be incorporated in an enterprise \nenvironment. This, however, is not the case for DeX and would probably be over the top for this \nproject.  \n \n \n\n8 \n\n\n\nFinal remarks \n\nKeywords \nOne thing that all of the guides for SEO had in common was the importance of keywords, which is \na topic that I haven\u2019t really touched yet in this research document.  \n \nThe importance of keywords is one which I overlooked often during my research, but it still plays a \nvery prominent role in the increase of SEO. The theory behind it is that once you fill your website \nwith certain \u2018keywords\u2019, crawlers will recognize it and add a \u2018general topic\u2019 to a website. This \ngeneral topic will then also be added to your website. As a result, your website has the potential to \nalso show up when the \u2018general topic\u2019 is searched for.  \n \nThe more keywords you have, the broader your general topics are, the more potential you have to \nappear in the SERPs. Therefore it\u2019s always a good idea to do some research into keywords and \nimplement them on your website.   \n\n9 \n\n\n\nConclusion \nIn conclusion, the Google SEO getting started tutorial is probably the best resource to get started \nwith SEO, in combination with the Google Search Console. This is because Google offers \nentry-level resources with a lot of additional options to go more in-depth into specific subjects. \n \nThe other blogs are definitely as useful if not more, but not for starting out with SEO, since it is too \nspecialized in my opinion.  \n\n10 \n\n\n"
            }]
        },
        "DeXSSRAngularXtoAngularUniversal": {
            "hand-ins": [{
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\nDeX\u00a0\nConversion to\u00a0\nAngular Universal\u00a0\n\n\u00a0\n\n\u00a0\n\n12.11.2020\u00a0\n\nVersion 0.1\u00a0\n\n\u00a0 \u00a0\n\n\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\nVersion History\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0 \u00a0\n\n1\u00a0\n\nVersion\u00a0 Date\u00a0 Author\u00a0 Changes\u00a0\n\nV0.1\u00a0 12/11/2020\u00a0 Niek van Dam\u00a0 Initial document structure\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\nTable of Content\u00a0\n\n\u00a0\n\nVersion History 1\u00a0\n\nTable of Content 2\u00a0\n\n1.Introduction 3\u00a0\n\n2.Pros and Cons to Angular Universal 4\u00a0\n\nPros 4\u00a0\n\nSSR 4\u00a0\n\nResponse time 4\u00a0\n\nCons 5\u00a0\n\nJavaScript 5\u00a0\n\nDOM 5\u00a0\n\nDocumentation 5\u00a0\n\n3. How to convert angularX app to Angular Universal 6\u00a0\n\n4. Conclusion 7\u00a0\n\n\u00a0 \u00a0\n\n2\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\n1.Introduction\u00a0\n\nThis research document has been made after a discussion with some people within DeX, to see\u00a0\n\nwhether or not the updating of AngularX to Angular Universal would positively improve the\u00a0\n\nproject. The pros of Angular Universal are mostly SEO related, which is why I decided to pick up\u00a0\n\nthis research document. Firstly I am going over the pros of converting from AngularX to Angular\u00a0\n\nUniversal, and afterwards I will go over some cons which appear when converting to Universal.\u00a0\u00a0\n\n\u00a0 \u00a0\n\n3\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\n2.Pros and Cons to Angular Universal\u00a0\n\nPros\u00a0\n\nFirst we will list all the pros of converting from Angular to Angular Universal\u00a0\n\nSSR\u00a0\n\nThe main pro to using Angular Universal over Angular is the support for SSR(\u200bS\u200berver \u200bS\u200bide\u00a0\nR\u200bendering). This gives us the ability to improve SEO, by returning a fully rendered HTML page to\u00a0\nthe WebCrawlers which can be instantly read.\u00a0\u00a0\n\nBecause the content is rendered on Server Side, we are able to manipulate the website with\u00a0\n\njavascript before it is actually crawled by bots. This enables us to create dynamic OG and meta\u00a0\n\ntags, in comparison to default angular, where the javascript is only executed if the crawler\u00a0\n\nsupports it. This way we can ensure that every netizen gets the same page served to them.\u00a0\u00a0\n\n\u00a0\n\nResponse time\u00a0\n\nThis goes hand in hand with SSR, the response time of Angular Universal is faster because of the\u00a0\n\nway it handles loading webpages. Angular Universal sends the information in \u2018stages\u2019, it loads\u00a0\n\nHTML/CSS first, which shows the user a basic \u2018shell\u2019. After this is sent, it starts sending the\u00a0\n\nJavaScript. This way the user gets a seamless instant loading page, as long as they do not\u00a0\n\ninteract with any buttons with javascript handlers in the first second of loading as this will not be\u00a0\n\nloaded at that time.\u00a0\u00a0\n\nThe faster loading time also improves site traffic, as 53% of all mobile site visits are abandoned if\u00a0\n\npages take over three seconds to load. As of right now, the loading time is 2.3 seconds\u00a0\n\naccording to LightHouse reports.  The loading time will be reduced when using Universal, as\u00a0\n\neverything is done by the server which removes the slow loading time for users with bad\u00a0\n\nhardware.\u00a0\u00a0 \u00a0\n\n4\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\nCons\u00a0\n\nOf course, not all that glitters is gold, which is also the case for Angular Universal. We will now be\u00a0\n\ndiscussing the Cons that appear when switching from Angular to Angular Universal\u00a0\u00a0\n\nJavaScript\u00a0\n\nAngular Universal generates a static version of web apps when loading, called the \u2018shell\u2019.As I said\u00a0\n\nbefore, this shell appears once the HTML/CSS is done loading, after which it starts loading the\u00a0\n\nJavaScript of the web page. This usually is not a big deal, unless when working with big\u00a0\n\nJavaScript files, since the user will have to wait until the javascript is finished loading in order to\u00a0\n\nactually have a functioning webpage. One way to tackle this problem is by implementing lazy\u00a0\n\nloading. This is still an inconvenient way of getting things done with javascript.\u00a0\u00a0\n\nDOM\u00a0\n\nBecause Angular Universal generates a static webpage, the DOM (\u200bD\u200bocument \u200bO\u200bbject \u200bM\u200bodel) will\u00a0\nnot work as expected. As a result, DOM manipulation becomes a tricky situation, e.g. setting\u00a0\n\nclasses of elements or updating ID\u2019s become unavailable to you. According to angular\u00a0\n\nconventions, you are not recommended to be working with the DOM regardless. As long as this\u00a0\n\nis already the case, there is nothing to worry about with the DOM.\u00a0\u00a0\n\nBut there is a pitfall to this, even if you are compliant to all the conventions, you can still get\u00a0\n\nscrewed over by this constraint. The developer might not be using the DOM, but libraries sure\u00a0\n\nwill. A big load of libraries are currently working with DOM manipulation. Many libraries have\u00a0\n\nbeen working on this issue since SSR came out, but there are still some libraries that do not\u00a0\n\nsupport \u2018out of the box\u2019 functionality.  This issue only applies to libraries which offer direct\u00a0\n\ncomponents or directives. Libraries with business logic should have no issues with functioning as\u00a0\n\nexpected.\u00a0\u00a0\n\nDocumentation\u00a0\n\nThe documentation that I could find for Angular is limited, there are some basic tutorials. Besides\u00a0\n\nthis there were little to no big comprehensive guides or documentations which you can use as a\u00a0\n\nreference point. If you stumble into any exceptions while working you are on your own.\u00a0\u00a0\n\n\u00a0 \u00a0\n\n5\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\n3. Conclusion\u00a0\n\nAfter trying to convert the AngularX project to Angular Universal, I found out that this is definitely\u00a0\n\nnot a worthwhile conversion. I tried to use a few different tutorials, but none of them seemed to\u00a0\n\nwork as expected. When I finally did manage to convert it to Angular Universal, I was presented\u00a0\n\nwith a broad array of broken libraries. I tried fixing these libraries, but most of the issues were too\u00a0\n\ncomplex to solve in a matter of hours and would require us to rewrite parts of certain libraries.\u00a0\u00a0\n\nIn summary, it is better to stay with AngularX and not convert to Angular universal as it would\u00a0\n\nheavily impact the project in a negative way.\u00a0\n\n6\u00a0\n\n\n"
            }]
        },
        "EerstewerkdagEvaluatie": {
            "hand-ins": [{
                "text": "Evaluatie en Reflectie:  les geven\n\nInleiding\nNa twee jaar geleden afgestudeerd te zijn aan het Scalda College voor Techniek en Design in Vlissingen ben ik door mijn oude opleidingshoofd (Bram)  van Software Development teruggevraagd om studenten te begeleiden als onderwijsassistent. Binnen de opleiding zitten 3 klassen, waarvan klassen 2 en 3 bezig zijn met groepsprojecten en het maken van webapps. Jaar 1 is nog bezig het met exploreren van software development en databases.\nOrigineel was het idee om online begeleiding te geven, dit mocht dankzij de nieuwe maatregelen ook op locatie. \n\nEvaluatie\nMijn eerste dag als onderwijsassistent was een leuke en interessante ervaring, in het algemeen beter dan verwacht. \nDe dag begon met een stand-up, waar ik een aantal groepen moest overzien om te kijken of de stand-up volgens plan ging, en of er geen problemen waren. Na de stand-ups ben ik bij de groepen langsgegaan waar problemen werden besproken tijdens de stand-ups en heb ik gekeken of er behoefte was aan extra uitleg/hulp.  \nDe verdere invulling van de ochtenduren bestond voornamelijk uit het sparren met studenten over waarom bepaalde implementaties wel/niet handig zouden zijn binnen het project. Na bij een handvol aan studenten rond te zijn gegaan kwam ik er wachter dat de leesbaarheid van de code laag lag, waardoor ze vaak ook zelf verward waren over hun code. Dit was goed te merken als je vragen stelde over de locatie van sommige variabelen en methoden. Toen ik dit zag heb ik het besproken met Bram en gevraagd of er profijt kon worden gehaald uit een kleine workshop over clean code schrijven. \nZelf leek het mij leuk en leerzaam om een workshop te geven, wat een van de redenen was voor dit aanbod. Ook de docenten waren blij met dit aanbod en gaven mij een half uur om kort iets voor te bereiden. In dit half uur heb ik snel een kleine presentatie bij elkaar gegooid over code smells en het maken van clean code, de overige tijd heb ik gevuld met het uitleggen seperation of concerns.  Aan het einde van de presentatie heb ik veel positieve reacties ontvangen, wat mij motiveert om vaker in de toekomst zulke kleine workshops te geven indien nodig. \nDe middag heb ik besteed aan het helpen van eerstejaars, zij bezig waren met het vak Databases. Ze waren vooral bezig met het exploreren van `INNER JOIN` statements, waar ik samen met ze een handvol aan opdrachten ben langsgelopen. In het begin legde ik zelf de vraag uit en ging dan met de studenten de approach ontleden. Het doel was dat ik de eerstejaars zelf het verband liet leggen tussen het gebruiken van bepaalde syntax en bepaalde problemen.  \nIk heb de dag enorm positief ervaren en heb veel leuke feedback ontvangen over mijn manier van lesgeven/uitleggen. Dit heeft mij enorm gemotiveerd om het volgende week weer te doen. \n\nReflectie\nTijdens het lesgeven heb ik wel een paar keer het idee gehad dat ik erg chaotisch bezig was. Zo vergat ik om terug te komen naar bepaalde studenten zoals ik had gezegd, en was ik i.p.v. iets aan het uitzoeken voor 1 student opeens weer een andere student aan het helpen. Dit is iets waar ik in de toekomst beter op zou moeten letten om het voor iedereen soepel te laten lopen. \nNa een kleine brainstorm over hoe ik dit zou kunnen oplossen kwam ik op het idee om de volgende keer een klein lijstje te maken met namen van studenten waar ik nog langs moet. Als ik langs een student ben geweest kan ik deze dan weer afstrepen en kan ik door naar de volgende. \n\nConclusie\nAan het einde van de dag kan ik heel positief terugkijken op hoe mijn eerste werkdag is verlopen en ben ik enthousiast om  volgende week weer terug te gaan. Ik vind het fijn dat de studenten open staan voor mijn input en er wederzijds respect is, wat een goede werksfeer maakt. Wel heb ik gemerkt dat ik mijn taken wel goed op orde moet hebben zodat andere hier niet de dupe van worden. \n"
            }]
        },
        "JetsonNano2GBfoutmeldingen": {
            "hand-ins": [{
                "text": ["---------- Tools/environment_install/install-prereqs-ubuntu.sh start ----------\n", "Get:1 file:/var/cuda-repo-10-2-local-10.2.89  InRelease\n", "Ign:1 file:/var/cuda-repo-10-2-local-10.2.89  InRelease\n", "Get:2 file:/var/visionworks-repo  InRelease\n", "Ign:2 file:/var/visionworks-repo  InRelease\n", "Get:3 file:/var/visionworks-sfm-repo  InRelease\n", "Ign:3 file:/var/visionworks-sfm-repo  InRelease\n", "Get:4 file:/var/visionworks-tracking-repo  InRelease\n", "Ign:4 file:/var/visionworks-tracking-repo  InRelease\n", "Get:5 file:/var/cuda-repo-10-2-local-10.2.89  Release [574 B]\n", "Get:6 file:/var/visionworks-repo  Release [2001 B]\n", "Get:7 file:/var/visionworks-sfm-repo  Release [2005 B]\n", "Get:8 file:/var/visionworks-tracking-repo  Release [2010 B]\n", "Get:5 file:/var/cuda-repo-10-2-local-10.2.89  Release [574 B]\n", "Get:6 file:/var/visionworks-repo  Release [2001 B]\n", "Get:7 file:/var/visionworks-sfm-repo  Release [2005 B]\n", "Get:8 file:/var/visionworks-tracking-repo  Release [2010 B]\n", "Hit:9 http://ports.ubuntu.com/ubuntu-ports bionic InRelease\n", "Hit:10 http://ports.ubuntu.com/ubuntu-ports bionic-updates InRelease\n", "Hit:11 http://ports.ubuntu.com/ubuntu-ports bionic-backports InRelease\n", "Hit:12 https://repo.download.nvidia.com/jetson/common r32.5 InRelease\n", "Hit:13 http://ports.ubuntu.com/ubuntu-ports bionic-security InRelease\n", "Hit:14 https://repo.download.nvidia.com/jetson/t210 r32.5 InRelease\n", "Reading package lists...\n", "1\n", "##############################################\n", "Add user to dialout group to allow managing serial ports\n", "##############################################\n", "Done!\n", "python-wxgtk3.0 - Python interface to the wxWidgets Cross-platform C++ GUI toolkit\n", "python-wxgtk3.0-dev - Development files for wxPython\n", "Reading package lists...\n", "Building dependency tree...\n", "Reading state information...\n", "build-essential is already the newest version (12.4ubuntu1).\n", "ccache is already the newest version (3.4.1-1).\n", "gawk is already the newest version (1:4.1.4+dfsg-1build1).\n", "libtool is already the newest version (2.4.6-2).\n", "libtool-bin is already the newest version (2.4.6-2).\n", "make is already the newest version (4.1-9.1ubuntu1).\n", "python-dev is already the newest version (2.7.15~rc1-1).\n", "python-numpy is already the newest version (1:1.13.3-2ubuntu1).\n", "python-pyparsing is already the newest version (2.2.0+dfsg1-2).\n", "python-serial is already the newest version (3.4-2).\n", "python-setuptools is already the newest version (39.0.1-2).\n", "python-yaml is already the newest version (3.12-1build2).\n", "gcovr is already the newest version (3.4-1).\n", "lcov is already the newest version (1.13-3).\n", "libcsfml-audio2.4 is already the newest version (2.4-2).\n", "libcsfml-dev is already the newest version (2.4-2).\n", "libcsfml-graphics2.4 is already the newest version (2.4-2).\n", "libcsfml-network2.4 is already the newest version (2.4-2).\n", "libcsfml-system2.4 is already the newest version (2.4-2).\n", "libcsfml-window2.4 is already the newest version (2.4-2).\n", "libsfml-audio2.4 is already the newest version (2.4.2+dfsg-4).\n", "libsfml-dev is already the newest version (2.4.2+dfsg-4).\n", "libsfml-graphics2.4 is already the newest version (2.4.2+dfsg-4).\n", "libsfml-network2.4 is already the newest version (2.4.2+dfsg-4).\n", "libsfml-system2.4 is already the newest version (2.4.2+dfsg-4).\n", "libsfml-window2.4 is already the newest version (2.4.2+dfsg-4).\n", "python-matplotlib is already the newest version (2.1.1-2ubuntu3).\n", "python-scipy is already the newest version (0.19.1-2ubuntu1).\n", "python-wxgtk3.0 is already the newest version (3.0.2.0+dfsg-7).\n", "g++ is already the newest version (4:7.4.0-1ubuntu2.3).\n", "g++-arm-linux-gnueabihf is already the newest version (4:7.4.0-1ubuntu2.3).\n", "git is already the newest version (1:2.17.1-1ubuntu0.8).\n", "libpython2.7-stdlib is already the newest version (2.7.17-1~18.04ubuntu1.6).\n", "libxml2-dev is already the newest version (2.9.4+dfsg1-6.1ubuntu1.3).\n", "libxslt1-dev is already the newest version (1.1.29-5ubuntu0.2).\n", "python-psutil is already the newest version (5.4.2-1ubuntu0.1).\n", "wget is already the newest version (1.19.4-1ubuntu2.2).\n", "pkg-config-arm-linux-gnueabihf is already the newest version (4:7.4.0-1ubuntu2.3).\n", "python-opencv is already the newest version (3.2.0+dfsg-4ubuntu0.1).\n", "python-pip is already the newest version (9.0.1-2.3~ubuntu1.18.04.4).\n", "xterm is already the newest version (330-1ubuntu2.2).\n", "The following packages were automatically installed and are no longer required:\n", "  apt-clone archdetect-deb bogl-bterm busybox-static cryptsetup-bin\n", "  dpkg-repack gir1.2-timezonemap-1.0 gir1.2-xkl-1.0 grub-common\n", "  kde-window-manager kinit kio kpackagetool5 kwayland-data kwin-common\n", "  kwin-data kwin-x11 libdebian-installer4 libkdecorations2-5v5\n", "  libkdecorations2private5v5 libkf5activities5 libkf5attica5\n", "  libkf5completion-data libkf5completion5 libkf5declarative-data\n", "  libkf5declarative5 libkf5doctools5 libkf5globalaccel-data libkf5globalaccel5\n", "  libkf5globalaccelprivate5 libkf5idletime5 libkf5jobwidgets-data\n", "  libkf5jobwidgets5 libkf5kcmutils-data libkf5kcmutils5 libkf5kiocore5\n", "  libkf5kiontlm5 libkf5kiowidgets5 libkf5newstuff-data libkf5newstuff5\n", "  libkf5newstuffcore5 libkf5package-data libkf5package5 libkf5plasma5\n", "  libkf5quickaddons5 libkf5solid5 libkf5solid5-data libkf5sonnet5-data\n", "  libkf5sonnetcore5 libkf5sonnetui5 libkf5textwidgets-data libkf5textwidgets5\n", "  libkf5waylandclient5 libkf5waylandserver5 libkf5xmlgui-bin libkf5xmlgui-data\n", "  libkf5xmlgui5 libkscreenlocker5 libkwin4-effect-builtins1 libkwineffects11\n", "  libkwinglutils11 libkwinxrenderutils11 libqgsttools-p1 libqt5designer5\n", "  libqt5help5 libqt5multimedia5 libqt5multimedia5-plugins\n", "  libqt5multimediaquick-p5 libqt5multimediawidgets5 libqt5opengl5\n", "  libqt5quickwidgets5 libqt5sql5 libqt5test5 libxcb-composite0 libxcb-cursor0\n", "  libxcb-damage0 os-prober python3-dbus.mainloop.pyqt5 python3-icu python3-pam\n", "  python3-pyqt5 python3-pyqt5.qtsvg python3-pyqt5.qtwebkit python3-sip\n", "  qml-module-org-kde-kquickcontrolsaddons qml-module-qtmultimedia\n", "  qml-module-qtquick2 rdate tasksel tasksel-data\n", "Use 'sudo apt autoremove' to remove them.\n", "0 upgraded, 0 newly installed, 0 to remove and 157 not upgraded.\n", "Collecting future\n", "  Using cached https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz\n", "Collecting lxml\n", "  Using cached https://files.pythonhosted.org/packages/e5/21/a2e4517e3d216f0051687eea3d3317557bde68736f038a3b105ac3809247/lxml-4.6.3.tar.gz\n", "Collecting pymavlink\n", "  Using cached https://files.pythonhosted.org/packages/1b/59/34e07d7049e0cded0e7ffbae3e44c36d4a182c85c2655dea5ca4cf1d9d9a/pymavlink-2.4.14.tar.gz\n", "Collecting MAVProxy\n", "  Using cached https://files.pythonhosted.org/packages/40/e5/827562a3f2db94621ecd375edd79da367ec8a7740993087d8ccb3da7ecc2/MAVProxy-1.8.34.tar.gz\n", "Collecting pexpect\n", "  Using cached https://files.pythonhosted.org/packages/39/7b/88dbb785881c28a102619d46423cb853b46dbccc70d3ac362d99773a78ce/pexpect-4.8.0-py2.py3-none-any.whl\n", "Collecting flake8\n", "  Downloading https://files.pythonhosted.org/packages/a0/b0/3b5820728d687f2c000476216a3fccc7a03baac1034afc0284ccde25e26d/flake8-3.9.1-py2.py3-none-any.whl (73kB)\n", "Collecting pygame\n", "  Using cached https://files.pythonhosted.org/packages/c7/b8/06e02c7cca7aec915839927a9aa19f749ac17a3d2bb2610b945d2de0aa96/pygame-2.0.1.tar.gz\n", "    Complete output from command python setup.py egg_info:\n", "    \n", "    \n", "    WARNING, No \"Setup\" File Exists, Running \"buildconfig/config.py\"\n", "    Using UNIX configuration...\n", "    \n", "    sh: 1: sdl2-config: not found\n", "    sh: 1: sdl2-config: not found\n", "    sh: 1: sdl2-config: not found\n", "    \n", "    Hunting dependencies...\n", "    \n", "    ---\n", "    For help with compilation see:\n", "        https://www.pygame.org/wiki/CompileUbuntu\n", "    To contribute to pygame development see:\n", "        https://www.pygame.org/contribute.html\n", "    ---\n", "    \n", "    Traceback (most recent call last):\n", "      File \"<string>\", line 1, in <module>\n", "      File \"/tmp/pip-build-Ddcj_7/pygame/setup.py\", line 318, in <module>\n", "        buildconfig.config.main(AUTO_CONFIG)\n", "      File \"buildconfig/config.py\", line 221, in main\n", "        deps = CFG.main(**kwds)\n", "      File \"buildconfig/config_unix.py\", line 194, in main\n", "        DependencyProg('SDL', 'SDL_CONFIG', 'sdl2-config', '2.0', ['sdl']),\n", "      File \"buildconfig/config_unix.py\", line 39, in __init__\n", "        self.ver = config[0].strip()\n", "    IndexError: list index out of range\n", "    \n", "    ----------------------------------------"]
            }]
        },
        "ORProjectstructure": {
            "hand-ins": [null]
        },
        "PoCObjectDetection": {
            "hand-ins": []
        },
        "PoCTextgrading": {
            "hand-ins": []
        },
        "ResearchNavigationmethods": {
            "hand-ins": [{
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDrone_brainstorm\n\n\n------------------------------------brainstorm-Sieuwe------------------------------------------------------------\n\nProof of concept\nTo prove the hardware pipeline from sensor input to motor control a proof of concept can be\nmade. This proof of concept will also be a practise on working with the different modules.\nThe proof of concept is to create an algorithm that can autonomously follow a moving\nperson. To keep the proof of concept simple the drone does not need to take into account\nobstacles. The drone only has to keep following a person in an open area.\n\nA possible approach for this can consist of the following pipeline.\n\nHardware\nA RGB camera connected with a csi interface to the jetson nano\nA solid state lidar (tf luna) connected to the Jetson nano with i2c\nThese two sensors have been calibrated so that the 5*5cm square area of the solid state\nlidar is in the middle of the RGB camera feed at a distance of 5m.\nThe jetson nano is connected to the pixhawk using a usb interface.\nThe drone actuators will be controlled by the pixhawk.\n\nSoftware\nThe jetson nano will run a python script which follows the following steps:\n1- Using jetson utils library a mobilenet model will produce bounding boxes at 20fps. Only\nbounding boxes containing people will be passed to SORT.\n2- A tracker like SORT will track bounding box id\u2019s between frames.\n\nDuring each tracker update (also 20 times a second) the jetson nano will:\n1- Calculate the centerpoint of the bounding box with ID=1 given by SORT. (other\nbounding boxes will be ignored)\n2- Use the centerpoint of the incoming image to calculate the distance between the\ncenterpoint and the bounding box centerpoint. This distance will feeded into either a\nPID controller if possible or else a deque of 10. The average of this deque is than\ncalculated and a factor is applied to create two values ranging from -1 to 1. The first\nvalue is the X axis and the second value is the Z axis.\n3- To generate the Y axis (depth), the lidar sensor is used. However the data from the\nlidar sensor is only used when the centerpoint of the image is in a small area around\nthe centerpoint of the bounding box. This is to stop the lidar from getting a distance\nother than distance between the drone and the person. This distance is fed inside a\nseparate PID controller or deque like explained above to keep the distance 5m.\n4- all values coming from the 3 PID controllers, for x, y and z, ranging -1 to 1 will be\nconverted into usable commands for the dronekit library. Using the dronekit library\nthe commands are send to the pixhawk. The pixhawk will in its turn update the\nmotors.\n\nThe following steps are performed 20 times a second to create smooth motion.\n\nIf the drone loses track of the person the drone will hover in its place for 20 seconds. If in the\ntime SORT gives a new bounding box then the script starts following this new boundingbox.\nOtherwise the drone will start beeping with the internal buzzer. After another 20 seconds the\ndrone will land automatically if the user has not taken control.\n\n\n\nLimitations to this approach are that the drone can follow persons good in straight lines.\nForward backwards left right. But if the persons starts walking in for example a circle then\nthe drone will lose track since there is no yaw control. Only pitch (X axis), height (Z axis) and\nroll (Y axis) are controller. However using the X and Z axis a possible Yaw can be calculated.\n\n\n\nFull navigation\n\nDirections\nWhen looking at full navigation for an autonomous quadcopter, there are 4 axis of movement\nwith 7 directions combined\n\n1- up and down\n2- forward or backward (roll)\n3- left right (pitch)\n4- rotate (yaw)\n\n3 of these four actions can when performed hit an obstacle. Yaw does not since it is done in\nplace. Only when dynamic objects are in the area, it could hit the drone.\n\nFor each movement direction the drone has to check whether it is possible or not without\nhitting any obstacles. One direct possibility would be to mount sensors on each side of the\ndrone to account for obstacles in all movable directions. Some directions are however\nperformed more during flight than others. And to keep the system as simple as possible, this\nis not a beneficial approach. For example backwards is something that could be eliminated\nas movement because the drone could just rotate 180 degree to make a the backwards\nmovement just a forward movement.\n\nThis is one of the main ideas of this navigation approach. While the drone is capable of\nmoving in all directions, making the drone movible on only the necessary directions to reach\nall possible locations will make the sensor package easier to use.\n\nThese directions are:\n1 - up and down, to fly over or under obstacles\n2 - forward, to have a horizontal movement\n3- yaw, to make the drone change its heading.\n\nThis will make the drone slower since it has to adjust its yaw frequently. But this does make\nthe navigation task easier since there are only 4 possible directions instead of 7.\n\nVision\nTo check if movement is possible concerning obstacles, it is important that the drone\n\nEnd to End full navigation approach.\nWhen looking at\n\n\n\nBrainstorm Niek\n\nPoC - Object detection\nIn order to determine whether or not the drone can properly recognise all possible\nobstacles/threats while flying, we can make the drone fly a predefined path with already\nknown obstacles. Once the flight is done, we can analyse the list of detected objects the\ndrone has seen on its way, and validate whether or not the object detection script detects all\nnecessary obstacles.\n\nHardware requirements\n- RGB camera\n\n- Used for object detection\n- Jetson nano\n\n- Used to run the object detection script on\n- Pixhawk\n\n- Flight controller for the drone\n\nSoftware requirements\nThe software required for this PoC consists mainly of the object detection script to be\nexecuted on the jetson nano, as the GPS coordinates can already be set using the dronekit.\n\nThe flowchart of this system would look as follows, assuming you have already mapped out\nthe course which the drone is going to fly. As well as having a list of all objects which the\ndrone should detect.\n\n1. Start image detection algorithm on drone\n2. Send coordinates through dronekit\n3. Let the drone fly to location x on the map\n4. List all objects which are detected along the way\n5. Save all detected objects to a local file\n6. Compare detected objects to the actual objects\n\nWhy\nThe main reason why I think this is an important PoC to realise is because of the fact that\nthe follow-up actions of the drone will most likely depend on the detected objects within the\ntracks. If the objects detected along its way are not really what it should be detecting we\ncould expect to see the drone execute redundant/unusual maneuvers, which heightens the\nchance of crashing.\n\n\n\nPoC - Simulation\nWhen flying the drone outside while working on PoC\u2019s, there is a real chance that the drone\nmay crash, resulting in damage. To counter this we can attempt to simulate the drone in a\ncontrolled environment first like: a simulated environment.\n\nWithin this simulated environment we can control the amount of detectable objects, as wel\n\nSoftware Requirements\nThe software requirements for this PoC require only one thing, which is the software on\nwhich we can run the simulation. Some additional research still has to be done regarding\nwhether or not we are using Unity or a custom made solution like this quadcopter controller\n\nWhy\nWhen simulating a drone in a safe environment like Unity, we lose the risk of the drone\ncrashing, and increase the rate at which we can test approaches. When having to implement\na new algorithm onto the drone, taking it outside and testing physically, it can take 15-20\nminutes, whereas simulating this would take up to 5.\n\nhttps://github.com/simeonradivoev/Quadcopter-Controller\n\n\nPOC - Decision making (Bas)\nIn order to make correct and reliable decisions we need a Algorithm that controls the\nmovements of the drone based on input. The main objective of this POC being: training an\nalgorithm to make its own decisions not based on a pre set course. Proving the POC could\nbe done by analysing the flight path over a preset course and observing the drones decision\nmaking.\n\nAlgorithm specifics\n\nThe chosen algorithm would be Deep Reinforcement Learning (preferably we would like to\nstart training in a simulated environment being for example a Unity3D sim. The drone would\nbe rewarded points whenever the drone comes closer to the predefined target. Everytime\nthe drone \u201ccrashes\u201d points would be detracted. A crash would be simulated by sonars which\nwould intervene whenever the drone would be actually close to crashing.\n\nHardware requirements\n\n\u25cf RGB Camera\n\u25cf Jetson Nano\n\u25cf The drone actuators will be controlled by the pixhawk\n\u25cf 4 Sonars TBD.\n\nSoftware requirements\n\nThe software requirements for this POC would be the followup of Niek\u2019s POC meaning most\nof the software components remain the same.\n\n\u25cf Start image detection algorithm on drone\n\u25cf Send coordinates through dronekit\n\u25cf Decision making algorithm to fly as fast as possible to the provided coordinates\n\u25cf Sonar intervenes when the sonar detects an object at a forward distance off 30 cm or\n\na z-axis detection at a distance off 15 cm ( deducting points ).\n\u25cf Let the drone fly to location x on the map\n\n\n\nBrainStorm\n- eroverheen eromheen wanneer?\n- hoe bepaal je de ideale vlieghoogte als die niet defined it.\n- 3D bounding boxes annotations\n- Controlling the drone with quick object detection + reinforcement learning model.\n\nObject det.\n\n- YOLO\n\nStereo vision\n- https://www.sciencedirect.com/science/article/pii/S2590005620300011 Focused op\n\nhet object voor zich.\n\navg mean\nhttps://stackoverflow.com/questions/13728392/moving-average-or-running-mean\n\npid\nhttps://pypi.org/project/simple-pid/\n\nhttps://www.sciencedirect.com/science/article/pii/S2590005620300011\nhttps://stackoverflow.com/questions/13728392/moving-average-or-running-mean\n\n"
            }]
        },
        "ResearchProficiencyanalysis": {
            "hand-ins": [{
                "text": "Version Control\n\n\n\n\n\nIntroduction\nWithin the Quantified Student project, we thought of a new feature where we would analyse the written text from the students and detect whether their proficiency level in that language is up to standards. Before we can go into creating PoCs about this new feature, we first must research whether this is even feasible. \nThe research will be conducted by incorporating Literature study as main research method. \nResearch questions\nThe main question I want to get answered in this research is the following:\nIs it possible to analyse someone\u2019s language level based using NLP?\nIs the approach that is proposed feasible?\nIs the approach able to be implemented for the target languages (Dutch and English)?\n\n\nPaper 1:\n\nPredicting proficiency levels in learner writings by transferring a linguistic complexity model from expert-written coursebooks | \n\nThis paper covers the measuring of proficiency levels by comparing the linguistic complexity to that of a textbook at the given level. The language used in the textbooks is recorded per proficiency level, measured by the CEFR (Common European Framework of Reference for languages). This level goes from A to C, respectively being ranked from basic to proficient user. These grades can further be subdivided according to the needs of the local context. The idea behind this research method is that the model can read the input data from the student, normalize it to its basal form and compare the formatted text to the proficiency level.\nThe focus within this research paper was put on the correcting and normalizing of grammar mistakes, which led to greater performance within the network. This is not something which I think we should be focussing on right now, as this is supposed to be a secondary feature, not something which we should direct all our time and resources towards. However, it is good to keep in mind that these things can still improve performance within NLP. \nWhen asking the question whether this is feasible to implement into our own systems, I would have to say no. There are multiple reasons for this, the main one being that this method takes up a lot of resources and time to properly implement, which seems over-the-top for a small feature. Besides, this application has only been tested on one language (Swedish), which has an easier grammar system than the languages we would have to implement this software into. In our implementation, we would have to teach the network two languages (assuming people fill in their FeedPulse in either English or Dutch), this adds another dimension of difficulty as these languages have a more complex grammar system than the language used in the paper. \nIn conclusion, I do not think this is a viable approach to use, however, it did explain a lot about the data cleaning process going on behind the scenes in NLP. \n\n\nAlternative methods to analysing text proficiency\nDuring my research I saw a lot of instances where the NLP model used was way over the top for our implementation, which, although interesting, did not meet the requirements needed (mostly since the approach was not feasible and too avant-garde). There were some more basic approaches which were covered throughout my research, which seemed more in-line with what we are trying to achieve. The most promising methods have been noted down below.\n\nAnalysing proficiency based on grammar mistakes\nThe analysing of proficiency based on grammar mistakes is a more feasible goal than the methods which are proposed in the research papers. Mostly because the analysing of grammar mistakes is a lot easier to do and does not require the same amount of preparation to be done beforehand. One of the easiest ways to implement this metric is by calculating the average rate of grammar mistakes per sentence. We can measure these against a predefined scale, which can later be given back to the student as feedback. \nThe reason this is easier to implement than one of these research papers is because we can incorporate pre-existing software for this, and do not require us to build our own solution from scratch.\n\nCalculating the ARI (Automated readability index)\nThe readability index is a simple but powerful formula which rates the readability of the given text. The formula only requires basic metrics, which can easily be extracted from the data. The formula is as follows:\n4.71 (\nWhere c is the number of letters and numbers, w is the number of spaces, and s is the number of sentences (not necessarily the number of periods present in a piece of text). \nThis formula outputs a number between 1-14, which can be used to calculate the grade level at which the text has been written. The scores can be a decimal number, in which case they will be rounded up; causing a 10.1 to be interpreted as an 11. The meaning of the scores is in the table to the left, taken from the  regarding ARI.\nThe ARI looks like a good metric to use to determine proficiency of the writer, without having to train a model to predict it for us. Since this is only a basic formula, it can more easily be implemented into the system, requiring basic text analysis at best. \n\n\nGunning fog index\nIn linguistics, the Gunning fog Index is a readability formula for English writing which estimates the years of formal education needed to understand the text on first reading. This method has been developed in 1952 by Robert Gunning and has been used since. The fog index is commonly used to confirm that text can be read easily by the intended audience and can also be used to indicate the level (proficiency) of your writing skills. \nThe Gunning fog index is calculated with the following formula: \n\nIn comparison to the ARI, we do not round this number up or down when checking the end score, if the number is a decimal, it should be treated as \u2018in between\u2019 the levels. The results of this index can be seen in the table to the left. \nIn conclusion, this index can be used within our application, besides one critical point. This index is only applicable to the English language, which goes against our target languages (Dutch and English). Therefore, we would only be able to apply this to the English side of canvas. \n\nFlesch-Kincaid grade\nThe Flesch-Kincaid grading system is a widely used readability formula which assesses the approximate reading level of a given text. It has been developed by the US Navy who worked with the Reading Ease formula first but has been converted to the analysis of reading levels instead of reading ease. The obsolete version still exists; however, it is not as clear in its grading as the current formula.\nThe formula uses the same principle as the Gunning fog index but uses different variables and therefore also outputs a different result. The result which this grade gives is more generalised, whereas the fog index is specific enough to classify writer grades. \nThe formula for this grade is as follows: \n \nWhere all variables describe the count of a property within the given text (words -> total words, syllables -> total syllables). This formula results in a number between 0-18 and displays the data in 6 different sections, in value pairs of 3. \nThis grade level has a lot more potential than the Gunning fog index, as it can be widely applied to every language for as far as I can tell. This gives it significant pros over the other linguistical grading formulas.\n\n\ntRANSLATION OF TEXT FOR GRADING PURPOSES\nThe readability indexes discussed above all give a clear and direct grading of the text to be analysed, but has one prominent downfall: the language. The readability indexes are all made with the mindset of grading English texts. In theory these readability indexes can be modified to a desired language, as the majority of these readability indexes solely require the amount of: syllables, words and sentences. Before we can reliably apply this to every language, I decided to validate it myself with the following pieces of text, found on the \u2018\u2019:\n\n\u201cIk ben makelaar in koffi, en woon op de Lauriergracht. Het is mijn gewoonte niet, romans te schrijven, of zulke dingen, en het heeft dan ook lang geduurd, voor ik er toe overging een paar riem \npapier extra te bestellen, en het werk aan te vangen, dat gij, lieve lezer, in de hand hebt genomen, en dat ge lezen moet als ge makelaar in koffie zijt, of als ge wat anders zijt. Niet alleen dat ik nooit iets schreef wat naar een roman geleek, maar ik houd er zelfs niet van, iets dergelijks te lezen.\u201d (Max Havelaar - Multatuli) \n\n\n\n\n\u201cDe volle maan, tragisch dien avond, was reeds vroeg, nog in den laatsten dagschemer opgerezen als een immense, bloedroze bol, vlamde als een zonsondergang laag achter de tamarindeboomen der Lange Laan en steeg, langzaam zich louterende van hare tragische tint, in een vagen hemel op. Een doodsche stilte spande alom als een sluier van zwijgen, of, na de lange middagsi\u00ebsta, de avondrust zonder overgang van leven begon.\u201d  (De Stille Kracht \u2013 Louis Couperus) \n\n\n\n\n\n\u201cOnbegrijpelijk veel mensen hebben familiebetrekkingen, vrienden of kennissen te Amsterdam. Het is een verschijnsel dat ik eenvoudig toeschrijf aan de veelheid der inwoners van die hoofdstad. Ik had er voor een paar jaren nog een verre neef. Waar hij nu is, weet ik niet. Ik geloof dat hij naar de West gegaan is. Misschien heeft de een of ander van mijn lezers hem wel brieven meegegeven. In dat geval hebben zij een nauwgezette, maar onvriendelijke bezorger gehad, als uit de inhoud van deze weinige bladzijden waarschijnlijk duidelijk worden zal.\u201d (Camera Obscura \u2013 Hildebrand)\n\nWhen running these samples through the grading indexes we see that the result from best/worst grading fluctuates by a lot. Besides this we added one additional grade, the Flesch Reading Ease grading, which shows The fact that the data is not uniform in it\u2019s best/worst grading shows me that Dutch is not made for the grading indexes. In order to use the  grading indexes we will have to convert it to a uniform language so that all students will get the same results. \n\n\n\n\nConclusion\nAfter reading a handful of academic papers and researching a wide array of language grading systems I can safely say that the academic papers are above our reach. The methods which are being used are advanced enough to be branched into their own little project, which is over the top for the small feature we are trying to implement.\nThe grading systems and readability indexes, however, show a more promising result. These grading systems are more generic, which respectively, leads to an easier implementation. Besides that, the data which is required to work for these formulas is significantly lower than what would be needed for the NLP approaches. Where the NLP approaches would require several batches of textbook examples, combined with data cleaning; the formulas only require primitive parameters which can be extracted from the text with relative ease. Therefore, I think that we should incorporate the grading systems into this feature, instead of using an NLP solution.\nWhen choosing between the indexes, there is one which significantly stands out above the rest in terms of usefulness in my opinion: the Flesch-Kincaid grade. Where the other tools output a grade, which is specifically focussed on the American schooling system, the Flesch grade can be represented in more \u2018global\u2019 terms of difficulty. In order to make the grades work as expected we will have to find a translation service which will allow us to convert the given text to English, as these algorithms seem to be prominently language-based. \nTo counter the point of being focussed on the American schooling system, we could convert these values and turn them into a more Europeanised value system. This, however, would still give us the issue of these values being \u2018class\u2019 specific. Because it is so specific, it can more easily be prone to flaws of grading a student one rating too high-low, whereas the Flesch score puts you in a broader category.\n\nIn conclusion, I would like to implement the Flesch-Kincaid grading into this system, accompanied by the ARI grades. Due to the constraints which these grading systems put on the language, we do have to translate the to-be analysed text to English . Once converted, these two metrics combined give a clear and concrete overview of the level of writing, without having to write a big amount of software, which makes them ideal for this solution.\n\n\n"
            }]
        },
        "WindPowerCreateprototype": {
            "hand-ins": [{
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlan of attack\n\n\n15-12-2020 \n\n \n  \n\nPlan of attack \nMeasurements wind speed \n\nNiek & Elmira \n\n\n\nPlan of attack  Niek & Elmira \n\n \n1 \n\nIndex \nVersion control ........................................................................................................................................ 1 \n\nSituation ................................................................................................................................................... 2 \n\nThe setup ................................................................................................................................................. 3 \n\nComponents list: .................................................................................................................................. 3 \n\nThe data to be gathered .......................................................................................................................... 4 \n\nPlanning ................................................................................................................................................... 5 \n\n \n\nVersion control \n \n\nVersion Author Date Changes \n\nV0.1 Niek & Elmira 15-12-2020 Setup document \nV0.2 Elmira Drost 21-12-2020 Added the Situation \n\n \n\n\n\nPlan of attack  Niek & Elmira \n\n \n2 \n\nSituation \nSince the last press conference on the 14th of December the Netherlands has declared a country-\n\nwide lockdown. This caused us to be unable to actively work on the wind turbine project as planned. \n\nDue to this series of unfortunate events, we started looking for alternative wind turbine setups \n\nwhich can be worked at from home. After brainstorming with Eric, we came up with a plan to use an \n\nArduino in Elmira\u2019s and Niek\u2019s backyard to simulate a wind turbine.   \n\n\n\nPlan of attack  Niek & Elmira \n\n \n3 \n\nThe setup \nWe looked at two different approaches for this experiment, using two different tutorials. In both \n\ntutorials, wind speed is measured using an anemometer. The main difference in both is that they use \n\na different methodology for measuring the RPM. The RJ-11 meter uses a RJ-11 adapter to export the \n\nwind speed data, whereas the inspeed anemometer uses a power cable, where you have to measure \n\nthe amount of rotations per second manually.   \n\nParts needed RJ-11 Anemometer Parts needed Inspeed Anemometer  \n\nArduino Uno Wifi 47,90 Arduino Uno Wifi 47.90 \nRJ-11 Connector 24,88  0 \nAnemometer RJ-11 25,90  0 \n 0  Inspeed classic \n\nAnemometer \n59,00 \n\n4.7K Resistors 9,61 4.7K Resistors 9,61 \nM2M wires 6,49 M2M wires 6,49 \nPower(bank) source 14,95 Power(bank) source 14,95 \n\nBreadboard 6,50 Breadboard 6,50 \n\nTotal cost 136,23  233,55 \n\n \n\nAfter comparing feasibility and effectivity, we decided to choose the RJ-11 Anemometer, and are \n\ngoing to continue developing the prototype with that. \n\nComponents list: \nThe setup will be the following: \n\n \n\n \n\n\u2022 Arduino Uno WiFi  \n\n\u2022 Anemometer \n\n\u2022 RJ-11 Connector for anemometer \n\n\u2022 4.7K Resistor \n\n\u2022 External 5V power source \n\n  \n\nhttp://forcetronic.blogspot.com/2016/12/measuring-wind-speed-with-anemometer.html\nhttp://cactus.io/hookups/weather/anemometer/davis/hookup-arduino-to-davis-anemometer\nhttps://www.bol.com/nl/p/arduino-uno-wifi-rev2/9200000114847338/\nhttps://www.bol.com/nl/p/arduino-uno-wifi-rev2/9200000114847338/\nhttps://nl.rs-online.com/web/p/chip-programming-adapters/6677889/\nhttps://www.amazon.nl/gp/product/B07BMVYBW9/ref=ox_sc_act_title_1?smid=A2ES8YDVVOD63L&psc=1\nhttps://www.store.inspeed.com/Inspeed-Classic-Anemometer-Sensor-Only-WS.htm\nhttps://www.store.inspeed.com/Inspeed-Classic-Anemometer-Sensor-Only-WS.htm\nhttps://www.amazon.nl/Projects-10EP5124K70uk-4-7k-weerstanden-Pack/dp/B07YNVLCRX/ref=sr_1_2?__mk_nl_NL=%C3%85M%C3%85%C5%BD%C3%95%C3%91&crid=2WF723DXMLXN0&dchild=1&keywords=4.7k+ohm&qid=1608218400&sprefix=4.7%2Caps%2C313&sr=8-2\nhttps://www.amazon.nl/Projects-10EP5124K70uk-4-7k-weerstanden-Pack/dp/B07YNVLCRX/ref=sr_1_2?__mk_nl_NL=%C3%85M%C3%85%C5%BD%C3%95%C3%91&crid=2WF723DXMLXN0&dchild=1&keywords=4.7k+ohm&qid=1608218400&sprefix=4.7%2Caps%2C313&sr=8-2\nhttps://www.bol.com/nl/p/dupont-jumper-kabels-40-stuks-10cm-voor-breadboard-arduino/9200000112732180/?bltgh=lyoMWCofCzeEMT5ANBFBIg.1_28.39.ProductTitle\nhttps://www.bol.com/nl/p/dupont-jumper-kabels-40-stuks-10cm-voor-breadboard-arduino/9200000112732180/?bltgh=lyoMWCofCzeEMT5ANBFBIg.1_28.39.ProductTitle\nhttps://www.bol.com/nl/p/mophie-power-boost-xl-powerbank-10-400-mah-zwart/9200000080445342/?bltgh=peqX4kc0h17VwWWy5CpMDw.r1tPn425oYeScLM8YiRMCg_0_45.49.ProductTitle\nhttps://www.bol.com/nl/p/mophie-power-boost-xl-powerbank-10-400-mah-zwart/9200000080445342/?bltgh=peqX4kc0h17VwWWy5CpMDw.r1tPn425oYeScLM8YiRMCg_0_45.49.ProductTitle\nhttps://www.bol.com/nl/p/breadboard-170-contactpunten-arduino-compatible-wit/9300000008153326/?bltgh=s6mMzyeveRG7KugrjO9GQQ.nsY9ZSFS6Cxau5nA-6tDJA_0_23.30.ProductImage\nhttps://www.bol.com/nl/p/breadboard-170-contactpunten-arduino-compatible-wit/9300000008153326/?bltgh=s6mMzyeveRG7KugrjO9GQQ.nsY9ZSFS6Cxau5nA-6tDJA_0_23.30.ProductImage\n\n\nPlan of attack  Niek & Elmira \n\n \n4 \n\nThe data to be gathered \nThe Arduino will be set up to record the current data every x minutes. Once these x minutes have \n\npassed, we will start the gathering of data. The anemometer will record the current wind speed in \n\nm/s, accompanied by an OpenWeather API call which returns the current weather status. \n\nThe data from the anemometer will be converted to what the power output should have been, had \n\nwe used a TESUP turbine.  We can calculate this output by referencing the spec sheet of the turbine, \n\nwhich shows the output in W (watts) at the given wind speed. This will be stored as the \u2018current \n\npower output\u2019. Once the data collection is done, we will use this current power output as output \n\nvariable in the prediction model. This means that this data will be the \u2018expected output\u2019 of the \n\nprediction model.   \n\nThe OpenWeather API will return a collection of weather data statistics, e.g.: Type of weather, \n\ntemperature, the angle of the wind. This weather data will be used as the prediction parameters, \n\nthese are parameters that the prediction model will use as input to predict the output variables.   \n\n \n\n  \n\n\n\nPlan of attack  Niek & Elmira \n\n \n5 \n\nPlanning \nPlanning for delivery time of the parts: \n\nParts needed RJ-11 Anemometer Delivery time \n\nArduino Uno Wifi 47,90 Circa 2 werkdagen \nRJ-11 Connector 24,88 Circa 2 werkdagen \nAnemometer RJ-11 25,90 Maandag, 21 dec \n -  \n\n4.7K Resistors 9,61 dinsdag, 29 dec \nM2M wires 6,49 21 december \nPower(bank) source 14,95 1 werkdag \n\nBreadboard 6,50 2 werkdagen \n\n \n\nPlanning for setup of the windspeeldcalculator2.0: \n\nActivities Days \nSetup physically 2 days \n\nProgram time Arduino 1 day \n\nProgram time link tesup specsheet + opweather \napi \n\n3 days \n\n \n\nhttp://forcetronic.blogspot.com/2016/12/measuring-wind-speed-with-anemometer.html\nhttps://www.bol.com/nl/p/arduino-uno-wifi-rev2/9200000114847338/\nhttps://nl.rs-online.com/web/p/chip-programming-adapters/6677889/\nhttps://www.amazon.nl/gp/product/B07BMVYBW9/ref=ox_sc_act_title_1?smid=A2ES8YDVVOD63L&psc=1\nhttps://www.amazon.nl/Projects-10EP5124K70uk-4-7k-weerstanden-Pack/dp/B07YNVLCRX/ref=sr_1_2?__mk_nl_NL=%C3%85M%C3%85%C5%BD%C3%95%C3%91&crid=2WF723DXMLXN0&dchild=1&keywords=4.7k+ohm&qid=1608218400&sprefix=4.7%2Caps%2C313&sr=8-2\nhttps://www.bol.com/nl/p/dupont-jumper-kabels-40-stuks-10cm-voor-breadboard-arduino/9200000112732180/?bltgh=lyoMWCofCzeEMT5ANBFBIg.1_28.39.ProductTitle\nhttps://www.bol.com/nl/p/mophie-power-boost-xl-powerbank-10-400-mah-zwart/9200000080445342/?bltgh=peqX4kc0h17VwWWy5CpMDw.r1tPn425oYeScLM8YiRMCg_0_45.49.ProductTitle\nhttps://www.bol.com/nl/p/breadboard-170-contactpunten-arduino-compatible-wit/9300000008153326/?bltgh=s6mMzyeveRG7KugrjO9GQQ.nsY9ZSFS6Cxau5nA-6tDJA_0_23.30.ProductImage\n\n\tVersion control\n\tSituation\n\tThe setup\n\tThe data to be gathered\n\tPlanning\n\n"
            }]
        },
        "WindPowerDatacollectionflowchart": {
            "hand-ins": [{
                "text": "Version Control\n\nTable of Contents\n\n\n\nIntroduction\nAt first, the goal of this semester was to install a wind turbine on location of Strijp-TQ. Due to Covid-19 we are unable to physically the wind turbine as expected. The goal of this wind turbine was to gather data on how much energy is being generated relative to the current wind status. Because of this we had to look for alternative ways to simulate a wind turbine and still gather the required data. \n\nAfter spending some time looking for alternatives, we decided to go for a \u2018diy turbine\u2019. This turbine consists of an Anemometer and an ESP32 WiFi to gather the required data. As we cannot install this on Strijp TQ, we decided to order the items to our homes and set them up there. As I am currently not in Eindhoven, it would be a feasible plan to install 2 unique wind turbine locations to gather more data. This would not have been possible were I to stay in Eindhoven, as that would just result in duplicate data entries. \n\n\nDiagram\nThis diagram shows how the data will be collected from the ESP32. Once the script is loaded, a piece of code will be executed every n (defined at startup) minutes. This code measures the wind speed and converts it to a hypothetical output. This output would have been the actual output in Watts, were the specified wind turbine to be installed. \nAfter getting the current \u2018generated power\u2019, we call the OpenWeather API and get the real time weather data. This way we can create one entry containing the current weather and generated power. This data will then be sent to our API, where it will be merged into a bigger dataset. \nIn case of failure during the transmission of data, we store the variables in a temporary list, from which the call can be executed again at the next iteration. \nWhen the process is done, the script will wait for n minutes, and repeats the previously explained code. \n"
            }]
        },
        "WindPowerDocumentationIntroductiontoproject": {
            "hand-ins": [{
                "text": "Version control\n\n\nIntroduction\nThe goal of the project was to create a reliable wind power prediction model based on historical weather and power data. It was initiated in partnership with a company named OpenRemote. They plan to create two new buildings in Strijp-S, Eindhoven including wind power solution. This means that turbines are going to be installed on the rooftops of these buildings in order to generate power for OpenRemote. If the power generated for a certain time in the future can be predicted based on the weather forecast, the power consumption of the building can be managed so that it relies more or less on the power generated directly from its wind turbines Therefore, it is important to create a precise machine learning model to predict the power generated from a wind turbine.\n\nCurrent situation\nThe first phase of the project was to create a Proof of Concept of the model. After an extensive research what data we need for the wind power prediction model and which algorithm to use to train it, we started building the Proof of Concept with a relatively small demo dataset with weather and power data that we found in Kaggle. After showing it to OpenRemote we concluded that we could start the second phase of the project and work with a real wind turbine.\n\nWind Turbine\nWe need to install a real wind turbine to create a \u201cdynamic\u201d prediction model based on the turbine specifications and real-time data. We have carefully researched and selected a small wind turbine that suits our needs for testing. After talking with OpenRemote about it they gave us green light to purchase the turbine (). It was meant to be installed on the rooftop of the Strijp-TQ building at the beginning of January 2021, but because of the current lockdown, this is not possible at the time of writing this document. However, the building manager of TQ gave us a green light to install it, so in the future, the turbine could be installed, and the testing with a real wind turbine could be started. \nThe data for the power generated from this model of wind turbine could be read with a small module from the company that manufactured it, TESUP. They also provide a cloud solution where all this data could be stored and consumed in the future. This data is an important part of the dataset that is needed to create a precise power prediction model.\nAnemometer\nIn order to gather real-time precise wind data to be used by the power prediction model, we decided that at least one anemometer should be installed close to the location of the Wind Turbine. The anemometer will be connected to an Arduino which will send the wind measurements to an API made to store the data in a database. This data could be used to train the model at first, but later also make real-time predictions based on it.\nAfter a few weeks of data gathering, we plan to have enough data to create a large enough dataset, the training of the model could start.\n\n\nTarget Approach\nThe library we have decided to work with is called Keras, an easy-to-use Machine Learning library, built on top of TensorFlow. It is a beginner friendly library with a lot of readily documentation and examples for implementation. This makes it a good tool to use in order to create a basic working example.\nKeras to Tribuo \nIn the future OpenRemote would like to switch to a Java-based machine learning library (Tribuo). When using java, they can port the program to their own OpenRemote service and use it as one of the \u2018smart sensors\u2019. For testing purposes however, we are still able to use Keras. \nA conversion to Tribuo would not have a big impact on the design, as both libraries support Linear Regression. However, the Tribuo library from Oracle is newer, and therefore has a smaller amount of documentation/community surrounding it. Hypothetically speaking this shouldn\u2019t be a problem, as the documentation is probably up-to-date enough to develop the library without a hassle. When running into any unexpected errors, it would be nice to have a community to speed up the development process.  \nSupervised learning\nThe target approach is to use a machine learning strategy called \u2018Linear Regression\u2019, which is a form of supervised learning. In this case, we train the prediction algorithm on a bunch of different inputs and also give it the output variable. By giving the algorithm the input and expected outcome, it can analyse its own predictions and adjust which parameters to prioritize (also called the weights of the network).  By giving it enough examples of these in- and outputs, the network can finetune the weights in such a way that it will also be able to start accurately predicting output values for input values on which it has not been trained. \nA prominent risk to take into account when programming a linear regression model is under- or overfitting the model. The first occurs when there is too little data, causing the model to create very \u2018broad\u2019 and \u2018unspecific\u2019 predictions. The latter means that the model is trained too often on the data, meaning that it will start to memorize the data, instead of predicting it. This causes the model to overperform on the given model, but broadly underperform on any other model. \n\n\nProof of Concepts\nIn the past, we have tried to approach the problem with a handful of other methods. The findings on those are listed below:\nARIMA\nArima (Auto Regressive Integrated Moving Average) is a type of model that tries to \u2018predict\u2019 the future trend based on its own past values. As weather prediction also comes in time series, we thought that this could be easily translated to an ARIMA model. This went easier than expected but did not yield excellent results. Mostly because the data was not cleaned in a proper way, which made the time series on which the ARIMA model was applied have major inconsistencies. \nBesides this we also found out that it did not help us as you cannot use it to predict any new or \u2018dynamic\u2019 situations, as it is only applied to one dataset. The moment that any outside factor was to change, the model would not be able to pick this up and continue with outdated data. These are reasons why we decided to drop the ARIMA forecasting model and decided to switch to other approaches.\n\nLSTM\nLSTM (Long Short-Term Memory) is a type of Recurrent Neural Network mainly used in deep learning, time series forecasting and text analysis. This type of network is unlike most neural networks, as the previously predicted values can be stored, and used as an additional parameter in the next prediction.  This can be extremely useful in, for example, the generation of pieces of text. In this case, the network will remember the previously outputted word, and can this way make sure that the following word to be predicted matches. \nThis approach can be used in time series predictions as well but requires a different method of creating the X and Y datasets. To prepare the datasets for the training with LSTM, the Y values need to be set back a certain number of steps, called timesteps.  from Machine Learning Mastery covers the use of timesteps well.\nAfter discussing this technique with OpenRemote, they commented on the fact that this method would not be efficient, as it is only able to predict the future of a series. This means that if we have the weather prediction for the coming 5 days, it would only be able to continue predicting from there on out, meaning from day 5+. \n\n\nTraining\nAs we discussed above, we will be training the model with supervised linear regression. We will be using a collection of input variables to train the network. The input variables will be as follows:\nOpenWeather weather prediction \nTemperature\nHumidity\nPressure\nWind speed\nWind degrees\nWeather ID\nThe formula for approximating the wind power output\nP = \u03c0/2 * r\u00b2 * v\u00b3 * \u03c1 * \u03b7\nr = Radius of the blades (m)\nv = Wind speed m/s\n\u03c1 = air density (kg/m3)\n\u03b7 = efficiency factor (%)\nAlthough we will be training this network to predict the outcome for a time series (3-7 days), we are not going to give the neural network an array of n days of weather prediction and expect it to output a series for n days. Instead, we will let it train 1:1 with the OpenWeather prediction. This means that instead of predicting all n days at once in bulk, we are going to predict them one by one. This will result in several different outputs, one for each period for which we have OpenWeather data (probably 3 hours).  \n\nThe network will use these 7 unique values as input variables in the first Dense layer. After this first dense layer, the values will be put through 2 more Dense layers with each 21 nodes (this is still open for modification). We have settled on this as the initial combination, although it can still be modified to see whether more layers/nodes can yield a better result. The only thing to keep in mind, is that the last layer of the network needs only one node, as only one prediction will be made.  As for the loss function of this model, we use MSE (Mean Squared Error) as yields the best result in the case of Linear Regression. \n\n\n"
            }]
        },
        "WindPowerProjectProposal": {
            "hand-ins": [{
                "text": "\n\n\nVersion control\n\n\nProject Assignment\nProblem to solve\nNo accurate prediction models for wind power generation without a subscription.\nThere are already several prediction models for this online, but these are hidden behind a subscription. \nThere are no Edge Gateways within OpenRemote at the moment.\nCheck the potential usage of Tribuo .\n\nProject goals\nSoftware\nA prediction model (preferably in Java) for wind power, based on outside factors like wind speed, direction and configuration. This model should be able to run on a RPI. \nAn easy to use dashboard which displays the wind power prediction \nInfrastructure \nThe ability to create and manage Edge Gateways from a central instance.\n\nProject scope\nWe will be researching the Tribuo library and any other potential libraries as a machine-learning candidate.  \nResearch and create a dataset for the wind power prediction model.\nCreating a dashboard on which the power prediction can be viewed.\nDeveloping a prediction model that will be able to run on an RPI3.\nCreating a solution for deploying edge gateways.\n\nResearch Questions & Sub questions\nHow to predict the generation of wind power?\nWhat type of prediction model would be beneficial?\nWhat ML library would be optimal for predicting wind power generation? (QoL, efficiency, community etc.)\nHow to deploy the OpenRemote software on edge gateways in a manageable fashion.\nHow to deploy?\nHow can it be kept manageable?\n\n\n\nProject Deliverables\nInfrastructure\nA solution for orchestrating deployments from OpenRemote software on edge gateways.\nDocumentation\nConfigurations\nUser install guide\nSoftware\nA prediction model for approximating wind power generation.\nA software solution which has the potential to display the predicted power yield on an OpenRemote dashboard. \nDocumentation\n\n\n\nTeam\nThe team consist of the following members:\nNiek van Dam\nDaan de Weirdt\nElmira Drost \nDaniel Vaswani\nMartin Markov\nInside the team there are 2 subgroups, one is based on the Software side and the other is based on the Infrastructure side.\nSoftware\nThe group of software will be focusing on the Software side of the project. They will be focusing on the prediction of the wind power. The group of Software consist of the following members:\nNiek van Dam\nDaniel Vaswani\nMartin Markov\nThere is 1 group leader in the Software group who is also responsible for the communication between the stakeholders. The group leader is: Niek van Dam.\nThey will be solving the following sub questions for the project:\nHow to predict the generation of wind power?\nWhat type of prediction model would be beneficial?\nWhat ML library would be optimal for predicting wind power generation? (efficiency, community etc.)\nInfrastructure\nThe group of Infrastructure will be focusing on the Infrastructure side of the project. They will be focusing on deploying software on edge gateways. The group of Infrastructure consist of the following members:\nDaan de Weirdt\nElmira Drost\nThere is 1 group leader in the Infrastructure group who is also responsible for the communication between the stakeholders. The group leader is: Daan de Weirdt.\nThey will be solving the following sub questions for the project:\nHow to deploy the OpenRemote software on edge gateways in a manageable fashion.\nHow to deploy?\nHow can it be kept manageable?\n\n \n\nActivities and Planning\nDuring this project, several activities will be undertaken to ensure success. While a more detailed planning will be available in this projects Microsoft Teams environment, a rough timeline will be sketched here. \n\n\nRisks\nIn the following section are listed probable underlying problems (and some probable solutions) that may arise throughout the project.\nGeneral\nCovid-19\nDue to these uncertain times, there is a chance that physical meetings and team meetings can be delayed/cancelled. This would delay the progression of the project. \nSoftware\nSelecting suitable technologies\nWhen choosing technologies, the software development team should be aware of the underlying risk of getting stuck with a technical problem that is not listed in the documentation of the used technology, or is not yet discovered as a bug/implemented feature from its maintainers. In case of a similar problem the development team should choose technologies that are widely used and have large enough community which would eventually be able to help\nWhile it could be tempting to use cutting-edge technology, the development team should take into consideration the risk of using It as the main one for the project.\nSelecting source of data\nWhen selecting data source, the development team should take into consideration the probability of getting misleading data from an external source, no matter how well the service is advertised\nEven though it may seem preferable to set up our sensors for gathering wind data from the buildings, there is always a chance of not setting them up correctly or using not sensitive enough devices\nBudgeting\nIn the case of any purchasing needs, we should check the costs and discuss them with OpenRemote\nWork organization\nThroughout the work process, any conflicts, blockers or miscommunication between team members can arise. Problems like this should be resolved in an \u201cagile\u201c manner by discussing them with the team.\n\nInfrastructure\nDelivery time\nWhen purchasing the Raspberry PI\u2019s online, there is a chance that online delivery will get delayed. This would delay the testing and implementation of OpenBalena. \n\n"
            }]
        },
        "WindPowerProofofConcepts": {
            "hand-ins": []
        },
        "WindPowerResearchWhatlibrarytouse": {
            "hand-ins": [{
                "text": "Version history\n\n\n\n\nReason for this research\nTo get started on our project for predicting wind energy, we need to know what the potential options in front of us are so that we can make a well-considered decision.  We will look at what the best frameworks are which can be applied to our project, as well as potential prediction model types. \nBesides prediction models this research document will also go into detail about what method of data gathering to use. There are a handful of options to gather data for a dataset, but not all are viable nor cost effective. \n\nThis document will start with a list of all the research questions that we want to answer in this\ndocument. After this we will look through viable and possible answers to the research questions, and discuss \n\nThe target audience for this research is software engineers.  \nResearch questions\nBefore the start of the research, we have to define several research questions which we are going to answer while doing said research. The main questions are written down below, with each containing a subset of child questions. \n\nMain Questions\nWhich ML library has the most potential to deliver good results\nCan it run on an RPI?\nDoes it support continual learning?\nIs there a community behind the library?\nHow difficult is it to implement?\nWhat is the best way to test the results against real life data\nDIY Wind turbine\nWindstats\nOther API\n\n\nWhich ML library to use?\n\nTensorFlow\nTensorFlow is Google\u2019s library for dataflow programming, written in Python. It has a comprehensive set of tools to its disposal, both high and low-level. Keras is meant for getting high performance on big datasets and is used for things like object detection. \nTensorFlow has a large community, making it an accessible library to use for developing the prediction model. Furthermore, it is a well-documented and maintained library, which would really improve the speed at which we can develop the model. This library has full support for RPI and even has tutorials for setting it up on a Raspberry Pi. \nSince this is a high- and low-level library, I think the implementation of this library should be doable. From what I have read the implementation seems comprehensible (enough) to actually get started with it. \nTribuo\nTribuo is Oracle\u2019s latest machine learning library, with support from java 8+ until the latest versions. It is purposely written in an enterprise language like Java instead of traditional languages like Python or R. This way you do not need to handle two languages at the same time for simple ML operations, and instead keep everything tidy in the same package.  Tribuo has one unique feature which makes it an interesting option, which is that it can remember exactly what its inputs were, and it is also able to describe the range and type of each input. This makes Tribuo ideal for backtesting results, since you can see the exact inputs which resulted in the output.   \nThis library is quite new, as it has been changed to Open-Source only a month ago (15 September 2020). This also means that the community for Tribuo is still developing and is as of today, quite small. A small community is not necessarily the end of the world, but it would be good to have a community to ask things to when we get stuck. \nTribuo can run on a RPI, but it does not have any known optimizations for it, whereas TensorFlow does have some.  Besides that, Tribuo is a high-level library, this means that you can very concretely communicate with the library and not have to deal with any complicated ML issues. Also, Tribuo does not have (for as far as I could find) any concrete support for Continuous learning, which is necessary.  \nKeras\nThis library is built on top of Tensorflow 2.0 and is written to enable fast experimentation with deep neural networks. This is an interesting library, since it is of such high-level that it can run on top of TensorFlow, but also libraries like CNTK and Theano. It scores high grades in ease to use and the simplicity it brings with it. \nKeras has a big community and is surprisingly enough more popular than the framework that it was written on top of, being TensorFlow. Besides that, it also has the ability to integrate continuous learning and run on a RPI. \nPyTorch\nPyTorch is an open source machine learning library for Python, based on Torch. It was developed by Facebook\u2019s AI research group and is used for applications like natural language processing. This library is however of lower-level and focusses on direct work with array expressions. This library has gained loads of interest over the last few years, especially in the academic sector. \nBesides being a hard to approach library PyTorch has a lot of potential. I do not think however that it is suitable for our project, since this means that we have to learn a lot of theory and apply this before we can actually get started with the project.  \n\n\nHow to test against real-life data?\nTo create a precise prediction model, we need to test our algorithm against real data. This could be done in two ways: by using external services or collecting our own data from a wind turbine that would be purchased. Both have their pros and cons.\nOwn wind turbine\nIn a meeting with the stakeholders of this project, it was suggested to find a small wind turbine that can be easily installed in a suitable location. The turbine must be able to collect statistics about the generated power in small enough periods of time. This requirement is crucial since the data collected will be used in training the prediction model. While this solution would be independent by any other external services it hides a lot of questions that our team could not be able to solve throughout the duration of the project like choosing a suitable location for the turbine, the proper selection and installation of it, as well as maintaining it. \nExternal services\nBy choosing to use external services for our data we ensure that we would have enough reliable historical data to train the prediction model from day 1 of the development process. This would speed up the process of training.\n\nWindstats.nl\nThis service provides live and historical data for wind farms across The Netherlands. It is the only one we found providing such information. We have reached out to windstats.nl to request sample data from one of their windmills. They sent us the format of the data that they provide. The information we need from them is how much power a windmill is generating at given timeframe and at what location. They provide this data. Their subscription costs 995 euro per year but offer a discount of 25% for educational institutions.\n\nOpenWeatherMap API\nopenweathermap.org provides historical weather information for any location. There is an option to get bulk historical data for up to 40 years. From this source we can gather the wind data needed to compare against the power generated data at the same timeframe.\n\n\n\nConclusion\nWe are going to try and create a working project with Keras, as this library comes out on the top in user friendliness and usability. There is a lot of documentation on Keras readily available, which will help the process. \n\nIn order to test it against real life data we will be purchasing our own wind turbine, which will store the energy output every x minutes. This can go hand-in-hand with windstats.nl, where we will gather the other data necessary to predict the wind speed. "
            }]
        },
        "WindPowerSprintdeliveries": {
            "hand-ins": [{
                "text": "Reflection\nDelivering a new iteration to OpenRemote has always been a good experience so far. The people at OpenRemote have a really open mindset, and aren\u2019t afraid to give us their own input, which gives a really good vibe during the delivery. \nDuring almost all of the deliveries, I am the one who has the word most of the time. This makes sense for the most part, as I am the \u2018project lead\u2019, so this is expected from me. In the beginning I was really opposed to the idea of continuously presenting to OpenRemote, this however, changed rapidly. Right now we are at the 5th sprint delivery, and I can comfortably say that my presenting skill have increased.  \nAlthough I am the one communicating with OpenRemote, I\u2019ll still try to get the others to give in some input as well, to make things less \u2018one sided\u2019. This usually consists of asking my teammates for their opinion, or asking if there have been any points that I have missed during the presentation. Usually there is nothing left to add, but I still think it\u2019s important to give everyone a chance at presenting if they want to.\nIn terms of the \u2018content\u2019 and \u2018actual product\u2019 that we are delivering to OpenRemote, I am usually not 100% satisfied with the result we are delivering. I think this is because we chose to work in springs of two weeks, and I simply do not have enough time to create a good functioning product in that timespan considering I have more projects to work on. Luckily, we also have Infrastructure, who are working on a different \u2018sub-project\u2019. This means that when the Software project doesn\u2019t have a lot of impressive deliverables, we always have infrastructure, and vice-versa. "
            }, {
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWind Energy Sprint Delivery \n\n\n\n\nWind Energy\nSprint Delivery \nDaan de Weirdt \nNiek  van Dam\nDaniel Vaswani\nMartin markov\nElmira Drost \n\n\n\nIndex\nGoal of the sprint\nPOC \nNext sprint\n\n\n\nGoal of the sprint\nResearch data sources\nPoC Software\nPoC Infrastructure\n\n\n\nResearch Data Sources\nWindmill Power Generated Data\nWindstats.nl offers historical data for power generated for each windmill in the windfarms in The Netherlands\nWind Historical Weather Data\nOpenWeatherMap API offers bulk historical weather data for any location for up to 40 years back\n\n\n\n\nPoC Software\nLSTM RNN\nDesign \nTurbine dataset\nNaN values\n\n\n\n\n\nPoC Infrastructure\nBalenaCloud\nBalenaOS\nBalenaOS Raspberry Pi\n\n\n\n\nNext Sprint \nSoftware: \u00a0\tRebase/optimize prediction model on the previously created PoC\nInfrastructure :\tTesting on Raspberry PI with OpenRemote Software \nMisc: \nSolving NaN values with specialist from Fontys\nObtain or Convert docker file to a lower version\n\n\n\nQuestions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n.MsftOfcThm_Accent1_Fill {\n fill:#BE977B; \n}\n.MsftOfcThm_Accent1_Stroke {\n stroke:#BE977B; \n}\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/docProps/thumbnail.jpeg\n\n"
            }, {
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWind Energy Sprint Delivery \n\n\n\n\nWind Energy\nSprint Delivery \nDaan de Weirdt \nNiek  van Dam\nDaniel Vaswani\nMartin markov\nElmira Drost \n\n\n\nIndex\nGoal of the sprint\nResearch data sources\nPOC's\u00a0\nDemo\nNext sprint\n\n\n\nGoal of the sprint\nData sources\nPoC Software\nPoC Infrastructure\n\n\n\nResearch Data Sources\n\nWindstats gives turbine specs\nNo real-time turbine data\u00a0\n\n\n\n\n\n\n\nResearch Data Sources\nOption 1:\nResearch new datasets\n+ More relevant data\u00a0\n+ Larger dataset\n- Potentially\u00a0slow development\n- Possibly costly\nOption 2:\nCreate a wind turbine/weather station\n+ More accurate data\n+ No subscription\n- Retrieving data\n- Time consuming\n\n\n\n\nResearch Data Sources\nOption 3:\nHybrid option (openweather + our own turbine)\n+ Most accurate data\u00a0\n- Time consuming\n\n\n\n\nPoC Software\nLSTM RNN\nTurbine dataset\n\n\n\n\n\n\nWind Turbine for backtesting\nAutoRegressive Integrated Moving Average\nPopular time series forecasting\n\n\n\n\n\nPoC Infrastructure\nOpenBalena vs BalenaCloud\nOpenBalena\nDemo\n\n\n\n\n\nDemo\n\n\n\nSoftware\n\n\nInfrastructure\n\n\n\nNext Sprint \nS: \u00a0Create final prediction model\nI:\u00a0\u00a0 Linking more RPI\u2019s with OpenBalena\nMisc: \n\n\n\nQuestions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n.MsftOfcThm_Accent1_Fill {\n fill:#BE977B; \n}\n.MsftOfcThm_Accent1_Stroke {\n stroke:#BE977B; \n}\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/docProps/thumbnail.jpeg\n\n"
            }, {
                "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWind Energy Sprint Delivery \n\n\n\n\nWind Energy\nSprint Delivery \nDaan de Weirdt \nNiek  van Dam\nDaniel Vaswani\nMartin markov\nElmira Drost \n\n\n\n1\n\n\nIndex\nGoal of the sprint\nWind turbine\nCloud formation\nNext sprint\n\n\n\nGoal of the sprint\nWind Turbine setup\nCloudformation script\n\n\n\nWind turbine\nDocument Building Management\nPermit\n\n\n\n\n\nCloud formation\nCloud-formation\u00a0succesfull\nInstall-script for openbalena not fully functional.\n\n\n\n\n\n\nNext Sprint \nS: Backtesting model on the real-time data  & installation of wind turbine\nI: Deployment of OpenBalena using cloudformation and research OpenBalena-api.\nMisc: \n\n\n\nQuestions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n.MsftOfcThm_Accent1_Fill {\n fill:#BE977B; \n}\n.MsftOfcThm_Accent1_Stroke {\n stroke:#BE977B; \n}\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/docProps/thumbnail.jpeg\n\n"
            }]
        },
        "WindPowerTurbineinstallationspecifications": {
            "hand-ins": [{
                "text": "\n\nVersion control\n\n\n\n\nIntroduction\nThis document is meant to give insight into why we want to install a wind turbine on the TQ building. \nWe are a group of Fontys ICT Delta students working on a project for , where we have to predict Wind Turbine power output. For this we would like to install a wind turbine on the roof of the TQ building. \nWhy are we installing a turbine?\nWe are a group of Delta students working on a project for OpenRemote, where we have to predict the wind power which turbines give off. So far we created a proof of Concept with Artificial Intelligence, this is currently working an ideal wind dataset with a predefined wind turbine. This means that our algorithm will be able to predict the wind turbine power output in ideal situations with wind turbine x. \nAfter we have researched and found out that the Proof of Concept works as expected, we enter the second stage of the project. We need to create a \u2018dynamic\u2019 prediction model based on the proof of concept. In this case, dynamic means that the model must be able to predict the wind power output of different types of wind turbines, by just giving in a few specifications. In order to create this dynamic model we need to install our own wind turbine and train it on the real life data as a first step. \nWhere could it optimally be?\nOur intentions are to install the wind turbine on top of the roof of Strijp-TQ. Specifically, on top of the elevated part of the roof , located above the staircases. We chose this location because this is close to our base of operations, combined with the fact that this is high up, and eligible for harvesting a lot of wind data. \n \n\n\nPermit\nA permit for the installation of a wind turbine is not necessary, as the windmill is less than 4 meters. A permit is also not needed in case the turbine isn\u2019t connect to the powergrid.\nWhat is the impact\nIt will have an impact on the weight (circa 30kgs) of the building and it adds a little extra height (3,58 meter) on the building.\nWhat has to be taken into account?\nWe have to take the bearing force of the pole into account when installing this turbine. We calculated the maximum force and torque that a pole should withstand in order to stand on the roof while maintaining a safe environment. The highest force that our pole should be able to withstand is 24kg, which we calculated by taking the highest windspeed which Brabant has seen in the last 15 years (100 km/h).\nCalculations\nThe calculations are as follows:\nAverage air density = 1.225 kg/m3\nMass of the air (m) = air density (kg/m3)  * area (m2)= 1.225 kg/m3   *  0.25m2 (assuming the pole is 250cm and 10cm wide) = 0.306 kg\nAcceleration (a) = max wind speed(m/s)2  = 100 km/h =  27.7778 m/s2 = 661.6\nForce to withstand = m * a = 661.6 * 0.306 = 202.4 N = 20.639 KG\nWhat is the added value?\nThe added value for Strijp-TQ is that it can come in the media as a positive point. Strijp-TQ will have a good imago with green energy and a learning environment. \nSafety\nFor the safety we kept the following things in mind:\nVery high windspeeds till 100km/h\nSteady pole for the turbine\n\nWhat will be installed?\nParts installed\nOn the roof of Strijp-TQ there will be the following items installed:\nA turbine\nCharge controller\n2,5 meter pole of steel\nBattery for the energy\nArduino \nPot with soil\nTurbine\nSpecifications of the turbine\n\n\nConclusion\nA turbine on the roof for a period of a few months.\nQuestions? \nIf you have any questions contact Eric Slaats for the needed information."
            }]
        },
        "WindPowerWindPredictionFlowchart": {
            "hand-ins": [{
                "text": "Version control\n\n\nTable of Contents\n\n\nIntroduction\nWhen training a neural network, a lot of specialized terminology is usually incorporated into the script, which can make it difficult for outsiders to read and comprehend the code. Therefore this sequence diagram below can be used to explain the code more clearly, without having to know what each method concretely does. \n\n\nDiagram\nThe flowchart below describes the script which is used to train the Neural Network for predicting wind power, a project for OpenRemote. At the moment the script converts a big (100 000+ rows) csv dataset into a pandas dataframe. Pandas has a broad array of tools for handling big datasets, which makes it ideal in this situation. After importing, we clean up all rows with invalid data, as to not feed any invalid data to the neural network when training. Before creating the test and train split dataset, we have to normalize all of the data. We are forced to normalize the data between 0-1 to prevent exploding/vanishing gradients, a phenomena which occurs when data is recursively multiplied by the network and exponentially goes up or down. Best case scenario this causes an OverflowError, worst case you don\u2019t notice it until the network is put to practice. \nOnce normalization is done, we split the data into the test and train datasets, and fit the datasets on the model. Once the fitting/training is done, we plot the historical test results and are able to validate whether or not the model is trained in the best way\n\n"
            }]
        }
    }
}